---
title: 深度学习
---

# 深度学习

## 形态

- **欠拟合**：这些曲线之间的差距很小，验证损失从未增加，因此网络拟合不足的可能性比拟合过度的可能性更大。如果真是这样的话，那就值得试验更多的容量。

- **过拟合**：现在，验证损失很早就开始增加，而训练损失继续减少。这表明网络已经开始过度填充。在这一点上，我们需要尝试一些措施来防止它，要么减少单位数量，要么通过提前停止等方法。

- **提前停止**：一旦开始过拟合，早期停止回调确实停止了训练。此外，通过包括 `restore_best_weights`，我们仍然可以将模型保持在验证损失最低的位置。

## 仿射变换

仿射变换（`affine transformation`），特点是通过`加权`和对特征进行`线性变换`（linear transformation）， 并通过`偏置项`来进行平移（`translation`）。

## 从线性回归到深度网络

![An image](./img/singleneuron.svg)

线性回归是一个**单层神经网络**，层数为 1。我们可以将线性回归模型视为仅由单个人工神经元组成的神经网络，或称为单层神经网络。

对于线性回归，每个输入都与每个输出（在本例中只有一个输出）相连，称为全连接层（fully-connected layer）或称为稠密层（dense layer）。

## 多层感知机

线性意味着单调假设，可能会出错。对于深度神经网络，我们使用观测数据来联合学习隐藏层表示和应用于该表示的线性预测器。

![An image](./img/mlp.svg)

我们可以通过在网络中加入一个或多个隐藏层来克服线性模型的限制， 使其能处理更普遍的函数关系类型。 要做到这一点，最简单的方法是将许多全连接层堆叠在一起。 每一层都输出到上面的层，直到生成最后的输出。 我们可以把前层看作表示，把最后一层看作线性预测器。 这种架构通常称为多层感知机（`multilayer perceptron`），通常缩写为`MLP`。

为了发挥多层架构的潜力，我们还需要一个额外的关键要素：在仿射变换之后对每个隐藏单元应用**非线性**的**激活函数（activation function）**。激活函数的输出被称为活性值（activations）。一般来说，有了激活函数，就不可能再将我们的多层感知机退化成线性模型。

### 激活函数

激活函数（`activation function`）通过计算加权和并加上偏置来确定神经元是否应该被激活， 它们将输入信号转换为输出的可微运算。

#### ReLU

最受欢迎的激活函数是修正线性单元（`Rectified linear unit`，**ReLU**），因为它实现简单，同时在各种预测任务中表现良好。ReLU 提供了一种非常简单的非线性变换。

给定元素`x`，`ReLU`函数被定义为该元素与 0 的最大值：`ReLU(x) = max(x, 0)`。

```python
import torch
from d2l import torch as d2l

x = torch.arange(-8.0, 8.0, 0.5, requires_grad=True)
y = torch.relu(x)
d2l.plot(x.detach(), y.detach(), 'x', 'relu(x)', figsize=(5, 2.5))
```

![An image](./img/relu.svg)

#### sigmoid

对于一个定义域在 `R` 中的输入， `sigmoid` 函数将输入变换为区间(0, 1)上的输出。 `因此，sigmoid` 通常称为**挤压函数（squashing function）**: 它将范围`（-inf, inf）`中的任意输入压缩到区间`（0, 1）`中的某个值。

在最早的神经网络中，科学家们感兴趣的是对“激发”或“不激发”的生物神经元进行建模。 因此，这一领域的先驱可以一直追溯到人工神经元的发明者麦卡洛克和皮茨，他们专注于阈值单元。 阈值单元在其输入低于某个阈值时取值 0，当输入超过阈值时取值 1。

当我们想要将输出视作二元分类问题的概率时， sigmoid 仍然被广泛用作输出单元上的激活函数，后面关于循环神经网络的章节中，我们将描述利用 sigmoid 单元来控制时序信息流的架构。

当输入接近 0 时，sigmoid 函数接近线性变换。（`sigmoid` 可以视为 `softmax` 的特例）

```python
y = torch.sigmoid(x)
d2l.plot(x.detach(), y.detach(), 'x', 'sigmoid(x)', figsize=(5, 2.5))
```

![An image](./img/sigmoid.svg)

#### tanh

与 `sigmoid` 函数类似，`tanh(双曲正切)`函数也能将其输入压缩转换到区间`(-1, 1)`上。当输入在 0 附近时，tanh 函数接近线性变换。 函数的形状类似于 sigmoid 函数， 不同的是 tanh 函数关于坐标系原点中心对称。

```python
y = torch.tanh(x)
d2l.plot(x.detach(), y.detach(), 'x', 'tanh(x)', figsize=(5, 2.5))
```

![An image](./img/tanh.svg)
