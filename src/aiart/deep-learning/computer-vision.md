---
title: è®¡ç®—æœºè§†è§‰ã€Computer Vision
outline: deep
---

# CVï¼šComputer Vision

è®¡ç®—æœºè§†è§‰ã€æœºå™¨è§†è§‰ã€‚æ— è®ºæ˜¯åŒ»ç–—è¯Šæ–­ã€**è‡ªåŠ¨é©¾é©¶**ï¼Œè¿˜æ˜¯æ™ºèƒ½æ»¤æ³¢å™¨ã€æ‘„åƒå¤´ç›‘æ§ï¼Œè®¸å¤šè®¡ç®—æœºè§†è§‰é¢†åŸŸçš„åº”ç”¨éƒ½ä¸æˆ‘ä»¬å½“å‰å’Œæœªæ¥çš„ç”Ÿæ´»å¯†åˆ‡ç›¸å…³ã€‚

## å¾®è°ƒ(fine-tuning)

**è¿ç§»å­¦ä¹ **å°†ä»æºæ•°æ®é›†ä¸­å­¦åˆ°çš„çŸ¥è¯†è¿ç§»åˆ°ç›®æ ‡æ•°æ®é›†ï¼Œ**å¾®è°ƒ**æ˜¯è¿ç§»å­¦ä¹ çš„å¸¸è§æŠ€å·§ã€‚

**å¾®è°ƒ**åŒ…æ‹¬ä»¥ä¸‹å››ä¸ªæ­¥éª¤ï¼š

1. åœ¨æºæ•°æ®é›†ï¼ˆä¾‹å¦‚ ImageNet æ•°æ®é›†ï¼‰ä¸Šé¢„è®­ç»ƒç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œå³**æºæ¨¡å‹**ã€‚
2. åˆ›å»ºä¸€ä¸ªæ–°çš„ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œå³**ç›®æ ‡æ¨¡å‹**ã€‚è¿™å°†å¤åˆ¶æºæ¨¡å‹ä¸Šçš„æ‰€æœ‰æ¨¡å‹è®¾è®¡åŠå…¶å‚æ•°ï¼ˆè¾“å‡ºå±‚é™¤å¤–ï¼‰ã€‚æˆ‘ä»¬å‡å®šè¿™äº›æ¨¡å‹å‚æ•°åŒ…å«ä»æºæ•°æ®é›†ä¸­å­¦åˆ°çš„çŸ¥è¯†ï¼Œè¿™äº›çŸ¥è¯†ä¹Ÿå°†é€‚ç”¨äºç›®æ ‡æ•°æ®é›†ã€‚æˆ‘ä»¬è¿˜å‡è®¾æºæ¨¡å‹çš„è¾“å‡ºå±‚ä¸æºæ•°æ®é›†çš„æ ‡ç­¾å¯†åˆ‡ç›¸å…³ï¼›å› æ­¤ä¸åœ¨ç›®æ ‡æ¨¡å‹ä¸­ä½¿ç”¨è¯¥å±‚ã€‚
3. å‘ç›®æ ‡æ¨¡å‹æ·»åŠ **è¾“å‡ºå±‚**ï¼Œå…¶è¾“å‡ºæ•°æ˜¯ç›®æ ‡æ•°æ®é›†ä¸­çš„ç±»åˆ«æ•°ã€‚ç„¶åéšæœºåˆå§‹åŒ–è¯¥å±‚çš„æ¨¡å‹å‚æ•°ã€‚
4. åœ¨ç›®æ ‡æ•°æ®é›†ï¼ˆå¦‚æ¤…å­æ•°æ®é›†ï¼‰ä¸Šè®­ç»ƒç›®æ ‡æ¨¡å‹ã€‚**è¾“å‡ºå±‚å°†ä»å¤´å¼€å§‹**è¿›è¡Œè®­ç»ƒï¼Œè€Œ**å…¶ä»–å±‚**å‚æ•°å°†æ ¹æ®æºæ¨¡å‹å‚æ•°è¿›è¡Œ**å¾®è°ƒ**ã€‚

é€šå¸¸ï¼Œ**å¾®è°ƒ**å‚æ•°ä½¿ç”¨**è¾ƒå°å­¦ä¹ ç‡**ï¼Œè€Œ**ä»å¤´å¼€å§‹**è®­ç»ƒè¾“å‡ºå±‚å¯ä»¥ä½¿ç”¨**æ›´å¤§å­¦ä¹ ç‡**ã€‚

**ç¤ºæ„ä»£ç **ï¼š

```py
import os
import torch
import torchvision
from torch import nn
from d2l import torch as d2l

finetune_net = torchvision.models.resnet18(pretrained=True)
finetune_net.fc = nn.Linear(finetune_net.fc.in_features, 2)
nn.init.xavier_uniform_(finetune_net.fc.weight)

# å¦‚æœparam_group=Trueï¼Œè¾“å‡ºå±‚ä¸­çš„æ¨¡å‹å‚æ•°å°†ä½¿ç”¨åå€çš„å­¦ä¹ ç‡
def train_fine_tuning(net, learning_rate, batch_size=128, num_epochs=5, param_group=True):
  train_iter = torch.utils.data.DataLoader(torchvision.datasets.ImageFolder(
    os.path.join(data_dir, 'train'), transform=train_augs),
    batch_size=batch_size, shuffle=True)

  test_iter = torch.utils.data.DataLoader(torchvision.datasets.ImageFolder(
    os.path.join(data_dir, 'test'), transform=test_augs),
    batch_size=batch_size)

  devices = d2l.try_all_gpus()
  loss = nn.CrossEntropyLoss(reduction="none")

  if param_group:
    params_1x = [
      param for name, param in net.named_parameters() if name not in ["fc.weight", "fc.bias"]
    ]
    trainer = torch.optim.SGD(
      [{'params': params_1x}, {'params': net.fc.parameters(), 'lr': learning_rate * 10}],
      lr=learning_rate, weight_decay=0.001
    )
  else:
    trainer = torch.optim.SGD(net.parameters(), lr=learning_rate, weight_decay=0.001)

  d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)

# ä½¿ç”¨è¾ƒå°å­¦ä¹ ç‡ï¼Œé€šè¿‡å¾®è°ƒé¢„è®­ç»ƒè·å¾—çš„æ¨¡å‹å‚æ•°
train_fine_tuning(finetune_net, 5e-5)
```

## ç›®æ ‡æ£€æµ‹ä¸è¾¹ç•Œæ¡† ğŸ”¥ğŸ”¥ğŸ”¥

**ç›®æ ‡æ£€æµ‹/è¯†åˆ«**ï¼šä¸ä»…å¯ä»¥æ£€æµ‹<sup>object detection</sup>è¯†åˆ«<sup>object recognition</sup>å›¾åƒä¸­æ‰€æœ‰æ„Ÿå…´è¶£çš„ç‰©ä½“ï¼Œè¿˜èƒ½è¯†åˆ«å®ƒä»¬çš„ä½ç½®ï¼Œè¯¥ä½ç½®é€šå¸¸ç”±çŸ©å½¢è¾¹ç•Œæ¡†è¡¨ç¤ºã€‚

**è¾¹ç•Œæ¡†**ï¼šåœ¨ç›®æ ‡æ£€æµ‹ä¸­ï¼Œé€šå¸¸ä½¿ç”¨è¾¹ç•Œæ¡†<sup>bounding box</sup>æ¥æè¿°å¯¹è±¡çš„ç©ºé—´ä½ç½®ã€‚

è¾¹ç•Œæ¡†é€šå¸¸æ˜¯**çŸ©å½¢**ï¼Œä¸¤ç§å¸¸ç”¨è¾¹ç•Œæ¡†è¡¨ç¤ºã€Œä¸­å¿ƒ $(x,y)$ï¼Œå®½åº¦ï¼Œé«˜åº¦ã€å’Œã€Œå·¦ä¸Š $x$ï¼Œå³ä¸‹ $y$ã€ã€‚

## é”šæ¡†ä¸äº¤å¹¶æ¯”(IoU)

### é”šæ¡†

ç›®æ ‡æ£€æµ‹ç®—æ³•é€šå¸¸ä¼šé‡‡æ ·å¤§é‡åŒºåŸŸï¼Œåˆ¤æ–­å…¶ä¸­æ˜¯å¦åŒ…å«ç›®æ ‡ï¼Œå¹¶è°ƒæ•´è¾¹ç•Œæ›´å‡†ç¡®åœ°é¢„æµ‹ç›®æ ‡çœŸå®è¾¹ç•Œæ¡†<sup>ground-truth bounding box</sup>ï¼Œè¿™äº›è¾¹ç•Œæ¡†è¢«ç§°ä¸ºé”šæ¡†<sup>anchor box</sup>ã€‚

ä¸åŒæ¨¡å‹é‡‡æ ·å„å¼‚ã€‚æ¯”å¦‚ä»¥æ¯ä¸ªåƒç´ ä¸ºä¸­å¿ƒï¼Œç”Ÿæˆå¤šä¸ªç¼©æ”¾æ¯”å’Œå®½é«˜æ¯”<sup>aspect ratio</sup>çš„ä¸åŒè¾¹ç•Œæ¡†ã€‚

```py
# ç”Ÿæˆä»¥æ¯ä¸ªåƒç´ ä¸ºä¸­å¿ƒå…·æœ‰ä¸åŒå½¢çŠ¶çš„é”šæ¡†
def multibox_prior(data, sizes, ratios):
  in_height, in_width = data.shape[-2:]
  device, num_sizes, num_ratios = data.device, len(sizes), len(ratios)
  boxes_per_pixel = (num_sizes + num_ratios - 1)
  size_tensor = torch.tensor(sizes, device=device)
  ratio_tensor = torch.tensor(ratios, device=device)

  # ä¸ºäº†å°†é”šç‚¹ç§»åŠ¨åˆ°åƒç´ çš„ä¸­å¿ƒï¼Œéœ€è¦è®¾ç½®åç§»é‡ã€‚
  # å› ä¸ºä¸€ä¸ªåƒç´ çš„é«˜ä¸º1ä¸”å®½ä¸º1ï¼Œæˆ‘ä»¬é€‰æ‹©åç§»æˆ‘ä»¬çš„ä¸­å¿ƒ0.5
  offset_h, offset_w = 0.5, 0.5
  steps_h = 1.0 / in_height  # åœ¨yè½´ä¸Šç¼©æ”¾æ­¥é•¿
  steps_w = 1.0 / in_width  # åœ¨xè½´ä¸Šç¼©æ”¾æ­¥é•¿

  # ç”Ÿæˆé”šæ¡†çš„æ‰€æœ‰ä¸­å¿ƒç‚¹
  center_h = (torch.arange(in_height, device=device) + offset_h) * steps_h
  center_w = (torch.arange(in_width, device=device) + offset_w) * steps_w
  shift_y, shift_x = torch.meshgrid(center_h, center_w, indexing='ij')
  shift_y, shift_x = shift_y.reshape(-1), shift_x.reshape(-1)

  # ç”Ÿæˆâ€œboxes_per_pixelâ€ä¸ªé«˜å’Œå®½ï¼Œ
  # ä¹‹åç”¨äºåˆ›å»ºé”šæ¡†çš„å››è§’åæ ‡(xmin,xmax,ymin,ymax)
  w = torch.cat((size_tensor * torch.sqrt(ratio_tensor[0]),
                  sizes[0] * torch.sqrt(ratio_tensor[1:])))\
                  * in_height / in_width  # å¤„ç†çŸ©å½¢è¾“å…¥
  h = torch.cat((size_tensor / torch.sqrt(ratio_tensor[0]),
                  sizes[0] / torch.sqrt(ratio_tensor[1:])))
  # é™¤ä»¥2æ¥è·å¾—åŠé«˜å’ŒåŠå®½
  anchor_manipulations = torch.stack((-w, -h, w, h)).T.repeat(in_height * in_width, 1) / 2

  # æ¯ä¸ªä¸­å¿ƒç‚¹éƒ½å°†æœ‰â€œboxes_per_pixelâ€ä¸ªé”šæ¡†ï¼Œ
  # æ‰€ä»¥ç”Ÿæˆå«æ‰€æœ‰é”šæ¡†ä¸­å¿ƒçš„ç½‘æ ¼ï¼Œé‡å¤äº†â€œboxes_per_pixelâ€æ¬¡
  out_grid = torch.stack([shift_x, shift_y, shift_x, shift_y],
              dim=1).repeat_interleave(boxes_per_pixel, dim=0)
  output = out_grid + anchor_manipulations
  return output.unsqueeze(0)
```

é‚£ä¹ˆå¦‚ä½•è¡¡é‡é”šæ¡†**å‡†ç¡®æ€§**å‘¢ï¼Ÿæ¢è¨€ä¹‹ï¼Œè‹¥å·²çŸ¥ç›®æ ‡çœŸå®è¾¹ç•Œæ¡†ï¼Œå¦‚ä½•è¡¡é‡é”šæ¡†å’Œ**çœŸå®è¾¹ç•Œæ¡†**ä¹‹é—´**ç›¸ä¼¼æ€§**ï¼Ÿæ°å¡å¾·ç³»æ•°<sup>Jaccard</sup>å¯ä»¥è¡¡é‡ä¸¤è€…ä¹‹é—´ç›¸ä¼¼æ€§ã€‚

### äº¤å¹¶æ¯”

**IoU**ï¼šIntersection Over Union **äº¤å¹¶æ¯”**ï¼Œä¸¤ä¸ªè¾¹ç•Œæ¡†**äº¤é›†é™¤ä»¥å¹¶é›†**ï¼Œä¹Ÿè¢«ç§°ä¸ºæ°å¡å¾·ç³»æ•°ã€‚

$$
J(A, B) = \frac{|A \cap B|}{|A \cup B|}
$$

äº¤å¹¶æ¯”çš„å–å€¼èŒƒå›´åœ¨ 0 å’Œ 1 ä¹‹é—´ï¼š0 è¡¨ç¤ºä¸¤ä¸ªè¾¹ç•Œæ¡†æ— é‡åˆåƒç´ ï¼Œ1 è¡¨ç¤ºä¸¤ä¸ªè¾¹ç•Œæ¡†å®Œå…¨é‡åˆã€‚
![An Image](./img/iou.svg)

```py
# è®¡ç®—ä¸¤ä¸ªé”šæ¡†æˆ–è¾¹ç•Œæ¡†åˆ—è¡¨ä¸­æˆå¯¹çš„äº¤å¹¶æ¯”
def box_iou(boxes1, boxes2):
  box_area = lambda boxes: ((boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1]))
  # boxes1,boxes2,areas1,areas2çš„å½¢çŠ¶:
  # boxes1ï¼š(boxes1çš„æ•°é‡,4),
  # boxes2ï¼š(boxes2çš„æ•°é‡,4),
  # areas1ï¼š(boxes1çš„æ•°é‡,),
  # areas2ï¼š(boxes2çš„æ•°é‡,)
  areas1 = box_area(boxes1)
  areas2 = box_area(boxes2)
  # inter_upperlefts,inter_lowerrights,intersçš„å½¢çŠ¶:
  # (boxes1çš„æ•°é‡,boxes2çš„æ•°é‡,2)
  inter_upperlefts = torch.max(boxes1[:, None, :2], boxes2[:, :2])
  inter_lowerrights = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])
  inters = (inter_lowerrights - inter_upperlefts).clamp(min=0)
  # inter_areasandunion_areasçš„å½¢çŠ¶:(boxes1çš„æ•°é‡,boxes2çš„æ•°é‡)
  inter_areas = inters[:, :, 0] * inters[:, :, 1]
  union_areas = areas1[:, None] + areas2 - inter_areas
  return inter_areas / union_areas
```

### å°ç»“

**è®­ç»ƒæ—¶**ï¼šæˆ‘ä»¬éœ€è¦ç»™æ¯ä¸ªé”šæ¡†ä¸¤ç§ç±»å‹çš„æ ‡ç­¾ã€‚ä¸€ä¸ªæ˜¯ä¸é”šæ¡†ä¸­ç›®æ ‡æ£€æµ‹çš„ç±»åˆ«ï¼Œå¦ä¸€ä¸ªæ˜¯é”šæ¡†çœŸå®ç›¸å¯¹äºè¾¹ç•Œæ¡†çš„åç§»é‡ã€‚

**é¢„æµ‹æ—¶**ï¼šå¯ä»¥ä½¿ç”¨**éæå¤§å€¼æŠ‘åˆ¶**<sup>non-maximum suppressionï¼ŒNMS</sup>æ¥**ç§»é™¤ç±»ä¼¼**é¢„æµ‹è¾¹ç•Œæ¡†ï¼Œä»è€Œç®€åŒ–è¾“å‡ºã€‚

## å•å‘å¤šæ¡†æ£€æµ‹(SSD)

**SSD**ï¼šSingle Shot MultiBox Detectorï¼Œæ˜¯ä¸€ç§é«˜æ•ˆçš„ç›®æ ‡æ£€æµ‹ç®—æ³•ï¼Œç”±`Wei Liu`ç­‰äººåœ¨ 2016 å¹´[è®ºæ–‡](https://arxiv.org/abs/1512.02325)ä¸­æå‡ºã€‚å®ƒé€šè¿‡**å•æ¬¡å‰å‘ä¼ æ’­**å³å¯å®Œæˆç›®æ ‡æ£€æµ‹ï¼Œå…·æœ‰**é€Ÿåº¦å¿«ã€ç²¾åº¦é«˜**çš„ç‰¹ç‚¹ï¼Œå¹¿æ³›åº”ç”¨äº**å®æ—¶æ£€æµ‹**åœºæ™¯ã€‚

â€œå•å‘â€ï¼ˆ`Single Shot`ï¼‰æ˜¯æŒ‡ç®—æ³•ä»…éœ€ä¸€æ¬¡å‰å‘ä¼ æ’­ï¼ˆå³â€œå•æ¬¡é€šè¿‡ç¥ç»ç½‘ç»œâ€ï¼‰å³å¯ç›´æ¥è¾“å‡ºæ£€æµ‹ç»“æœï¼Œæ— éœ€åƒä¼ ç»Ÿä¸¤é˜¶æ®µæ–¹æ³•ï¼ˆå¦‚ Faster R-CNNï¼‰é‚£æ ·å…ˆç”Ÿæˆå€™é€‰åŒºåŸŸï¼ˆRegion Proposalsï¼‰ï¼Œå†å¯¹å€™é€‰åŒºåŸŸè¿›è¡Œåˆ†ç±»å’Œå›å½’ã€‚

åœ¨å¤šä¸ªå°ºåº¦ä¸‹ï¼Œç”Ÿæˆ**ä¸åŒå°ºå¯¸é”šæ¡†æ¥æ£€æµ‹ä¸åŒå°ºå¯¸ç›®æ ‡**ã€‚é€šè¿‡å®šä¹‰ç‰¹å¾å›¾çš„å½¢çŠ¶ï¼Œå†³å®šä»»ä½•å›¾åƒä¸Šå‡åŒ€é‡‡æ ·çš„é”šæ¡†ä¸­å¿ƒã€‚ä½¿ç”¨è¾“å…¥å›¾åƒåœ¨æŸä¸ª**æ„Ÿå—é‡**åŒºåŸŸå†…ä¿¡æ¯ï¼Œæ¥é¢„æµ‹è¾“å…¥å›¾åƒä¸Šä¸è¯¥åŒºåŸŸä½ç½®ç›¸è¿‘çš„é”šæ¡†ç±»åˆ«å’Œåç§»é‡ã€‚é€šè¿‡**æ·±åº¦å­¦ä¹ **ï¼Œç”¨**å¤šå±‚æ¬¡å›¾åƒåˆ†å±‚**è¡¨ç¤ºè¿›è¡Œ**å¤šå°ºåº¦ç›®æ ‡æ£€æµ‹**ã€‚

### æ¨¡å‹

å•å‘å¤šæ¡†æ£€æµ‹æ¨¡å‹ä¸»è¦ç”±**ä¸€ä¸ªåŸºç¡€ç½‘ç»œ**å—å’Œè‹¥å¹²**å¤šå°ºåº¦ç‰¹å¾**å—**ä¸²è”**è€Œæˆã€‚
![An Image](./img/ssd.svg)

### ç±»åˆ«é¢„æµ‹å±‚

è®¾ç›®æ ‡ç±»åˆ«æ•°é‡ä¸º $q$ã€‚è¿™æ ·ä¸€æ¥ï¼Œé”šæ¡†æœ‰ $q+1$ ä¸ªç±»åˆ«ï¼Œå…¶ä¸­ 0 ç±»æ˜¯èƒŒæ™¯ã€‚

```py
import torch
import torchvision
from torch import nn
from torch.nn import functional as F
from d2l import torch as d2l

def cls_predictor(num_inputs, num_anchors, num_classes):
  return nn.Conv2d(num_inputs, num_anchors * (num_classes + 1), kernel_size=3, padding=1)
```

ä½¿ç”¨å¡«å……ä¸º 1 çš„ 3x3 çš„å·ç§¯å±‚ï¼Œæ­¤å·ç§¯å±‚çš„**è¾“å…¥å’Œè¾“å‡º**å®½åº¦å’Œé«˜åº¦**ä¿æŒä¸å˜**ã€‚è¿™æ ·ä¸€æ¥ï¼Œè¾“å‡ºå’Œè¾“å…¥åœ¨ç‰¹å¾å›¾å®½å’Œé«˜ä¸Šçš„ç©ºé—´åæ ‡ä¸€ä¸€å¯¹åº”ã€‚

### è¾¹ç•Œæ¡†é¢„æµ‹å±‚

```py
def bbox_predictor(num_inputs, num_anchors):
  return nn.Conv2d(num_inputs, num_anchors * 4, kernel_size=3, padding=1)
```

ä¸åŒçš„æ˜¯ï¼Œè¿™é‡Œéœ€è¦ä¸ºæ¯ä¸ªé”šæ¡†é¢„æµ‹ 4 ä¸ªåç§»é‡ï¼Œè€Œä¸æ˜¯ num_classes + 1 ä¸ªç±»åˆ«ã€‚

### è¿ç»“å¤šå°ºåº¦çš„é¢„æµ‹

```py
def forward(x, block):
  return block(x)

Y1 = forward(torch.zeros((2, 8, 20, 20)), cls_predictor(8, 5, 10))
Y2 = forward(torch.zeros((2, 16, 10, 10)), cls_predictor(16, 3, 10))
Y1.shape, Y2.shape
# (torch.Size([2, 55, 20, 20]), torch.Size([2, 33, 10, 10]))
```

é™¤æ‰¹é‡å¤§å°å¤–ï¼Œå…¶ä»–ä¸‰ä¸ªç»´åº¦éƒ½ä¸åŒå°ºå¯¸ã€‚å°†é¢„æµ‹ç»“æœè½¬æˆäºŒç»´(`æ‰¹é‡å¤§å°ï¼Œé«˜ x å®½ x é€šé“æ•°`)æ ¼å¼ï¼Œååœ¨ç»´åº¦ 1 ä¸Šè¿ç»“ã€‚

```py
def flatten_pred(pred):
  return torch.flatten(pred.permute(0, 2, 3, 1), start_dim=1)

def concat_preds(preds):
  return torch.cat([flatten_pred(p) for p in preds], dim=1)

concat_preds([Y1, Y2]).shape
# torch.Size([2, 25300])
```

### é«˜å’Œå®½å‡åŠå—

```py
def down_sample_blk(in_channels, out_channels):
  blk = []
  for _ in range(2):
    blk.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))
    blk.append(nn.BatchNorm2d(out_channels))
    blk.append(nn.ReLU())
    in_channels = out_channels
  blk.append(nn.MaxPool2d(2))
  return nn.Sequential(*blk)
```

ç”±ä¸¤ä¸ªå¡«å……ä¸º 1 çš„ 3x3 å·ç§¯å±‚(ä¸å˜)ã€ä»¥åŠæ­¥å¹…ä¸º 2 çš„ 2x2 æœ€å¤§æ±‡èšå±‚(å‡åŠ)ç»„æˆã€‚

å¯¹äºæ­¤é«˜å’Œå®½å‡åŠå—çš„è¾“å…¥å’Œè¾“å‡ºç‰¹å¾å›¾ï¼Œ1x2+(3-1)+(3-1)=6ï¼Œæ‰€ä»¥è¾“å‡ºä¸­æ¯ä¸ªå•å…ƒåœ¨è¾“å…¥ä¸Šéƒ½æœ‰ä¸€ä¸ª 6x6 æ„Ÿå—é‡ã€‚å› æ­¤ï¼Œ**é«˜å’Œå®½å‡åŠ**å—ä¼š**æ‰©å¤§**æ¯ä¸ªå•å…ƒåœ¨å…¶è¾“å‡ºç‰¹å¾å›¾ä¸­çš„**æ„Ÿå—é‡**ã€‚

```py
forward(torch.zeros((2, 3, 20, 20)), down_sample_blk(3, 10)).shape
# torch.Size([2, 10, 10, 10])
```

## è¯­ä¹‰åˆ†å‰² ğŸ”¥ğŸ”¥ğŸ”¥

ç›®æ ‡æ£€æµ‹ä¸­ï¼Œæˆ‘ä»¬ä¸€ç›´ä½¿ç”¨**æ–¹å½¢è¾¹ç•Œæ¡†**æ¥æ ‡æ³¨å’Œé¢„æµ‹å›¾åƒä¸­çš„ç›®æ ‡ã€‚

### æ ¸å¿ƒæ¦‚å¿µ

**è¯­ä¹‰åˆ†å‰²**<sup>semantic segmentation</sup>ï¼Œæœ¬è´¨æ˜¯å¯†é›†é¢„æµ‹<sup>Dense Prediction</sup>ï¼Œä¸º**æ¯ä¸ªåƒç´ **åˆ†é…ä¸€ä¸ªè¯­ä¹‰ç±»åˆ«æ ‡ç­¾ï¼ˆå¦‚â€œäººâ€â€œè½¦â€â€œå¤©ç©ºâ€ï¼‰ï¼Œå®ç°åƒç´ çº§åˆ«åˆ†ç±»ã€‚æ ‡æ³¨å’Œé¢„æµ‹éƒ½æ˜¯**åƒç´ çº§**ï¼Œæ¯”ç›®æ ‡æ£€æµ‹**æ›´ç²¾ç»†**ã€‚

è¯­ä¹‰åˆ†å‰²è¾“å…¥å›¾åƒå’Œæ ‡ç­¾åœ¨åƒç´ ä¸Šä¸€ä¸€å¯¹åº”ï¼Œè¾“å…¥å›¾åƒä¼šè¢«**éšæœºè£å‰ª**ä¸ºå›ºå®šå°ºå¯¸è€Œ**ä¸æ˜¯ç¼©æ”¾**ã€‚

### å…³é”®æŠ€æœ¯

- **å…¨å·ç§¯ç½‘ç»œ(FCN)**ï¼šå°†ä¼ ç»Ÿ CNN ä¸­çš„å…¨è¿æ¥å±‚æ›¿æ¢ä¸ºå·ç§¯å±‚ï¼Œä½¿ç½‘ç»œå¯ä»¥æ¥å—ä»»æ„å°ºå¯¸çš„è¾“å…¥å¹¶è¾“å‡ºç›¸åº”å°ºå¯¸çš„åˆ†å‰²å›¾ã€‚
- **ç¼–ç å™¨-è§£ç å™¨ç»“æ„**ï¼šç¼–ç å™¨é€šè¿‡å·ç§¯å’Œä¸‹é‡‡æ ·æå–é«˜çº§ç‰¹å¾ï¼Œè§£ç å™¨é€šè¿‡ä¸Šé‡‡æ ·æ¢å¤ç©ºé—´åˆ†è¾¨ç‡ã€‚
- **è·³è·ƒè¿æ¥(Skip Connection)**ï¼šå°†æµ…å±‚ç‰¹å¾ä¸æ·±å±‚ç‰¹å¾èåˆï¼Œä¿ç•™æ›´å¤šç©ºé—´ç»†èŠ‚ä¿¡æ¯ã€‚

ä¼ ç»Ÿ CNN å…¨è¿æ¥å±•å¹³ã€å…¨å±€æ± åŒ–ä¼šä¸¢å¤±`ä½ç½®ä¿¡æ¯ã€ç›¸é‚»åƒç´ çš„æ¢¯åº¦ã€åŒºåŸŸä¸€è‡´æ€§`ç­‰**ç©ºé—´ä¿¡æ¯**ï¼Œè€Œè¯­ä¹‰åˆ†å‰²éœ€è¦ä¿ç•™ç©ºé—´åˆ†è¾¨ç‡ï¼Œå› æ­¤å¿…é¡»ä½¿ç”¨**å…¨å·ç§¯ç»“æ„**ã€‚

### ç»å…¸æ¨¡å‹

**1. FCN (Fully Convolutional Networks)**

- é¦–ä¸ªç«¯åˆ°ç«¯çš„å…¨å·ç§¯è¯­ä¹‰åˆ†å‰²ç½‘ç»œ
- ä½¿ç”¨**è½¬ç½®å·ç§¯**è¿›è¡Œä¸Šé‡‡æ ·
- å¼•å…¥è·³è·ƒè¿æ¥èåˆå¤šå±‚ç‰¹å¾

**2. U-Net**([è®ºæ–‡åœ°å€](https://arxiv.org/abs/1505.04597))

- åŒ»å­¦å›¾åƒåˆ†å‰²çš„ç»å…¸ç½‘ç»œ
- **ç»“æ„å¯¹ç§°**çš„ç¼–ç å™¨-è§£ç å™¨
- å¤§é‡è·³è·ƒè¿æ¥ä¿ç•™ç»†èŠ‚ä¿¡æ¯

**3. DeepLab ç³»åˆ—**

- ä½¿ç”¨ç©ºæ´å·ç§¯(Atrous Convolution)æ‰©å¤§æ„Ÿå—é‡
- å¼•å…¥ ASPP(Atrous Spatial Pyramid Pooling)æ¨¡å—æ•æ‰å¤šå°ºåº¦ä¿¡æ¯
- ä½¿ç”¨ CRF(Conditional Random Field)åå¤„ç†ç»†åŒ–è¾¹ç•Œ

## å…¨å·ç§¯ç½‘ç»œ(FCN)

**FCN**ï¼šFully Convolutional Networkï¼Œå³ç½‘ç»œ**å®Œå…¨ç”±å·ç§¯å±‚æ„æˆ**ï¼Œæ²¡æœ‰ä»»ä½•å…¨è¿æ¥å±‚ã€‚

è¿™ä¸€è®¾è®¡ä½¿å¾—ç½‘ç»œèƒ½å¤Ÿå¤„ç†ä»»æ„å°ºå¯¸çš„è¾“å…¥å›¾åƒï¼Œå¹¶è¾“å‡ºç›¸åº”å°ºå¯¸çš„å¯†é›†é¢„æµ‹ï¼ˆå¦‚å›¾åƒåˆ†å‰²ä¸­çš„é€åƒç´ åˆ†ç±»ï¼‰ã€‚

é€šè¿‡**è½¬ç½®å·ç§¯**ï¼Œå°†ä¸­é—´å±‚ç‰¹å¾å›¾çš„é«˜å’Œå®½å˜æ¢å›è¾“å…¥å›¾åƒçš„å°ºå¯¸ï¼Œè¾“å‡ºç±»åˆ«é¢„æµ‹ä¸è¾“å…¥å›¾åƒåœ¨åƒç´ çº§åˆ«ä¸Šå…·æœ‰ä¸€ä¸€å¯¹åº”å…³ç³»ï¼š**é€šé“ç»´è¾“å‡º**å³è¯¥ä½ç½®å¯¹åº”åƒç´ çš„**ç±»åˆ«é¢„æµ‹**ã€‚

### åˆå§‹åŒ–è½¬ç½®å·ç§¯å±‚

åœ¨å›¾åƒå¤„ç†ä¸­ï¼Œæˆ‘ä»¬æœ‰æ—¶éœ€è¦**å°†å›¾åƒæ”¾å¤§**ï¼Œå³**ä¸Šé‡‡æ ·**<sup>upsampling</sup>ã€‚

**åŒçº¿æ€§æ’å€¼**<sup>bilinear interpolation</sup> æ˜¯å¸¸ç”¨ä¸Šé‡‡æ ·æ–¹æ³•ä¹‹ä¸€ï¼Œå®ƒä¹Ÿç»å¸¸ç”¨äº**åˆå§‹åŒ–**è½¬ç½®å·ç§¯å±‚ã€‚

1. å°†è¾“å‡ºå›¾åƒçš„åæ ‡ $(x, y)$ æ˜ å°„åˆ°è¾“å…¥å›¾åƒçš„åæ ‡ $(x', y')$ ä¸Šã€‚ä¾‹å¦‚ï¼Œæ ¹æ®è¾“å…¥ä¸è¾“å‡ºçš„å°ºå¯¸ä¹‹æ¯”æ¥æ˜ å°„ã€‚è¯·æ³¨æ„ï¼Œæ˜ å°„åçš„ $x'$ å’Œ $y'$ æ˜¯å®æ•°ã€‚
2. åœ¨è¾“å…¥å›¾åƒä¸Šæ‰¾åˆ°ç¦»åæ ‡ $(x', y')$ æœ€è¿‘çš„ 4 ä¸ªåƒç´ ã€‚
3. è¾“å‡ºå›¾åƒåœ¨åæ ‡ $(x, y)$ çš„åƒç´ ä¾æ®è¾“å…¥å›¾åƒè¿™ 4 ä¸ªåƒç´ åŠå…¶ä¸ $(x', y')$ ç›¸å¯¹è·ç¦»æ¥è®¡ç®—ã€‚

```py
def bilinear_kernel(in_channels, out_channels, kernel_size):
  factor = (kernel_size + 1) // 2
  if kernel_size % 2 == 1:
    center = factor - 1
  else:
    center = factor - 0.5
  og = (torch.arange(kernel_size).reshape(-1, 1),
        torch.arange(kernel_size).reshape(1, -1))
  filt = (1 - torch.abs(og[0] - center) / factor) * \
          (1 - torch.abs(og[1] - center) / factor)
  weight = torch.zeros((in_channels, out_channels, kernel_size, kernel_size))
  weight[range(in_channels), range(out_channels), :, :] = filt
  return weight
```

### æ„é€ æ¨¡å‹

![An Image](./img/fcn.svg)

1. å…ˆä½¿ç”¨**å·ç§¯ç¥ç»ç½‘ç»œ**æŠ½å–å›¾åƒç‰¹å¾â€”â€”**ç¼–ç å™¨**æå–ç‰¹å¾(ä¸‹é‡‡æ ·)ã€‚
2. ç„¶å 1x1 å·ç§¯å±‚**å°†é€šé“æ•°å˜æ¢ä¸ºç±»åˆ«ä¸ªæ•°**â€”â€”è°ƒæ•´é€šé“æ•°ã€‚
3. æœ€å**è½¬ç½®å·ç§¯å±‚**å°†ç‰¹å¾å›¾é«˜å’Œå®½**å˜æ¢ä¸ºè¾“å…¥å›¾åƒå°ºå¯¸**â€”â€”**è§£ç å™¨**æ¢å¤åˆ†è¾¨ç‡(ä¸Šé‡‡æ ·)ã€‚

å› æ­¤ï¼Œæ¨¡å‹è¾“å‡ºä¸è¾“å…¥å›¾åƒçš„é«˜å’Œå®½ç›¸åŒï¼Œä¸”æœ€ç»ˆè¾“å‡ºé€šé“åŒ…å«äº†è¯¥ç©ºé—´ä½ç½®åƒç´ ç±»åˆ«é¢„æµ‹ã€‚

```py
import torch
import torchvision
from torch import nn
from torch.nn import functional as F
from d2l import torch as d2l

# 1.ä½¿ç”¨ImageNetæ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„ResNet-18æ¥æå–å›¾åƒç‰¹å¾(ç¼–ç å™¨)
pretrained_net = torchvision.models.resnet18(pretrained=True)
net = nn.Sequential(*list(pretrained_net.children())[:-2])

# ä½¿ç”¨Pascal VOC2012è®­ç»ƒé›†
num_classes = 21
# 2.æ·»åŠ 1x1å·ç§¯å±‚(è°ƒæ•´é€šé“æ•°)
net.add_module('final_conv', nn.Conv2d(512, num_classes, kernel_size=1))
# 3.æ·»åŠ è½¬ç½®å·ç§¯å±‚(è§£ç å™¨)
# æ­¥å¹…ä¸ºsï¼Œå¡«å……ä¸ºs/2(æ•´æ•°)ä¸”å·ç§¯æ ¸é«˜å’Œå®½ä¸º2sï¼Œè½¬ç½®å·ç§¯æ ¸ä¼šå°†è¾“å…¥é«˜å’Œå®½åˆ†åˆ«æ”¾å¤§så€
net.add_module('transpose_conv', nn.ConvTranspose2d(num_classes,
                num_classes, kernel_size=64, padding=16, stride=32))

# ç”¨åŒçº¿æ€§æ’å€¼çš„ä¸Šé‡‡æ ·åˆå§‹åŒ–è½¬ç½®å·ç§¯å±‚
W = bilinear_kernel(num_classes, num_classes, 64)
net.transpose_conv.weight.data.copy_(W)

# æŸå¤±å‡½æ•°
# å› ä¸ºä½¿ç”¨é€šé“é¢„æµ‹åƒç´ ç±»åˆ«ï¼Œæ‰€ä»¥éœ€è¦æŒ‡å®šé€šé“ç»´ã€‚ç”¨æ¯ä¸ªåƒç´ é¢„æµ‹ç±»åˆ«æ˜¯å¦æ­£ç¡®æ¥è®¡ç®—å‡†ç¡®ç‡ã€‚
def loss(inputs, targets):
  return F.cross_entropy(inputs, targets, reduction='none').mean(1).mean(1)

# è®­ç»ƒ
num_epochs, lr, wd, devices = 5, 0.001, 1e-3, d2l.try_all_gpus()
trainer = torch.optim.SGD(net.parameters(), lr=lr, weight_decay=wd)
d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)

# é¢„æµ‹
# éœ€è¦å°†è¾“å…¥å›¾åƒåœ¨å„ä¸ªé€šé“åšæ ‡å‡†åŒ–ï¼Œå¹¶è½¬æˆå·ç§¯ç¥ç»ç½‘ç»œæ‰€éœ€è¦å››ç»´è¾“å…¥æ ¼å¼
def predict(img):
  X = test_iter.dataset.normalize_image(img).unsqueeze(0)
  pred = net(X.to(devices[0])).argmax(dim=1)
  return pred.reshape(pred.shape[1], pred.shape[2])

# ä¸ºå¯è§†åŒ–é¢„æµ‹ç±»åˆ«ç»™æ¯ä¸ªåƒç´ ï¼Œå°†é¢„æµ‹ç±»åˆ«æ˜ å°„å›å®ƒä»¬åœ¨æ•°æ®é›†ä¸­çš„æ ‡æ³¨é¢œè‰²
def label2image(pred):
  colormap = torch.tensor(d2l.VOC_COLORMAP, device=devices[0])
  X = pred.long()
  return colormap[X, :]

# é¢„æµ‹å¯åŠ¨
voc_dir = d2l.download_extract('voc2012', 'VOCdevkit/VOC2012')
test_images, test_labels = d2l.read_voc_images(voc_dir, False)
n, imgs = 4, []
for i in range(n):
    crop_rect = (0, 0, 320, 480)
    X = torchvision.transforms.functional.crop(test_images[i], *crop_rect)
    pred = label2image(predict(X))
    imgs += [X.permute(1,2,0), pred.cpu(),
             torchvision.transforms.functional.crop(
                 test_labels[i], *crop_rect).permute(1,2,0)]
d2l.show_images(imgs[::3] + imgs[1::3] + imgs[2::3], 3, n, scale=2)
```

## è½¬ç½®å·ç§¯(åå·ç§¯/ä¸Šé‡‡æ ·)

å·ç§¯ç¥ç»ç½‘ç»œ<sup>CNN</sup>çš„å·ç§¯å±‚å’Œæ±‡èšå±‚ï¼Œé€šå¸¸ä¼šå‡å°‘**ä¸‹é‡‡æ ·**è¾“å…¥å›¾åƒç©ºé—´ç»´åº¦ï¼ˆé«˜å’Œå®½ï¼‰ã€‚

![An Image](./img/trans_conv.svg)
**è½¬ç½®å·ç§¯**<sup>transposed convolution</sup>é€šè¿‡å·ç§¯æ ¸**å¹¿æ’­**è¾“å…¥å…ƒç´ ï¼Œå¢åŠ **ä¸Šé‡‡æ ·**ä¸­é—´å±‚ç‰¹å¾å›¾ç©ºé—´ç»´åº¦ï¼Œå®ç°**è¾“å‡ºå¤§äºè¾“å…¥**ï¼Œç”¨äºé€†è½¬ä¸‹é‡‡æ ·å¯¼è‡´çš„ç©ºé—´å°ºå¯¸å‡å°ã€‚

**å¡«å……**ï¼šè½¬ç½®å·ç§¯ä¸­ï¼Œå¡«å……è¢«åº”**ç”¨äºè¾“å‡º**ï¼ˆå¸¸è§„å·ç§¯å°†å¡«å……åº”ç”¨äºè¾“å…¥ï¼‰ã€‚ä¾‹å¦‚ï¼Œå½“å°†é«˜å’Œå®½ä¸¤ä¾§å¡«å……æ•°æŒ‡å®šä¸º 1 æ—¶ï¼Œè½¬ç½®å·ç§¯è¾“å‡ºä¸­å°†**åˆ é™¤ç¬¬ä¸€å’Œæœ€åçš„è¡Œä¸åˆ—**ã€‚

![An Image](./img/trans_conv_stride2.svg)
**æ­¥å¹…**ï¼šè¢«æŒ‡å®šä¸ºä¸­é—´ç»“æœï¼ˆè¾“å‡ºï¼‰ï¼Œè€Œä¸æ˜¯è¾“å…¥ã€‚

**å¤šè¾“å…¥å’Œè¾“å‡ºé€šé“**ï¼šè½¬ç½®å·ç§¯ä¸å¸¸è§„å·ç§¯ä»¥**ç›¸åŒ**æ–¹å¼è¿ä½œã€‚

**çŸ©é˜µå˜æ¢**ï¼šè½¬ç½®å·ç§¯å±‚èƒ½å¤Ÿ**äº¤æ¢**å·ç§¯å±‚çš„**æ­£å‘ä¼ æ’­**å‡½æ•°å’Œ**åå‘ä¼ æ’­**å‡½æ•°ã€‚

## é£æ ¼è¿ç§»(style transfer)ğŸ”¥ğŸ”¥ğŸ”¥

æŠŠä¸€å¼ å›¾çš„â€œå†…å®¹â€å’Œå¦ä¸€å¼ å›¾çš„â€œé£æ ¼â€ç»“åˆï¼Œç”Ÿæˆä¸€å¼ â€œå†…å®¹ä¸å˜ä½†é£æ ¼å˜åŒ–â€çš„å›¾ã€‚

**å…¸å‹ä¾‹å­ï¼š**
ä½ æ‹äº†ä¸€å¼ çŒ«ï¼Œç”¨æ¢µé«˜ã€Šæ˜Ÿå¤œã€‹é£æ ¼è¿ç§»å®ƒï¼Œå°±èƒ½å¾—åˆ°ä¸€å¼ â€œæ˜Ÿå¤œé£æ ¼çŒ«â€ã€‚

**åº•å±‚åŸç†(ç®€åŒ–)ï¼š**

- å†…å®¹å›¾åƒä¿ç•™ç»“æ„å’Œå½¢çŠ¶ä¿¡æ¯ï¼ˆæ¯”å¦‚äººè„¸ã€ç‰©ä½“è½®å»“ï¼‰ã€‚
- é£æ ¼å›¾åƒæä¾›çº¹ç†ã€é¢œè‰²ã€ç¬”è§¦é£æ ¼ç­‰ã€‚
- é€šè¿‡ç¥ç»ç½‘ç»œï¼ˆå¦‚å·ç§¯ç¥ç»ç½‘ç»œ CNNï¼‰æå–ä¸¤è€…çš„ç‰¹å¾ï¼Œå¹¶ç»„åˆè¾“å‡ºã€‚

**å¸¸è§æŠ€æœ¯ï¼š**

- Gatys ç­‰äººæå‡ºçš„ç»å…¸ç¥ç»é£æ ¼è¿ç§»ï¼ˆåŸºäº VGG ç½‘ç»œï¼‰ã€‚
- æ›´å¿«çš„å®æ—¶é£æ ¼è¿ç§»ï¼ˆFast Style Transferï¼‰ç”¨äºç§»åŠ¨ç«¯ Appï¼ˆå¦‚ Prismaï¼‰ã€‚

**ç¼–ç å®ç°ï¼š**

```py
import torch
import torchvision
from torch import nn
from d2l import torch as d2l

content_img = d2l.Image.open('../img/rainier.jpg')
style_img = d2l.Image.open('../img/autumn-oak.jpg')

# é¢„å¤„ç†å’Œåå¤„ç†
rgb_mean = torch.tensor([0.485, 0.456, 0.406])
rgb_std = torch.tensor([0.229, 0.224, 0.225])

def preprocess(img, image_shape):
  transforms = torchvision.transforms.Compose([
    torchvision.transforms.Resize(image_shape),
    torchvision.transforms.ToTensor(),
    torchvision.transforms.Normalize(mean=rgb_mean, std=rgb_std)])
  return transforms(img).unsqueeze(0)

def postprocess(img):
  img = img[0].to(rgb_std.device)
  img = torch.clamp(img.permute(1, 2, 0) * rgb_std + rgb_mean, 0, 1)
  return torchvision.transforms.ToPILImage()(img.permute(2, 0, 1))

# æŠ½å–å›¾åƒç‰¹å¾
# ä½¿ç”¨åŸºäºImageNetæ•°æ®é›†é¢„è®­ç»ƒçš„VGG-19æ¨¡å‹æ¥æŠ½å–å›¾åƒç‰¹å¾
pretrained_net = torchvision.models.vgg19(pretrained=True)
style_layers, content_layers = [0, 5, 10, 19, 28], [25]
# æŠ½å–ç‰¹å®šå±‚ï¼Œæ–°å»ºç½‘ç»œæ¨¡å‹
net = nn.Sequential(*[pretrained_net.features[i] for i in range(max(content_layers + style_layers) + 1)])
# æŠ½å–ã€å­˜å‚¨ç›®æ ‡å†…å®¹å±‚å’Œé£æ ¼å±‚
def extract_features(X, content_layers, style_layers):
  contents = []
  styles = []
  for i in range(len(net)):
    X = net[i](X)
    if i in style_layers:
      styles.append(X)
    if i in content_layers:
      contents.append(X)
  return contents, styles

# æŠ½å–å†…å®¹ç‰¹å¾
def get_contents(image_shape, device):
  content_X = preprocess(content_img, image_shape).to(device)
  contents_Y, _ = extract_features(content_X, content_layers, style_layers)
  return content_X, contents_Y
# æŠ½å–é£æ ¼ç‰¹å¾
def get_styles(image_shape, device):
  style_X = preprocess(style_img, image_shape).to(device)
  _, styles_Y = extract_features(style_X, content_layers, style_layers)
  return style_X, styles_Y


# å®šä¹‰æŸå¤±å‡½æ•°(å†…å®¹æŸå¤± + é£æ ¼æŸå¤± + å…¨å˜åˆ†æŸå¤±)
# å†…å®¹æŸå¤±ï¼šä½¿åˆæˆå›¾åƒä¸å†…å®¹å›¾åƒåœ¨å†…å®¹ç‰¹å¾ä¸Šæ¥è¿‘
# é€šè¿‡å¹³æ–¹è¯¯å·®å‡½æ•°è¡¡é‡åˆæˆå›¾åƒä¸å†…å®¹å›¾åƒåœ¨ å†…å®¹ç‰¹å¾ å·®å¼‚
def content_loss(Y_hat, Y):
  # æˆ‘ä»¬ä»åŠ¨æ€è®¡ç®—æ¢¯åº¦çš„æ ‘ä¸­åˆ†ç¦»ç›®æ ‡ï¼š
  # è¿™æ˜¯ä¸€ä¸ªè§„å®šçš„å€¼ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªå˜é‡ã€‚
  return torch.square(Y_hat - Y.detach()).mean()

# é£æ ¼æŸå¤±ï¼šä½¿åˆæˆå›¾åƒä¸é£æ ¼å›¾åƒåœ¨é£æ ¼ç‰¹å¾ä¸Šæ¥è¿‘
# é€šè¿‡å¹³æ–¹è¯¯å·®å‡½æ•°è¡¡é‡åˆæˆå›¾åƒä¸é£æ ¼å›¾åƒåœ¨ é£æ ¼ç‰¹å¾ å·®å¼‚
def style_loss(Y_hat, gram_Y):
  return torch.square(gram(Y_hat) - gram_Y.detach()).mean()
# æ ¼æ‹‰å§†çŸ©é˜µï¼šè¡¨è¾¾é£æ ¼ç‰¹å¾ä¹‹é—´äº’ç›¸å…³æ€§ï¼Œè¡¨è¾¾é£æ ¼å±‚è¾“å‡ºçš„é£æ ¼
def gram(X):
  num_channels, n = X.shape[1], X.numel() // X.shape[1]
  X = X.reshape((num_channels, n))
  return torch.matmul(X, X.T) / (num_channels * n)

# å…¨å˜åˆ†æŸå¤±ï¼šå‡å°‘åˆæˆå›¾åƒä¸­å™ªç‚¹
# åˆæˆå›¾åƒä¼šæœ‰é«˜é¢‘å™ªç‚¹ï¼Œææš—æˆ–æäº®ã€‚å…¨å˜åˆ†å»å™ª(total variation denoising)ä½¿é‚»è¿‘åƒç´ å€¼ç›¸ä¼¼ã€‚
def tv_loss(Y_hat):
  return 0.5 * (torch.abs(Y_hat[:, :, 1:, :] - Y_hat[:, :, :-1, :]).mean() +
               torch.abs(Y_hat[:, :, :, 1:] - Y_hat[:, :, :, :-1]).mean())

# é£æ ¼è½¬ç§»æŸå¤±å‡½æ•°æ˜¯å†…å®¹æŸå¤±ã€é£æ ¼æŸå¤±å’Œæ€»å˜åŒ–æŸå¤±çš„åŠ æƒå’Œ
content_weight, style_weight, tv_weight = 1, 1e3, 10
def compute_loss(X, contents_Y_hat, styles_Y_hat, contents_Y, styles_Y_gram):
  # åˆ†åˆ«è®¡ç®—å†…å®¹æŸå¤±ã€é£æ ¼æŸå¤±å’Œå…¨å˜åˆ†æŸå¤±
  contents_l = [content_loss(Y_hat, Y) * content_weight for Y_hat, Y in zip(contents_Y_hat, contents_Y)]
  styles_l = [style_loss(Y_hat, Y) * style_weight for Y_hat, Y in zip(styles_Y_hat, styles_Y_gram)]
  tv_l = tv_loss(X) * tv_weight
  # å¯¹æ‰€æœ‰æŸå¤±æ±‚å’Œ
  l = sum(10 * styles_l + contents_l + [tv_l])
  return contents_l, styles_l, tv_l, l

# åˆå§‹åŒ–åˆæˆå›¾åƒï¼šé£æ ¼è¿ç§»ä¸­ï¼Œåˆæˆçš„å›¾åƒæ˜¯è®­ç»ƒæœŸé—´å”¯ä¸€éœ€è¦æ›´æ–°çš„å˜é‡ã€‚
# å®šä¹‰ä¸€ä¸ªæ¨¡å‹ï¼Œå°†åˆæˆå›¾åƒè§†ä¸ºæ¨¡å‹å‚æ•°ï¼Œæ¨¡å‹å‰å‘ä¼ æ’­åªéœ€è¿”å›æ¨¡å‹å‚æ•°å³å¯ã€‚
class SynthesizedImage(nn.Module):
  def __init__(self, img_shape, **kwargs):
    super(SynthesizedImage, self).__init__(**kwargs)
    self.weight = nn.Parameter(torch.rand(*img_shape))

  def forward(self):
    return self.weight

# åˆ›å»ºæ¨¡å‹å®ä¾‹ï¼Œå¹¶åˆå§‹åŒ–ä¸ºå†…å®¹å›¾åƒ X
def get_inits(X, device, lr, styles_Y):
  gen_img = SynthesizedImage(X.shape).to(device)
  gen_img.weight.data.copy_(X.data)
  trainer = torch.optim.Adam(gen_img.parameters(), lr=lr)
  styles_Y_gram = [gram(Y) for Y in styles_Y]
  return gen_img(), styles_Y_gram, trainer

# è®­ç»ƒ
def train(X, contents_Y, styles_Y, device, lr, num_epochs, lr_decay_epoch):
  X, styles_Y_gram, trainer = get_inits(X, device, lr, styles_Y)
  scheduler = torch.optim.lr_scheduler.StepLR(trainer, lr_decay_epoch, 0.8)
  animator = d2l.Animator(xlabel='epoch', ylabel='loss', xlim=[10, num_epochs],
                          legend=['content', 'style', 'TV'],
                          ncols=2, figsize=(7, 2.5))
  for epoch in range(num_epochs):
    trainer.zero_grad()
    contents_Y_hat, styles_Y_hat = extract_features(X, content_layers, style_layers)
    contents_l, styles_l, tv_l, l = compute_loss(X, contents_Y_hat, styles_Y_hat, contents_Y, styles_Y_gram)
    l.backward()
    trainer.step()
    scheduler.step()
    if (epoch + 1) % 10 == 0:
      animator.axes[1].imshow(postprocess(X))
      animator.add(epoch + 1, [float(sum(contents_l)), float(sum(styles_l)), float(tv_l)])
  return X

# gogogo!!!
device, image_shape = d2l.try_gpu(), (300, 450)
net = net.to(device)
content_X, contents_Y = get_contents(image_shape, device)
_, styles_Y = get_styles(image_shape, device)
output = train(content_X, contents_Y, styles_Y, device, 0.3, 500, 50)
```

**ä¸Šé¢ç¼–ç ä¸­æŠ€æœ¯æœ¯è¯­**ï¼š

- [æ ¼æ‹‰å§†çŸ©é˜µ(Gram Matrix)](/aiart/deep-learning/basic-concept.html#æ ¼æ‹‰å§†çŸ©é˜µ-gram-matrix)
- **å…¨å˜åˆ†æŸå¤±**å…¬å¼ä¸ºï¼š
  $$
  \sum_{i,j} |x_{i,j} - x_{i+1,j}| + |x_{i,j} - x_{i,j+1}|
  $$
