<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Transformer 架构 | AI边城</title>
    <meta name="description" content="A VitePress site">
    <meta name="generator" content="VitePress v1.6.3">
    <link rel="preload stylesheet" href="/assets/style.C0JqHpHB.css" as="style">
    <link rel="preload stylesheet" href="/vp-icons.css" as="style">
    
    <script type="module" src="/assets/app.D-HW7dAx.js"></script>
    <link rel="modulepreload" href="/assets/chunks/theme.B2ZgrB4E.js">
    <link rel="modulepreload" href="/assets/chunks/framework._eNwL97Z.js">
    <link rel="modulepreload" href="/assets/aiart_deep-learning_transformer.md.9gX9HOBB.lean.js">
    <link rel="icon" href="/logo.svg">
    <link rel="icon" href="https://avatars.githubusercontent.com/u/13958395?v=4">
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
    <link rel="preload" href="/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
  </head>
  <body>
    <div id="app"><div class="VPApp" data-v-66aaf361><!--[--><span tabindex="-1" data-v-41ee5c6f></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-41ee5c6f>Skip to content</a><!--]--><!----><!--[--><!--]--><header class="VPNav nav-bar" data-v-66aaf361 data-v-7e6fc192><div class="VPNavBar" data-v-7e6fc192 data-v-d70b65a3><div class="container" data-v-d70b65a3><a class="VPNavBarTitle" href="/" data-v-d70b65a3 data-v-cd01eb9f><!--[--><!--[--><!--[--><!--[--><div class="nav-bar-title" data-v-429880b3><div class="bar-logo" data-v-429880b3><img src="https://avatars.githubusercontent.com/u/13958395?v=4" alt="心水喵论" data-v-429880b3></div><div class="title-text" data-v-429880b3>AI边城</div></div><!--]--><!--]--><!--]--><!--]--></a><div class="content" data-v-d70b65a3><!----><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-d70b65a3 data-v-93035912><span id="main-nav-aria-label" class="visually-hidden" data-v-93035912>Main Navigation</span><!--[--><!--[--><a class="vt-link link VPNavBarMenuLink" href="/" data-v-93035912 data-v-1bf11343><!--[-->首页<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link VPNavBarMenuLink active" href="/aiart/machine-learning/overview.html" data-v-93035912 data-v-1bf11343><!--[-->AI·文艺<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link VPNavBarMenuLink" href="/vue/vue3/source-code.html" data-v-93035912 data-v-1bf11343><!--[-->Vue·周边<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link VPNavBarMenuLink" href="/vite/" data-v-93035912 data-v-1bf11343><!--[-->Vite·Press<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link VPNavBarMenuLink" href="/node/core/" data-v-93035912 data-v-1bf11343><!--[-->Node·周边<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link VPNavBarMenuLink" href="/basic/javascript/" data-v-93035912 data-v-1bf11343><!--[-->Js·Ts<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link VPNavBarMenuLink" href="/mac/setting.html" data-v-93035912 data-v-1bf11343><!--[-->Mac·Linux<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link VPNavBarMenuLink" href="/mathstat/math/number-theory.html" data-v-93035912 data-v-1bf11343><!--[-->数学·统计<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link VPNavBarMenuLink" href="/investment/stock-prediction.html" data-v-93035912 data-v-1bf11343><!--[-->投资·金融<!--]--><!----><!----></a><!--]--><!--]--><!----></nav><div class="VPNavBarAppearance appearance" data-v-d70b65a3 data-v-5b1af3f3><button class="vt-switch vt-switch-appearance" type="button" role="switch" aria-label="Toggle dark mode" aria-checked="false" data-v-5b1af3f3><span class="vt-switch-check"><span class="vt-switch-icon"><!--[--><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="vt-switch-appearance-sun"><path d="M12,18c-3.3,0-6-2.7-6-6s2.7-6,6-6s6,2.7,6,6S15.3,18,12,18zM12,8c-2.2,0-4,1.8-4,4c0,2.2,1.8,4,4,4c2.2,0,4-1.8,4-4C16,9.8,14.2,8,12,8z"></path><path d="M12,4c-0.6,0-1-0.4-1-1V1c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,3.6,12.6,4,12,4z"></path><path d="M12,24c-0.6,0-1-0.4-1-1v-2c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,23.6,12.6,24,12,24z"></path><path d="M5.6,6.6c-0.3,0-0.5-0.1-0.7-0.3L3.5,4.9c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C6.2,6.5,5.9,6.6,5.6,6.6z"></path><path d="M19.8,20.8c-0.3,0-0.5-0.1-0.7-0.3l-1.4-1.4c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C20.3,20.7,20,20.8,19.8,20.8z"></path><path d="M3,13H1c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S3.6,13,3,13z"></path><path d="M23,13h-2c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S23.6,13,23,13z"></path><path d="M4.2,20.8c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C4.7,20.7,4.5,20.8,4.2,20.8z"></path><path d="M18.4,6.6c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C18.9,6.5,18.6,6.6,18.4,6.6z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="vt-switch-appearance-moon"><path d="M12.1,22c-0.3,0-0.6,0-0.9,0c-5.5-0.5-9.5-5.4-9-10.9c0.4-4.8,4.2-8.6,9-9c0.4,0,0.8,0.2,1,0.5c0.2,0.3,0.2,0.8-0.1,1.1c-2,2.7-1.4,6.4,1.3,8.4c2.1,1.6,5,1.6,7.1,0c0.3-0.2,0.7-0.3,1.1-0.1c0.3,0.2,0.5,0.6,0.5,1c-0.2,2.7-1.5,5.1-3.6,6.8C16.6,21.2,14.4,22,12.1,22zM9.3,4.4c-2.9,1-5,3.6-5.2,6.8c-0.4,4.4,2.8,8.3,7.2,8.7c2.1,0.2,4.2-0.4,5.8-1.8c1.1-0.9,1.9-2.1,2.4-3.4c-2.5,0.9-5.3,0.5-7.5-1.1C9.2,11.4,8.1,7.7,9.3,4.4z"></path></svg><!--]--></span></span></button></div><div class="vt-social-links VPNavBarSocialLinks social-links" data-v-d70b65a3 data-v-59ed7a28><!--[--><a class="vt-social-link is-small" href="https://github.com/wswplay/wswplay.github.io" title="github" target="_blank" rel="noopener noreferrer"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="vt-social-link-icon"><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"></path></svg><span class="visually-hidden">github</span></a><!--]--></div><div class="vt-flyout VPNavBarExtra extra" data-v-d70b65a3 data-v-696b3a83><button type="button" class="vt-flyout-button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation"><!--[--><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="vt-flyout-button-icon"><circle cx="12" cy="12" r="2"></circle><circle cx="19" cy="12" r="2"></circle><circle cx="5" cy="12" r="2"></circle></svg><!--]--></button><div class="vt-flyout-menu"><div class="vt-menu"><!----><!--[--><!--[--><div class="vt-menu-group" data-v-696b3a83><div class="vt-menu-item item" data-v-696b3a83><p class="vt-menu-label" data-v-696b3a83>Appearance</p><div class="vt-menu-action action" data-v-696b3a83><button class="vt-switch vt-switch-appearance" type="button" role="switch" aria-label="Toggle dark mode" aria-checked="false" data-v-696b3a83><span class="vt-switch-check"><span class="vt-switch-icon"><!--[--><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="vt-switch-appearance-sun"><path d="M12,18c-3.3,0-6-2.7-6-6s2.7-6,6-6s6,2.7,6,6S15.3,18,12,18zM12,8c-2.2,0-4,1.8-4,4c0,2.2,1.8,4,4,4c2.2,0,4-1.8,4-4C16,9.8,14.2,8,12,8z"></path><path d="M12,4c-0.6,0-1-0.4-1-1V1c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,3.6,12.6,4,12,4z"></path><path d="M12,24c-0.6,0-1-0.4-1-1v-2c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,23.6,12.6,24,12,24z"></path><path d="M5.6,6.6c-0.3,0-0.5-0.1-0.7-0.3L3.5,4.9c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C6.2,6.5,5.9,6.6,5.6,6.6z"></path><path d="M19.8,20.8c-0.3,0-0.5-0.1-0.7-0.3l-1.4-1.4c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C20.3,20.7,20,20.8,19.8,20.8z"></path><path d="M3,13H1c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S3.6,13,3,13z"></path><path d="M23,13h-2c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S23.6,13,23,13z"></path><path d="M4.2,20.8c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C4.7,20.7,4.5,20.8,4.2,20.8z"></path><path d="M18.4,6.6c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C18.9,6.5,18.6,6.6,18.4,6.6z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="vt-switch-appearance-moon"><path d="M12.1,22c-0.3,0-0.6,0-0.9,0c-5.5-0.5-9.5-5.4-9-10.9c0.4-4.8,4.2-8.6,9-9c0.4,0,0.8,0.2,1,0.5c0.2,0.3,0.2,0.8-0.1,1.1c-2,2.7-1.4,6.4,1.3,8.4c2.1,1.6,5,1.6,7.1,0c0.3-0.2,0.7-0.3,1.1-0.1c0.3,0.2,0.5,0.6,0.5,1c-0.2,2.7-1.5,5.1-3.6,6.8C16.6,21.2,14.4,22,12.1,22zM9.3,4.4c-2.9,1-5,3.6-5.2,6.8c-0.4,4.4,2.8,8.3,7.2,8.7c2.1,0.2,4.2-0.4,5.8-1.8c1.1-0.9,1.9-2.1,2.4-3.4c-2.5,0.9-5.3,0.5-7.5-1.1C9.2,11.4,8.1,7.7,9.3,4.4z"></path></svg><!--]--></span></span></button></div></div></div><div class="vt-menu-group" data-v-696b3a83><div class="vt-menu-item item" data-v-696b3a83><div class="vt-social-links social-links" data-v-696b3a83><!--[--><a class="vt-social-link is-small" href="https://github.com/wswplay/wswplay.github.io" title="github" target="_blank" rel="noopener noreferrer"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="vt-social-link-icon"><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"></path></svg><span class="visually-hidden">github</span></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><button type="button" class="vt-hamburger VPNavBarHamburger hamburger" aria-label="Mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-d70b65a3 data-v-e27ae2fa><span class="vt-hamburger-container"><span class="vt-hamburger-top"></span><span class="vt-hamburger-middle"></span><span class="vt-hamburger-bottom"></span></span></button></div></div></div><!----></header><div class="VPLocalNav" data-v-66aaf361 data-v-17af4d12><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-17af4d12><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="menu-icon" data-v-17af4d12><path d="M17,11H3c-0.6,0-1-0.4-1-1s0.4-1,1-1h14c0.6,0,1,0.4,1,1S17.6,11,17,11z"></path><path d="M21,7H3C2.4,7,2,6.6,2,6s0.4-1,1-1h18c0.6,0,1,0.4,1,1S21.6,7,21,7z"></path><path d="M21,15H3c-0.6,0-1-0.4-1-1s0.4-1,1-1h18c0.6,0,1,0.4,1,1S21.6,15,21,15z"></path><path d="M17,19H3c-0.6,0-1-0.4-1-1s0.4-1,1-1h14c0.6,0,1,0.4,1,1S17.6,19,17,19z"></path></svg><span class="menu-text" data-v-17af4d12>Menu</span></button><div class="VPLocalNavOutlineDropdown" style="--vt-vh:0px;" data-v-17af4d12 data-v-bf487e00><button class="" data-v-bf487e00>本页目录 <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="icon" data-v-bf487e00><path d="M9,19c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l5.3-5.3L8.3,6.7c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l6,6c0.4,0.4,0.4,1,0,1.4l-6,6C9.5,18.9,9.3,19,9,19z"></path></svg></button><!----></div></div><aside class="VPSidebar" data-v-66aaf361 data-v-c52861ec><nav id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-c52861ec><!--[--><!--]--><span id="sidebar-aria-label" class="visually-hidden" data-v-c52861ec>Sidebar Navigation</span><!--[--><div class="group" data-v-c52861ec><section class="VPSidebarGroup" data-v-c52861ec data-v-1d117664><div class="title" data-v-1d117664><h2 class="title-text" data-v-1d117664>机器学习<!----></h2></div><!--[--><a class="link" href="/aiart/machine-learning/overview.html" data-v-1d117664 data-v-683aeeb6><p class="link-text" data-v-683aeeb6>概览<!----></p></a><a class="link" href="/aiart/machine-learning/conda.html" data-v-1d117664 data-v-683aeeb6><p class="link-text" data-v-683aeeb6>conda<!----></p></a><a class="link" href="/aiart/machine-learning/kaggle.html" data-v-1d117664 data-v-683aeeb6><p class="link-text" data-v-683aeeb6>kaggle<!----></p></a><!--]--></section></div><div class="group" data-v-c52861ec><section class="VPSidebarGroup" data-v-c52861ec data-v-1d117664><div class="title" data-v-1d117664><h2 class="active title-text" data-v-1d117664>深度学习<!----></h2></div><!--[--><a class="link" href="/aiart/deep-learning/overview.html" data-v-1d117664 data-v-683aeeb6><p class="link-text" data-v-683aeeb6>概览<!----></p></a><a class="link" href="/aiart/deep-learning/mathematics.html" data-v-1d117664 data-v-683aeeb6><p class="link-text" data-v-683aeeb6>数学基础<!----></p></a><a class="link" href="/aiart/deep-learning/basic-concept.html" data-v-1d117664 data-v-683aeeb6><p class="link-text" data-v-683aeeb6>基本概念<!----></p></a><a class="link" href="/aiart/deep-learning/linear-regression.html" data-v-1d117664 data-v-683aeeb6><p class="link-text" data-v-683aeeb6>线性回归<!----></p></a><a class="link" href="/aiart/deep-learning/cnn.html" data-v-1d117664 data-v-683aeeb6><p class="link-text" data-v-683aeeb6>卷积神经网络(CNN)<!----></p></a><a class="link" href="/aiart/deep-learning/rnn.html" data-v-1d117664 data-v-683aeeb6><p class="link-text" data-v-683aeeb6>循环神经网络(RNN)<!----></p></a><a class="link" href="/aiart/deep-learning/rnn-modern.html" data-v-1d117664 data-v-683aeeb6><p class="link-text" data-v-683aeeb6>现代RNN<!----></p></a><a class="link" href="/aiart/deep-learning/attention-mechanisms.html" data-v-1d117664 data-v-683aeeb6><p class="link-text" data-v-683aeeb6>注意力机制(attention)<!----></p></a><a class="link" href="/aiart/deep-learning/transformer.html" data-v-1d117664 data-v-683aeeb6><p class="link-text" data-v-683aeeb6>Transformer架构<!----></p></a><a class="link" href="/aiart/deep-learning/nlp.html" data-v-1d117664 data-v-683aeeb6><p class="link-text" data-v-683aeeb6>自然语言处理(NLP)<!----></p></a><a class="link" href="/aiart/deep-learning/gpt.html" data-v-1d117664 data-v-683aeeb6><p class="link-text" data-v-683aeeb6>GPT<!----></p></a><a class="link" href="/aiart/deep-learning/computer-vision.html" data-v-1d117664 data-v-683aeeb6><p class="link-text" data-v-683aeeb6>计算机视觉(CV)<!----></p></a><!--]--></section></div><div class="group" data-v-c52861ec><section class="VPSidebarGroup" data-v-c52861ec data-v-1d117664><div class="title" data-v-1d117664><h2 class="title-text" data-v-1d117664>Huggingface<!----></h2></div><!--[--><a class="link" href="/aiart/huggingface/config.html" data-v-1d117664 data-v-683aeeb6><p class="link-text" data-v-683aeeb6>基础配置<!----></p></a><a class="link" href="/aiart/huggingface/diffusers.html" data-v-1d117664 data-v-683aeeb6><p class="link-text" data-v-683aeeb6>Diffusers<!----></p></a><a class="link" href="/aiart/huggingface/accelerate.html" data-v-1d117664 data-v-683aeeb6><p class="link-text" data-v-683aeeb6>Accelerate<!----></p></a><a class="link" href="/aiart/huggingface/pytorch.html" data-v-1d117664 data-v-683aeeb6><p class="link-text" data-v-683aeeb6>Pytorch<!----></p></a><!--]--></section></div><div class="group" data-v-c52861ec><section class="VPSidebarGroup" data-v-c52861ec data-v-1d117664><div class="title" data-v-1d117664><h2 class="title-text" data-v-1d117664>ComfyUI<!----></h2></div><!--[--><a class="link" href="/aiart/comfyui/basic.html" data-v-1d117664 data-v-683aeeb6><p class="link-text" data-v-683aeeb6>基本概念<!----></p></a><!--]--></section></div><div class="group" data-v-c52861ec><section class="VPSidebarGroup" data-v-c52861ec data-v-1d117664><div class="title" data-v-1d117664><h2 class="title-text" data-v-1d117664>Python<!----></h2></div><!--[--><a class="link" href="/aiart/python/basic-info.html" data-v-1d117664 data-v-683aeeb6><p class="link-text" data-v-683aeeb6>安装与设置<!----></p></a><a class="link" href="/aiart/python/pip.html" data-v-1d117664 data-v-683aeeb6><p class="link-text" data-v-683aeeb6>pip<!----></p></a><a class="link" href="/aiart/python/oop.html" data-v-1d117664 data-v-683aeeb6><p class="link-text" data-v-683aeeb6>面向对象编程(OOP)<!----></p></a><a class="link" href="/aiart/python/pandas.html" data-v-1d117664 data-v-683aeeb6><p class="link-text" data-v-683aeeb6>Pandas<!----></p></a><a class="link" href="/aiart/python/pil.html" data-v-1d117664 data-v-683aeeb6><p class="link-text" data-v-683aeeb6>PIL<!----></p></a><!--]--></section></div><div class="group" data-v-c52861ec><section class="VPSidebarGroup" data-v-c52861ec data-v-1d117664><div class="title" data-v-1d117664><h2 class="title-text" data-v-1d117664>AI语录<!----></h2></div><!--[--><a class="link" href="/aiart/three-body.html" data-v-1d117664 data-v-683aeeb6><p class="link-text" data-v-683aeeb6>概率、记忆与意识<!----></p></a><a class="link" href="/aiart/chat-gpt.html" data-v-1d117664 data-v-683aeeb6><p class="link-text" data-v-683aeeb6>ChatGPT问答集锦<!----></p></a><!--]--></section></div><div class="group" data-v-c52861ec><section class="VPSidebarGroup" data-v-c52861ec data-v-1d117664><div class="title" data-v-1d117664><h2 class="title-text" data-v-1d117664>文艺<!----></h2></div><!--[--><a class="link" href="/aiart/sunzi-war-art.html" data-v-1d117664 data-v-683aeeb6><p class="link-text" data-v-683aeeb6>孙子兵法<!----></p></a><!--]--></section></div><!--]--><!--[--><!--]--></nav></aside><div id="VPContent" class="VPContent has-sidebar" data-v-66aaf361 data-v-caffc240><div class="VPContentDoc has-aside has-sidebar" data-v-caffc240 data-v-c9771c35><div class="container" data-v-c9771c35><div class="aside" data-v-c9771c35><div class="aside-container" data-v-c9771c35><!--[--><!--]--><div class="VPContentDocOutline" data-v-c9771c35 data-v-b24b265a><div class="outline-marker" data-v-b24b265a></div><div class="outline-title" data-v-b24b265a>本页目录</div><nav aria-labelledby="doc-outline-aria-label" data-v-b24b265a><span id="doc-outline-aria-label" class="visually-hidden" data-v-b24b265a>Table of Contents for current page</span><ul class="root" data-v-b24b265a data-v-ef8882f0><!--[--><li data-v-ef8882f0><a class="outline-link" href="#模型" style="" data-v-ef8882f0>模型</a><!----></li><li data-v-ef8882f0><a class="outline-link" href="#基于位置的前馈网络-ffn" style="" data-v-ef8882f0>基于位置的前馈网络(FFN)</a><ul class="nested" data-v-ef8882f0 data-v-ef8882f0><!--[--><li data-v-ef8882f0><a class="outline-link" href="#基于位置" style="" data-v-ef8882f0>基于位置</a><!----></li><li data-v-ef8882f0><a class="outline-link" href="#前馈网络" style="" data-v-ef8882f0>前馈网络</a><!----></li><!--]--></ul></li><li data-v-ef8882f0><a class="outline-link" href="#残差连接和层规范化-add-norm" style="" data-v-ef8882f0>残差连接和层规范化(add&amp;norm)</a><!----></li><li data-v-ef8882f0><a class="outline-link" href="#实现编码器" style="" data-v-ef8882f0>实现编码器</a><!----></li><li data-v-ef8882f0><a class="outline-link" href="#解码器实现" style="" data-v-ef8882f0>解码器实现</a><!----></li><li data-v-ef8882f0><a class="outline-link" href="#训练" style="" data-v-ef8882f0>训练</a><!----></li><!--]--></ul></nav></div><!--[--><!--[--><!--[--><div class="poetry" data-v-5b0a57b7>给每一条河每一座山取一个温暖的名字，我有一所房子，面朝大海，春暖花开。</div><!--]--><!--]--><!--]--><!----><!--[--><!--]--></div></div><div class="content" data-v-c9771c35><!--[--><!--]--><main data-v-c9771c35><div style="position:relative;" class="vt-doc aiart" data-v-c9771c35><div><h1 id="transformer-架构" tabindex="-1">Transformer 架构 <a class="header-anchor" href="#transformer-架构" aria-label="Permalink to &quot;Transformer 架构&quot;">​</a></h1><p>自注意力同时具有<strong>并行计算</strong>和<strong>最短最大路径长度</strong>这两个优势，因此使用自注意力来设计深度架构是很有吸引力的。</p><p><code>Transformer</code> 模型<strong>完全基于注意力机制</strong>，没有任何卷积层或循环神经网络层。</p><p>尽管 <code>Transformer</code> 最初是应用于在<strong>文本数据</strong>上的序列到序列学习，但现在已经推广到各种现代的深度学习中，例如<strong>语言、视觉、语音和强化学习</strong>领域。</p><h2 id="模型" tabindex="-1">模型 <a class="header-anchor" href="#模型" aria-label="Permalink to &quot;模型&quot;">​</a></h2><p><img src="/assets/transformer.C26fAZXE.svg" alt="An Image"> Transformer 是<strong>编码器－解码器</strong>架构实例，基于<strong>自注意力模块叠加</strong>而成。</p><p><strong>源序列</strong>(输入)嵌入<sup>embedding</sup>和<strong>目标序列</strong>(输出)嵌入，都加上<strong>位置编码</strong><sup>positional encoding</sup>，分别输入到编码器和解码器中。</p><p><strong>编码器</strong>：由多个相同层叠加而成的，<strong>每个层都有两个子层</strong>。</p><ul><li>第一个子层：<strong>多头自注意力</strong><sup>multi-head self-attention</sup>汇聚。</li><li>第二个子层：<strong>逐位前馈网络</strong><sup>positionwise feed-forward network：FFN</sup>。</li><li>计算时，查询、键和值都<strong>来自前一个编码器</strong>层输出，每个子层都采用了<strong>残差连接</strong><sup>residual connection</sup>，在残差连接加法计算后，应用<strong>层规范化</strong><sup>layer normalization</sup>。</li></ul><p><strong>解码器</strong>：也是由多个相同层叠加而成，同样使用了<strong>残差连接</strong>和<strong>层规范化</strong>。</p><ul><li>第三个子层：插入在这两个子层之间，称为<strong>编码器－解码器注意力</strong><sup>encoder-decoder attention</sup>层：<strong>查询</strong>来自<strong>前一个解码器</strong>层输出，而<strong>键和值</strong>来自<strong>整个编码器</strong>输出。</li><li><strong>解码器自注意力</strong>中，查询、键和值都来<strong>自上一个解码器</strong>层输出。但解码器中每个位置只能考虑该位置之前的所有位置。这种<strong>掩蔽</strong><sup>masked</sup>注意力保留了<a href="/aiart/deep-learning/rnn.html#自回归模型"><strong>自回归</strong><sup>auto-regressive</sup></a>属性，确保预测仅依赖于已生成的输出词元。</li></ul><h2 id="基于位置的前馈网络-ffn" tabindex="-1">基于位置的前馈网络(FFN) <a class="header-anchor" href="#基于位置的前馈网络-ffn" aria-label="Permalink to &quot;基于位置的前馈网络(FFN)&quot;">​</a></h2><p><code>Transformer</code> 模型中基于位置的前馈网络使用同一个<strong>多层感知机</strong>，作用是对所有序列<strong>位置表示进行转换</strong>。</p><div class="language-py line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">py</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#F97583;">class</span><span style="color:#B392F0;"> PositionWiseFFN</span><span style="color:#E1E4E8;">(</span><span style="color:#B392F0;">nn</span><span style="color:#E1E4E8;">.</span><span style="color:#B392F0;">Module</span><span style="color:#E1E4E8;">):</span></span>
<span class="line"><span style="color:#F97583;">  def</span><span style="color:#79B8FF;"> __init__</span><span style="color:#E1E4E8;">(self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs, </span><span style="color:#F97583;">**</span><span style="color:#E1E4E8;">kwargs):</span></span>
<span class="line"><span style="color:#79B8FF;">    super</span><span style="color:#E1E4E8;">(PositionWiseFFN, </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">).</span><span style="color:#79B8FF;">__init__</span><span style="color:#E1E4E8;">(</span><span style="color:#F97583;">**</span><span style="color:#E1E4E8;">kwargs)</span></span>
<span class="line"><span style="color:#79B8FF;">    self</span><span style="color:#E1E4E8;">.dense1 </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> nn.Linear(ffn_num_input, ffn_num_hiddens)</span></span>
<span class="line"><span style="color:#79B8FF;">    self</span><span style="color:#E1E4E8;">.relu </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> nn.ReLU()</span></span>
<span class="line"><span style="color:#79B8FF;">    self</span><span style="color:#E1E4E8;">.dense2 </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> nn.Linear(ffn_num_hiddens, ffn_num_outputs)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583;">  def</span><span style="color:#B392F0;"> forward</span><span style="color:#E1E4E8;">(self, X):</span></span>
<span class="line"><span style="color:#F97583;">    return</span><span style="color:#79B8FF;"> self</span><span style="color:#E1E4E8;">.dense2(</span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.relu(</span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.dense1(X)))</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 实例化</span></span>
<span class="line"><span style="color:#E1E4E8;">ffn </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> PositionWiseFFN(</span><span style="color:#79B8FF;">4</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">4</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">8</span><span style="color:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br></div></div><ul><li>在标准 <code>Transformer</code> 中，<code>ffn_num_outputs</code> 通常与模型主维度（如输入维度）一致，而隐藏层是主维度 4 倍（如输入 512 → 隐藏层 2048）。</li><li>本例输出维度（8）与输入（4）不同，或破坏残差连接条件（需 <code>输入维度 == 输出维度</code>）。</li></ul><h3 id="基于位置" tabindex="-1">基于位置 <a class="header-anchor" href="#基于位置" aria-label="Permalink to &quot;基于位置&quot;">​</a></h3><p><code>nn.Linear</code> 默认行为：<strong>位置独立</strong>计算，不混合不同位置信息。</p><ul><li>当输入是 (<code>B</code>=batch_size, <code>T</code>=sequence_length, <code>D_in</code>=输入特征维度) 时，对 <code>T</code> 的每个位置独立计算，即 &quot;位置相关&quot;。</li></ul><table tabindex="0"><thead><tr><th>层类型</th><th>位置独立</th><th>原因</th></tr></thead><tbody><tr><td><code>nn.Linear</code></td><td>✅ 是</td><td>默认独立处理 <code>(B, T, D)</code> 中每个 <code>T</code>。</td></tr><tr><td><code>nn.Conv1d</code></td><td>❌ 否</td><td>滑动窗口混合相邻位置的信息。</td></tr><tr><td><code>nn.LSTM</code></td><td>❌ 否</td><td>隐状态依赖前序位置的计算结果。</td></tr><tr><td><code>MultiHeadAttention</code></td><td>❌ 否</td><td>显式计算所有位置间的注意力权重。</td></tr><tr><td><code>PositionWiseFFN</code></td><td>✅ 是</td><td><code>nn.Linear</code> 堆叠，独立处理每个位置。</td></tr></tbody></table><h3 id="前馈网络" tabindex="-1">前馈网络 <a class="header-anchor" href="#前馈网络" aria-label="Permalink to &quot;前馈网络&quot;">​</a></h3><p><strong>前馈网络</strong><sup>Feedforward Neural Network, FNN</sup> 是一种最基本神经网络结构。其核心特点是：<strong>数据单向流动（从输入层 → 隐藏层 → 输出层），没有循环或反馈连接</strong>。</p><ul><li><strong>“Feed”</strong>：数据被“喂入”网络。</li><li><strong>“Forward”</strong>：数据只向前流动，不反向或循环。</li></ul><p><strong>数学表达</strong></p><p>给定输入 <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.09ex;" xmlns="http://www.w3.org/2000/svg" width="12.708ex" height="2.004ex" role="img" focusable="false" viewBox="0 -846 5616.8 886" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D417" d="M327 0Q306 3 174 3Q52 3 43 0H33V62H98L162 63L360 333L157 624H48V686H59Q80 683 217 683Q368 683 395 686H408V624H335L393 540L452 458L573 623Q573 624 528 624H483V686H494Q515 683 646 683Q769 683 778 686H787V624H658L575 511Q493 398 493 397L508 376Q522 356 553 312T611 229L727 62H835V0H824Q803 3 667 3Q516 3 489 0H476V62H513L549 63L401 274L247 63Q247 62 292 62H338V0H327Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(1146.8,0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z" style="stroke-width:3;"></path></g><g data-mml-node="msup" transform="translate(2091.6,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="211D" d="M17 665Q17 672 28 683H221Q415 681 439 677Q461 673 481 667T516 654T544 639T566 623T584 607T597 592T607 578T614 565T618 554L621 548Q626 530 626 497Q626 447 613 419Q578 348 473 326L455 321Q462 310 473 292T517 226T578 141T637 72T686 35Q705 30 705 16Q705 7 693 -1H510Q503 6 404 159L306 310H268V183Q270 67 271 59Q274 42 291 38Q295 37 319 35Q344 35 353 28Q362 17 353 3L346 -1H28Q16 5 16 16Q16 35 55 35Q96 38 101 52Q106 60 106 341T101 632Q95 645 55 648Q17 648 17 665ZM241 35Q238 42 237 45T235 78T233 163T233 337V621L237 635L244 648H133Q136 641 137 638T139 603T141 517T141 341Q141 131 140 89T134 37Q133 36 133 35H241ZM457 496Q457 540 449 570T425 615T400 634T377 643Q374 643 339 648Q300 648 281 635Q271 628 270 610T268 481V346H284Q327 346 375 352Q421 364 439 392T457 496ZM492 537T492 496T488 427T478 389T469 371T464 361Q464 360 465 360Q469 360 497 370Q593 400 593 495Q593 592 477 630L457 637L461 626Q474 611 488 561Q492 537 492 496ZM464 243Q411 317 410 317Q404 317 401 315Q384 315 370 312H346L526 35H619L606 50Q553 109 464 243Z" style="stroke-width:3;"></path></g></g><g data-mml-node="TeXAtom" transform="translate(755,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(759,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1537,0)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(2241,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(3019,0)"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z" style="stroke-width:3;"></path></g></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi mathvariant="bold">X</mi></mrow><mo>∈</mo><msup><mrow data-mjx-texclass="ORD"><mi mathvariant="double-struck">R</mi></mrow><mrow data-mjx-texclass="ORD"><mi>B</mi><mo>×</mo><mi>T</mi><mo>×</mo><mi>D</mi></mrow></msup></math></mjx-assistive-mml></mjx-container>（B=批大小，T=序列长度，D=特征维度）：</p><mjx-container tabindex="0" class="MathJax" jax="SVG" display="true" style="direction:ltr;display:block;text-align:center;margin:1em 0;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.566ex;" xmlns="http://www.w3.org/2000/svg" width="40.29ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 17808.1 1000" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mtext"><path data-c="46" d="M128 619Q121 626 117 628T101 631T58 634H25V680H582V676Q584 670 596 560T610 444V440H570V444Q563 493 561 501Q555 538 543 563T516 601T477 622T431 631T374 633H334H286Q252 633 244 631T233 621Q232 619 232 490V363H284Q287 363 303 363T327 364T349 367T372 373T389 385Q407 403 410 459V480H450V200H410V221Q407 276 389 296Q381 303 371 307T348 313T327 316T303 317T284 317H232V189L233 61Q240 54 245 52T270 48T333 46H360V0H348Q324 3 182 3Q51 3 36 0H25V46H58Q100 47 109 49T128 61V619Z" style="stroke-width:3;"></path><path data-c="46" d="M128 619Q121 626 117 628T101 631T58 634H25V680H582V676Q584 670 596 560T610 444V440H570V444Q563 493 561 501Q555 538 543 563T516 601T477 622T431 631T374 633H334H286Q252 633 244 631T233 621Q232 619 232 490V363H284Q287 363 303 363T327 364T349 367T372 373T389 385Q407 403 410 459V480H450V200H410V221Q407 276 389 296Q381 303 371 307T348 313T327 316T303 317T284 317H232V189L233 61Q240 54 245 52T270 48T333 46H360V0H348Q324 3 182 3Q51 3 36 0H25V46H58Q100 47 109 49T128 61V619Z" transform="translate(653,0)" style="stroke-width:3;"></path><path data-c="4E" d="M42 46Q74 48 94 56T118 69T128 86V634H124Q114 637 52 637H25V683H232L235 680Q237 679 322 554T493 303L578 178V598Q572 608 568 613T544 627T492 637H475V683H483Q498 680 600 680Q706 680 715 683H724V637H707Q634 633 622 598L621 302V6L614 0H600Q585 0 582 3T481 150T282 443T171 605V345L172 86Q183 50 257 46H274V0H265Q250 3 150 3Q48 3 33 0H25V46H42Z" transform="translate(1306,0)" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(2056,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(2445,0)"><g data-mml-node="mi"><path data-c="1D417" d="M327 0Q306 3 174 3Q52 3 43 0H33V62H98L162 63L360 333L157 624H48V686H59Q80 683 217 683Q368 683 395 686H408V624H335L393 540L452 458L573 623Q573 624 528 624H483V686H494Q515 683 646 683Q769 683 778 686H787V624H658L575 511Q493 398 493 397L508 376Q522 356 553 312T611 229L727 62H835V0H824Q803 3 667 3Q516 3 489 0H476V62H513L549 63L401 274L247 63Q247 62 292 62H338V0H327Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(3314,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(3980.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" style="stroke-width:3;"></path></g><g data-mml-node="msub" transform="translate(5036.6,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D416" d="M915 686L1052 683Q1142 683 1157 686H1164V624H1073L957 320Q930 249 900 170T855 52T839 10Q834 0 826 -5Q821 -7 799 -7H792Q777 -7 772 -5T759 10Q759 11 748 39T716 122T676 228L594 442L512 228Q486 159 455 78Q433 19 428 9T416 -5Q411 -7 389 -7H379Q356 -7 349 10Q349 12 334 51T288 170T231 320L116 624H24V686H35Q44 683 183 683Q331 683 355 686H368V624H323Q278 624 278 623L437 207L499 369L561 531L526 624H434V686H445Q454 683 593 683Q741 683 765 686H778V624H733Q688 624 688 623L847 207Q848 207 927 415T1006 624H905V686H915Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mn" transform="translate(1222,-150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(6884.3,0)"><path data-c="22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mtext" transform="translate(7384.6,0)"><path data-c="52" d="M130 622Q123 629 119 631T103 634T60 637H27V683H202H236H300Q376 683 417 677T500 648Q595 600 609 517Q610 512 610 501Q610 468 594 439T556 392T511 361T472 343L456 338Q459 335 467 332Q497 316 516 298T545 254T559 211T568 155T578 94Q588 46 602 31T640 16H645Q660 16 674 32T692 87Q692 98 696 101T712 105T728 103T732 90Q732 59 716 27T672 -16Q656 -22 630 -22Q481 -16 458 90Q456 101 456 163T449 246Q430 304 373 320L363 322L297 323H231V192L232 61Q238 51 249 49T301 46H334V0H323Q302 3 181 3Q59 3 38 0H27V46H60Q102 47 111 49T130 61V622ZM491 499V509Q491 527 490 539T481 570T462 601T424 623T362 636Q360 636 340 636T304 637H283Q238 637 234 628Q231 624 231 492V360H289Q390 360 434 378T489 456Q491 467 491 499Z" style="stroke-width:3;"></path><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" transform="translate(736,0)" style="stroke-width:3;"></path><path data-c="4C" d="M128 622Q121 629 117 631T101 634T58 637H25V683H36Q48 680 182 680Q324 680 348 683H360V637H333Q273 637 258 635T233 622L232 342V129Q232 57 237 52Q243 47 313 47Q384 47 410 53Q470 70 498 110T536 221Q536 226 537 238T540 261T542 272T562 273H582V268Q580 265 568 137T554 5V0H25V46H58Q100 47 109 49T128 61V622Z" transform="translate(1180,0)" style="stroke-width:3;"></path><path data-c="55" d="M128 622Q121 629 117 631T101 634T58 637H25V683H36Q57 680 180 680Q315 680 324 683H335V637H302Q262 636 251 634T233 622L232 418V291Q232 189 240 145T280 67Q325 24 389 24Q454 24 506 64T571 183Q575 206 575 410V598Q569 608 565 613T541 627T489 637H472V683H481Q496 680 598 680T715 683H724V637H707Q634 633 622 598L621 399Q620 194 617 180Q617 179 615 171Q595 83 531 31T389 -22Q304 -22 226 33T130 192Q129 201 128 412V622Z" transform="translate(1805,0)" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(9939.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="msub" transform="translate(10328.6,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D416" d="M915 686L1052 683Q1142 683 1157 686H1164V624H1073L957 320Q930 249 900 170T855 52T839 10Q834 0 826 -5Q821 -7 799 -7H792Q777 -7 772 -5T759 10Q759 11 748 39T716 122T676 228L594 442L512 228Q486 159 455 78Q433 19 428 9T416 -5Q411 -7 389 -7H379Q356 -7 349 10Q349 12 334 51T288 170T231 320L116 624H24V686H35Q44 683 183 683Q331 683 355 686H368V624H323Q278 624 278 623L437 207L499 369L561 531L526 624H434V686H445Q454 683 593 683Q741 683 765 686H778V624H733Q688 624 688 623L847 207Q848 207 927 415T1006 624H905V686H915Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mn" transform="translate(1222,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" style="stroke-width:3;"></path></g></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(11954.1,0)"><g data-mml-node="mi"><path data-c="1D417" d="M327 0Q306 3 174 3Q52 3 43 0H33V62H98L162 63L360 333L157 624H48V686H59Q80 683 217 683Q368 683 395 686H408V624H335L393 540L452 458L573 623Q573 624 528 624H483V686H494Q515 683 646 683Q769 683 778 686H787V624H658L575 511Q493 398 493 397L508 376Q522 356 553 312T611 229L727 62H835V0H824Q803 3 667 3Q516 3 489 0H476V62H513L549 63L401 274L247 63Q247 62 292 62H338V0H327Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(13045.3,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z" style="stroke-width:3;"></path></g><g data-mml-node="msub" transform="translate(14045.6,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D41B" d="M32 686L123 690Q214 694 215 694H221V409Q289 450 378 450Q479 450 539 387T600 221Q600 122 535 58T358 -6H355Q272 -6 203 53L160 1L129 0H98V301Q98 362 98 435T99 525Q99 591 97 604T83 620Q69 624 42 624H29V686H32ZM227 105L232 99Q237 93 242 87T258 73T280 59T306 49T339 45Q380 45 411 66T451 131Q457 160 457 230Q457 264 456 284T448 329T430 367T396 389T343 398Q282 398 235 355L227 348V105Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mn" transform="translate(672,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(15121.1,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(15732.3,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z" style="stroke-width:3;"></path></g><g data-mml-node="msub" transform="translate(16732.5,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D41B" d="M32 686L123 690Q214 694 215 694H221V409Q289 450 378 450Q479 450 539 387T600 221Q600 122 535 58T358 -6H355Q272 -6 203 53L160 1L129 0H98V301Q98 362 98 435T99 525Q99 591 97 604T83 620Q69 624 42 624H29V686H32ZM227 105L232 99Q237 93 242 87T258 73T280 59T306 49T339 45Q380 45 411 66T451 131Q457 160 457 230Q457 264 456 284T448 329T430 367T396 389T343 398Q282 398 235 355L227 348V105Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mn" transform="translate(672,-150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" style="stroke-width:3;"></path></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="block" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;overflow:hidden;width:100%;"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtext>FFN</mtext><mo stretchy="false">(</mo><mrow data-mjx-texclass="ORD"><mi mathvariant="bold">X</mi></mrow><mo stretchy="false">)</mo><mo>=</mo><msub><mrow data-mjx-texclass="ORD"><mi mathvariant="bold">W</mi></mrow><mn>2</mn></msub><mo>⋅</mo><mtext>ReLU</mtext><mo stretchy="false">(</mo><msub><mrow data-mjx-texclass="ORD"><mi mathvariant="bold">W</mi></mrow><mn>1</mn></msub><mrow data-mjx-texclass="ORD"><mi mathvariant="bold">X</mi></mrow><mo>+</mo><msub><mrow data-mjx-texclass="ORD"><mi mathvariant="bold">b</mi></mrow><mn>1</mn></msub><mo stretchy="false">)</mo><mo>+</mo><msub><mrow data-mjx-texclass="ORD"><mi mathvariant="bold">b</mi></mrow><mn>2</mn></msub></math></mjx-assistive-mml></mjx-container><ul><li><mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.339ex;" xmlns="http://www.w3.org/2000/svg" width="15.716ex" height="2.253ex" role="img" focusable="false" viewBox="0 -846 6946.4 996" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D416" d="M915 686L1052 683Q1142 683 1157 686H1164V624H1073L957 320Q930 249 900 170T855 52T839 10Q834 0 826 -5Q821 -7 799 -7H792Q777 -7 772 -5T759 10Q759 11 748 39T716 122T676 228L594 442L512 228Q486 159 455 78Q433 19 428 9T416 -5Q411 -7 389 -7H379Q356 -7 349 10Q349 12 334 51T288 170T231 320L116 624H24V686H35Q44 683 183 683Q331 683 355 686H368V624H323Q278 624 278 623L437 207L499 369L561 531L526 624H434V686H445Q454 683 593 683Q741 683 765 686H778V624H733Q688 624 688 623L847 207Q848 207 927 415T1006 624H905V686H915Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mn" transform="translate(1222,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(1903.3,0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z" style="stroke-width:3;"></path></g><g data-mml-node="msup" transform="translate(2848.1,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="211D" d="M17 665Q17 672 28 683H221Q415 681 439 677Q461 673 481 667T516 654T544 639T566 623T584 607T597 592T607 578T614 565T618 554L621 548Q626 530 626 497Q626 447 613 419Q578 348 473 326L455 321Q462 310 473 292T517 226T578 141T637 72T686 35Q705 30 705 16Q705 7 693 -1H510Q503 6 404 159L306 310H268V183Q270 67 271 59Q274 42 291 38Q295 37 319 35Q344 35 353 28Q362 17 353 3L346 -1H28Q16 5 16 16Q16 35 55 35Q96 38 101 52Q106 60 106 341T101 632Q95 645 55 648Q17 648 17 665ZM241 35Q238 42 237 45T235 78T233 163T233 337V621L237 635L244 648H133Q136 641 137 638T139 603T141 517T141 341Q141 131 140 89T134 37Q133 36 133 35H241ZM457 496Q457 540 449 570T425 615T400 634T377 643Q374 643 339 648Q300 648 281 635Q271 628 270 610T268 481V346H284Q327 346 375 352Q421 364 439 392T457 496ZM492 537T492 496T488 427T478 389T469 371T464 361Q464 360 465 360Q469 360 497 370Q593 400 593 495Q593 592 477 630L457 637L461 626Q474 611 488 561Q492 537 492 496ZM464 243Q411 317 410 317Q404 317 401 315Q384 315 370 312H346L526 35H619L606 50Q553 109 464 243Z" style="stroke-width:3;"></path></g></g><g data-mml-node="TeXAtom" transform="translate(755,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(828,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z" style="stroke-width:3;"></path></g><g data-mml-node="msub" transform="translate(1606,0)"><g data-mml-node="mi"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" transform="translate(861,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(576,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(921,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1441,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1961,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(2427,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g></g></g></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mi mathvariant="bold">W</mi></mrow><mn>1</mn></msub><mo>∈</mo><msup><mrow data-mjx-texclass="ORD"><mi mathvariant="double-struck">R</mi></mrow><mrow data-mjx-texclass="ORD"><mi>D</mi><mo>×</mo><msub><mi>D</mi><mrow data-mjx-texclass="ORD"><mi>h</mi><mi>i</mi><mi>d</mi><mi>d</mi><mi>e</mi><mi>n</mi></mrow></msub></mrow></msup></math></mjx-assistive-mml></mjx-container>, <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.339ex;" xmlns="http://www.w3.org/2000/svg" width="15.716ex" height="2.253ex" role="img" focusable="false" viewBox="0 -846 6946.4 996" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D416" d="M915 686L1052 683Q1142 683 1157 686H1164V624H1073L957 320Q930 249 900 170T855 52T839 10Q834 0 826 -5Q821 -7 799 -7H792Q777 -7 772 -5T759 10Q759 11 748 39T716 122T676 228L594 442L512 228Q486 159 455 78Q433 19 428 9T416 -5Q411 -7 389 -7H379Q356 -7 349 10Q349 12 334 51T288 170T231 320L116 624H24V686H35Q44 683 183 683Q331 683 355 686H368V624H323Q278 624 278 623L437 207L499 369L561 531L526 624H434V686H445Q454 683 593 683Q741 683 765 686H778V624H733Q688 624 688 623L847 207Q848 207 927 415T1006 624H905V686H915Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mn" transform="translate(1222,-150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(1903.3,0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z" style="stroke-width:3;"></path></g><g data-mml-node="msup" transform="translate(2848.1,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="211D" d="M17 665Q17 672 28 683H221Q415 681 439 677Q461 673 481 667T516 654T544 639T566 623T584 607T597 592T607 578T614 565T618 554L621 548Q626 530 626 497Q626 447 613 419Q578 348 473 326L455 321Q462 310 473 292T517 226T578 141T637 72T686 35Q705 30 705 16Q705 7 693 -1H510Q503 6 404 159L306 310H268V183Q270 67 271 59Q274 42 291 38Q295 37 319 35Q344 35 353 28Q362 17 353 3L346 -1H28Q16 5 16 16Q16 35 55 35Q96 38 101 52Q106 60 106 341T101 632Q95 645 55 648Q17 648 17 665ZM241 35Q238 42 237 45T235 78T233 163T233 337V621L237 635L244 648H133Q136 641 137 638T139 603T141 517T141 341Q141 131 140 89T134 37Q133 36 133 35H241ZM457 496Q457 540 449 570T425 615T400 634T377 643Q374 643 339 648Q300 648 281 635Q271 628 270 610T268 481V346H284Q327 346 375 352Q421 364 439 392T457 496ZM492 537T492 496T488 427T478 389T469 371T464 361Q464 360 465 360Q469 360 497 370Q593 400 593 495Q593 592 477 630L457 637L461 626Q474 611 488 561Q492 537 492 496ZM464 243Q411 317 410 317Q404 317 401 315Q384 315 370 312H346L526 35H619L606 50Q553 109 464 243Z" style="stroke-width:3;"></path></g></g><g data-mml-node="TeXAtom" transform="translate(755,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" transform="translate(861,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(576,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(921,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1441,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1961,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(2427,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g></g></g><g data-mml-node="mo" transform="translate(3051.4,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(3829.4,0)"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z" style="stroke-width:3;"></path></g></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mrow data-mjx-texclass="ORD"><mi mathvariant="bold">W</mi></mrow><mn>2</mn></msub><mo>∈</mo><msup><mrow data-mjx-texclass="ORD"><mi mathvariant="double-struck">R</mi></mrow><mrow data-mjx-texclass="ORD"><msub><mi>D</mi><mrow data-mjx-texclass="ORD"><mi>h</mi><mi>i</mi><mi>d</mi><mi>d</mi><mi>e</mi><mi>n</mi></mrow></msub><mo>×</mo><mi>D</mi></mrow></msup></math></mjx-assistive-mml></mjx-container></li><li>每个位置的输出仅依赖该位置的输入，不依赖其他位置。</li></ul><p><strong>前馈网络 vs 其他网络</strong></p><table tabindex="0"><thead><tr><th>网络类型</th><th>典型用途</th><th>数据流动方向</th><th>示例</th></tr></thead><tbody><tr><td><strong>前馈网络</strong></td><td>图像分类、特征提取</td><td>单向（输入 → 输出）</td><td>MLP, PositionWiseFFN</td></tr><tr><td><strong>循环网络</strong></td><td>时序数据（文本、语音）</td><td>双向（含时间反馈）</td><td>LSTM, GRU</td></tr><tr><td><strong>卷积网络</strong></td><td>图像、空间数据</td><td>局部连接+权重共享</td><td>ResNet, VGG</td></tr><tr><td><strong>Transformer</strong></td><td>序列建模（如机器翻译）</td><td>自注意力+前馈</td><td>BERT, GPT</td></tr></tbody></table><h2 id="残差连接和层规范化-add-norm" tabindex="-1">残差连接和层规范化(add&amp;norm) <a class="header-anchor" href="#残差连接和层规范化-add-norm" aria-label="Permalink to &quot;残差连接和层规范化(add&amp;norm)&quot;">​</a></h2><p><code>Transformer</code> 中的残差连接和层规范化，是训练非常<strong>深度模型</strong>的重要工具。</p><p><strong>层规范化</strong>和<strong>批量规范化</strong>的目标相同，但层规范化是基于<strong>特征维度进行规范化</strong>。尽管批量规范化在计算机视觉中被广泛应用，但在<strong>自然语言处理</strong>任务中（输入通常是变长序列）批量规范化通常不如层规范化的效果好。</p><p><strong>残差连接</strong>要求<strong>两个输入形状相同</strong>，以便<strong>加法</strong>操作后输出张量形状相同。</p><div class="language-py line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">py</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#6A737D;"># 残差连接后进行层规范化</span></span>
<span class="line"><span style="color:#F97583;">class</span><span style="color:#B392F0;"> AddNorm</span><span style="color:#E1E4E8;">(</span><span style="color:#B392F0;">nn</span><span style="color:#E1E4E8;">.</span><span style="color:#B392F0;">Module</span><span style="color:#E1E4E8;">):</span></span>
<span class="line"><span style="color:#F97583;">  def</span><span style="color:#79B8FF;"> __init__</span><span style="color:#E1E4E8;">(self, normalized_shape, dropout, </span><span style="color:#F97583;">**</span><span style="color:#E1E4E8;">kwargs):</span></span>
<span class="line"><span style="color:#79B8FF;">    super</span><span style="color:#E1E4E8;">(AddNorm, </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">).</span><span style="color:#79B8FF;">__init__</span><span style="color:#E1E4E8;">(</span><span style="color:#F97583;">**</span><span style="color:#E1E4E8;">kwargs)</span></span>
<span class="line"><span style="color:#79B8FF;">    self</span><span style="color:#E1E4E8;">.dropout </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> nn.Dropout(dropout)</span></span>
<span class="line"><span style="color:#79B8FF;">    self</span><span style="color:#E1E4E8;">.ln </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> nn.LayerNorm(normalized_shape)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583;">  def</span><span style="color:#B392F0;"> forward</span><span style="color:#E1E4E8;">(self, X, Y):</span></span>
<span class="line"><span style="color:#F97583;">    return</span><span style="color:#79B8FF;"> self</span><span style="color:#E1E4E8;">.ln(</span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.dropout(Y) </span><span style="color:#F97583;">+</span><span style="color:#E1E4E8;"> X)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 示例</span></span>
<span class="line"><span style="color:#E1E4E8;">add_norm </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> AddNorm([</span><span style="color:#79B8FF;">3</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">4</span><span style="color:#E1E4E8;">], </span><span style="color:#79B8FF;">0.5</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">add_norm.eval()</span></span>
<span class="line"><span style="color:#E1E4E8;">add_norm(torch.ones((</span><span style="color:#79B8FF;">2</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">3</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">4</span><span style="color:#E1E4E8;">)), torch.ones((</span><span style="color:#79B8FF;">2</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">3</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">4</span><span style="color:#E1E4E8;">))).shape</span></span>
<span class="line"><span style="color:#6A737D;"># torch.Size([2, 3, 4])</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br></div></div><h2 id="实现编码器" tabindex="-1">实现编码器 <a class="header-anchor" href="#实现编码器" aria-label="Permalink to &quot;实现编码器&quot;">​</a></h2><div class="language-py line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">py</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#6A737D;"># 编码器块</span></span>
<span class="line"><span style="color:#F97583;">class</span><span style="color:#B392F0;"> EncoderBlock</span><span style="color:#E1E4E8;">(</span><span style="color:#B392F0;">nn</span><span style="color:#E1E4E8;">.</span><span style="color:#B392F0;">Module</span><span style="color:#E1E4E8;">):</span></span>
<span class="line"><span style="color:#F97583;">  def</span><span style="color:#79B8FF;"> __init__</span><span style="color:#E1E4E8;">(self, key_size, query_size, value_size, num_hiddens,</span></span>
<span class="line"><span style="color:#E1E4E8;">              norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,</span></span>
<span class="line"><span style="color:#E1E4E8;">              dropout, use_bias</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">False</span><span style="color:#E1E4E8;">, </span><span style="color:#F97583;">**</span><span style="color:#E1E4E8;">kwargs):</span></span>
<span class="line"><span style="color:#79B8FF;">    super</span><span style="color:#E1E4E8;">(EncoderBlock, </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">).</span><span style="color:#79B8FF;">__init__</span><span style="color:#E1E4E8;">(</span><span style="color:#F97583;">**</span><span style="color:#E1E4E8;">kwargs)</span></span>
<span class="line"><span style="color:#79B8FF;">    self</span><span style="color:#E1E4E8;">.attention </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> d2l.MultiHeadAttention(</span></span>
<span class="line"><span style="color:#E1E4E8;">        key_size, query_size, value_size, num_hiddens, num_heads, dropout, use_bias)</span></span>
<span class="line"><span style="color:#79B8FF;">    self</span><span style="color:#E1E4E8;">.addnorm1 </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> AddNorm(norm_shape, dropout)</span></span>
<span class="line"><span style="color:#79B8FF;">    self</span><span style="color:#E1E4E8;">.ffn </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> PositionWiseFFN(ffn_num_input, ffn_num_hiddens, num_hiddens)</span></span>
<span class="line"><span style="color:#79B8FF;">    self</span><span style="color:#E1E4E8;">.addnorm2 </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> AddNorm(norm_shape, dropout)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583;">  def</span><span style="color:#B392F0;"> forward</span><span style="color:#E1E4E8;">(self, X, valid_lens):</span></span>
<span class="line"><span style="color:#E1E4E8;">    Y </span><span style="color:#F97583;">=</span><span style="color:#79B8FF;"> self</span><span style="color:#E1E4E8;">.addnorm1(X, </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.attention(X, X, X, valid_lens))</span></span>
<span class="line"><span style="color:#F97583;">    return</span><span style="color:#79B8FF;"> self</span><span style="color:#E1E4E8;">.addnorm2(Y, </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.ffn(Y))</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># Transformer编码器</span></span>
<span class="line"><span style="color:#F97583;">class</span><span style="color:#B392F0;"> TransformerEncoder</span><span style="color:#E1E4E8;">(</span><span style="color:#B392F0;">d2l</span><span style="color:#E1E4E8;">.</span><span style="color:#B392F0;">Encoder</span><span style="color:#E1E4E8;">):</span></span>
<span class="line"><span style="color:#F97583;">  def</span><span style="color:#79B8FF;"> __init__</span><span style="color:#E1E4E8;">(self, vocab_size, key_size, query_size, value_size,</span></span>
<span class="line"><span style="color:#E1E4E8;">              num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,</span></span>
<span class="line"><span style="color:#E1E4E8;">              num_heads, num_layers, dropout, use_bias</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">False</span><span style="color:#E1E4E8;">, </span><span style="color:#F97583;">**</span><span style="color:#E1E4E8;">kwargs):</span></span>
<span class="line"><span style="color:#79B8FF;">    super</span><span style="color:#E1E4E8;">(TransformerEncoder, </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">).</span><span style="color:#79B8FF;">__init__</span><span style="color:#E1E4E8;">(</span><span style="color:#F97583;">**</span><span style="color:#E1E4E8;">kwargs)</span></span>
<span class="line"><span style="color:#79B8FF;">    self</span><span style="color:#E1E4E8;">.num_hiddens </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> num_hiddens</span></span>
<span class="line"><span style="color:#79B8FF;">    self</span><span style="color:#E1E4E8;">.embedding </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> nn.Embedding(vocab_size, num_hiddens)</span></span>
<span class="line"><span style="color:#79B8FF;">    self</span><span style="color:#E1E4E8;">.pos_encoding </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> d2l.PositionalEncoding(num_hiddens, dropout)</span></span>
<span class="line"><span style="color:#79B8FF;">    self</span><span style="color:#E1E4E8;">.blks </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> nn.Sequential()</span></span>
<span class="line"><span style="color:#F97583;">    for</span><span style="color:#E1E4E8;"> i </span><span style="color:#F97583;">in</span><span style="color:#79B8FF;"> range</span><span style="color:#E1E4E8;">(num_layers):</span></span>
<span class="line"><span style="color:#79B8FF;">      self</span><span style="color:#E1E4E8;">.blks.add_module(</span><span style="color:#9ECBFF;">&quot;block&quot;</span><span style="color:#F97583;">+</span><span style="color:#79B8FF;">str</span><span style="color:#E1E4E8;">(i), EncoderBlock(</span></span>
<span class="line"><span style="color:#E1E4E8;">        key_size, query_size, value_size, num_hiddens,</span></span>
<span class="line"><span style="color:#E1E4E8;">        norm_shape, ffn_num_input, ffn_num_hiddens,</span></span>
<span class="line"><span style="color:#E1E4E8;">        num_heads, dropout, use_bias))</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583;">  def</span><span style="color:#B392F0;"> forward</span><span style="color:#E1E4E8;">(self, X, valid_lens, </span><span style="color:#F97583;">*</span><span style="color:#E1E4E8;">args):</span></span>
<span class="line"><span style="color:#6A737D;">    # 因为位置编码值在-1和1之间，</span></span>
<span class="line"><span style="color:#6A737D;">    # 因此嵌入值乘以嵌入维度的平方根进行缩放，</span></span>
<span class="line"><span style="color:#6A737D;">    # 然后再与位置编码相加。</span></span>
<span class="line"><span style="color:#E1E4E8;">    X </span><span style="color:#F97583;">=</span><span style="color:#79B8FF;"> self</span><span style="color:#E1E4E8;">.pos_encoding(</span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.embedding(X) </span><span style="color:#F97583;">*</span><span style="color:#E1E4E8;"> math.sqrt(</span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.num_hiddens))</span></span>
<span class="line"><span style="color:#79B8FF;">    self</span><span style="color:#E1E4E8;">.attention_weights </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> [</span><span style="color:#79B8FF;">None</span><span style="color:#E1E4E8;">] </span><span style="color:#F97583;">*</span><span style="color:#79B8FF;"> len</span><span style="color:#E1E4E8;">(</span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.blks)</span></span>
<span class="line"><span style="color:#F97583;">    for</span><span style="color:#E1E4E8;"> i, blk </span><span style="color:#F97583;">in</span><span style="color:#79B8FF;"> enumerate</span><span style="color:#E1E4E8;">(</span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.blks):</span></span>
<span class="line"><span style="color:#E1E4E8;">      X </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> blk(X, valid_lens)</span></span>
<span class="line"><span style="color:#79B8FF;">      self</span><span style="color:#E1E4E8;">.attention_weights[i] </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> blk.attention.attention.attention_weights</span></span>
<span class="line"><span style="color:#F97583;">    return</span><span style="color:#E1E4E8;"> X</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br></div></div><ul><li>嵌入和位置编码两数相加后生成“一个数”，实际上是<strong>高维空间</strong>中<strong>一个坐标点</strong>。</li><li>而这个点，可以看作是<strong>原始嵌入</strong>向量和<strong>位置编码</strong>向量在该维度上的<strong>线性组合</strong>。</li><li>模型通过<strong>训练</strong>，能够<strong>解耦</strong>出原始嵌入和位置信息。</li></ul><h2 id="解码器实现" tabindex="-1">解码器实现 <a class="header-anchor" href="#解码器实现" aria-label="Permalink to &quot;解码器实现&quot;">​</a></h2><div class="language-py line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">py</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#6A737D;"># 解码器中第i个块</span></span>
<span class="line"><span style="color:#F97583;">class</span><span style="color:#B392F0;"> DecoderBlock</span><span style="color:#E1E4E8;">(</span><span style="color:#B392F0;">nn</span><span style="color:#E1E4E8;">.</span><span style="color:#B392F0;">Module</span><span style="color:#E1E4E8;">):</span></span>
<span class="line"><span style="color:#F97583;">  def</span><span style="color:#79B8FF;"> __init__</span><span style="color:#E1E4E8;">(self, key_size, query_size, value_size, num_hiddens,</span></span>
<span class="line"><span style="color:#E1E4E8;">                norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,</span></span>
<span class="line"><span style="color:#E1E4E8;">                dropout, i, </span><span style="color:#F97583;">**</span><span style="color:#E1E4E8;">kwargs):</span></span>
<span class="line"><span style="color:#79B8FF;">    super</span><span style="color:#E1E4E8;">(DecoderBlock, </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">).</span><span style="color:#79B8FF;">__init__</span><span style="color:#E1E4E8;">(</span><span style="color:#F97583;">**</span><span style="color:#E1E4E8;">kwargs)</span></span>
<span class="line"><span style="color:#79B8FF;">    self</span><span style="color:#E1E4E8;">.i </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> i</span></span>
<span class="line"><span style="color:#79B8FF;">    self</span><span style="color:#E1E4E8;">.attention1 </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> d2l.MultiHeadAttention(</span></span>
<span class="line"><span style="color:#E1E4E8;">        key_size, query_size, value_size, num_hiddens, num_heads, dropout)</span></span>
<span class="line"><span style="color:#79B8FF;">    self</span><span style="color:#E1E4E8;">.addnorm1 </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> AddNorm(norm_shape, dropout)</span></span>
<span class="line"><span style="color:#79B8FF;">    self</span><span style="color:#E1E4E8;">.attention2 </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> d2l.MultiHeadAttention(</span></span>
<span class="line"><span style="color:#E1E4E8;">        key_size, query_size, value_size, num_hiddens, num_heads, dropout)</span></span>
<span class="line"><span style="color:#79B8FF;">    self</span><span style="color:#E1E4E8;">.addnorm2 </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> AddNorm(norm_shape, dropout)</span></span>
<span class="line"><span style="color:#79B8FF;">    self</span><span style="color:#E1E4E8;">.ffn </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> PositionWiseFFN(ffn_num_input, ffn_num_hiddens, num_hiddens)</span></span>
<span class="line"><span style="color:#79B8FF;">    self</span><span style="color:#E1E4E8;">.addnorm3 </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> AddNorm(norm_shape, dropout)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583;">  def</span><span style="color:#B392F0;"> forward</span><span style="color:#E1E4E8;">(self, X, state):</span></span>
<span class="line"><span style="color:#E1E4E8;">    enc_outputs, enc_valid_lens </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> state[</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">], state[</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">]</span></span>
<span class="line"><span style="color:#6A737D;">    # 训练阶段，输出序列的所有词元都在同一时间处理，</span></span>
<span class="line"><span style="color:#6A737D;">    # 因此state[2][self.i]初始化为None。</span></span>
<span class="line"><span style="color:#6A737D;">    # 预测阶段，输出序列是通过词元一个接着一个解码的，</span></span>
<span class="line"><span style="color:#6A737D;">    # 因此state[2][self.i]包含着直到当前时间步第i个块解码的输出表示</span></span>
<span class="line"><span style="color:#F97583;">    if</span><span style="color:#E1E4E8;"> state[</span><span style="color:#79B8FF;">2</span><span style="color:#E1E4E8;">][</span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.i] </span><span style="color:#F97583;">is</span><span style="color:#79B8FF;"> None</span><span style="color:#E1E4E8;">:</span></span>
<span class="line"><span style="color:#E1E4E8;">      key_values </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> X</span></span>
<span class="line"><span style="color:#F97583;">    else</span><span style="color:#E1E4E8;">:</span></span>
<span class="line"><span style="color:#E1E4E8;">      key_values </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torch.cat((state[</span><span style="color:#79B8FF;">2</span><span style="color:#E1E4E8;">][</span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.i], X), </span><span style="color:#FFAB70;">axis</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">    state[</span><span style="color:#79B8FF;">2</span><span style="color:#E1E4E8;">][</span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.i] </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> key_values</span></span>
<span class="line"><span style="color:#F97583;">    if</span><span style="color:#79B8FF;"> self</span><span style="color:#E1E4E8;">.training:</span></span>
<span class="line"><span style="color:#E1E4E8;">      batch_size, num_steps, _ </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> X.shape</span></span>
<span class="line"><span style="color:#6A737D;">      # dec_valid_lens的开头:(batch_size,num_steps),</span></span>
<span class="line"><span style="color:#6A737D;">      # 其中每一行是[1,2,...,num_steps]</span></span>
<span class="line"><span style="color:#E1E4E8;">      dec_valid_lens </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torch.arange(</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">, num_steps </span><span style="color:#F97583;">+</span><span style="color:#79B8FF;"> 1</span><span style="color:#E1E4E8;">, </span><span style="color:#FFAB70;">device</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">X.device).repeat(batch_size, </span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#F97583;">    else</span><span style="color:#E1E4E8;">:</span></span>
<span class="line"><span style="color:#E1E4E8;">      dec_valid_lens </span><span style="color:#F97583;">=</span><span style="color:#79B8FF;"> None</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;">    # 自注意力</span></span>
<span class="line"><span style="color:#E1E4E8;">    X2 </span><span style="color:#F97583;">=</span><span style="color:#79B8FF;"> self</span><span style="color:#E1E4E8;">.attention1(X, key_values, key_values, dec_valid_lens)</span></span>
<span class="line"><span style="color:#E1E4E8;">    Y </span><span style="color:#F97583;">=</span><span style="color:#79B8FF;"> self</span><span style="color:#E1E4E8;">.addnorm1(X, X2)</span></span>
<span class="line"><span style="color:#6A737D;">    # 编码器－解码器注意力。</span></span>
<span class="line"><span style="color:#6A737D;">    # enc_outputs的开头:(batch_size,num_steps,num_hiddens)</span></span>
<span class="line"><span style="color:#E1E4E8;">    Y2 </span><span style="color:#F97583;">=</span><span style="color:#79B8FF;"> self</span><span style="color:#E1E4E8;">.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens)</span></span>
<span class="line"><span style="color:#E1E4E8;">    Z </span><span style="color:#F97583;">=</span><span style="color:#79B8FF;"> self</span><span style="color:#E1E4E8;">.addnorm2(Y, Y2)</span></span>
<span class="line"><span style="color:#F97583;">    return</span><span style="color:#79B8FF;"> self</span><span style="color:#E1E4E8;">.addnorm3(Z, </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.ffn(Z)), state</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 解码器</span></span>
<span class="line"><span style="color:#F97583;">class</span><span style="color:#B392F0;"> TransformerDecoder</span><span style="color:#E1E4E8;">(</span><span style="color:#B392F0;">d2l</span><span style="color:#E1E4E8;">.</span><span style="color:#B392F0;">AttentionDecoder</span><span style="color:#E1E4E8;">):</span></span>
<span class="line"><span style="color:#F97583;">  def</span><span style="color:#79B8FF;"> __init__</span><span style="color:#E1E4E8;">(self, vocab_size, key_size, query_size, value_size,</span></span>
<span class="line"><span style="color:#E1E4E8;">                num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,</span></span>
<span class="line"><span style="color:#E1E4E8;">                num_heads, num_layers, dropout, </span><span style="color:#F97583;">**</span><span style="color:#E1E4E8;">kwargs):</span></span>
<span class="line"><span style="color:#79B8FF;">    super</span><span style="color:#E1E4E8;">(TransformerDecoder, </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">).</span><span style="color:#79B8FF;">__init__</span><span style="color:#E1E4E8;">(</span><span style="color:#F97583;">**</span><span style="color:#E1E4E8;">kwargs)</span></span>
<span class="line"><span style="color:#79B8FF;">    self</span><span style="color:#E1E4E8;">.num_hiddens </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> num_hiddens</span></span>
<span class="line"><span style="color:#79B8FF;">    self</span><span style="color:#E1E4E8;">.num_layers </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> num_layers</span></span>
<span class="line"><span style="color:#79B8FF;">    self</span><span style="color:#E1E4E8;">.embedding </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> nn.Embedding(vocab_size, num_hiddens)</span></span>
<span class="line"><span style="color:#79B8FF;">    self</span><span style="color:#E1E4E8;">.pos_encoding </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> d2l.PositionalEncoding(num_hiddens, dropout)</span></span>
<span class="line"><span style="color:#79B8FF;">    self</span><span style="color:#E1E4E8;">.blks </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> nn.Sequential()</span></span>
<span class="line"><span style="color:#F97583;">    for</span><span style="color:#E1E4E8;"> i </span><span style="color:#F97583;">in</span><span style="color:#79B8FF;"> range</span><span style="color:#E1E4E8;">(num_layers):</span></span>
<span class="line"><span style="color:#79B8FF;">      self</span><span style="color:#E1E4E8;">.blks.add_module(</span><span style="color:#9ECBFF;">&quot;block&quot;</span><span style="color:#F97583;">+</span><span style="color:#79B8FF;">str</span><span style="color:#E1E4E8;">(i),</span></span>
<span class="line"><span style="color:#E1E4E8;">          DecoderBlock(key_size, query_size, value_size, num_hiddens,</span></span>
<span class="line"><span style="color:#E1E4E8;">                        norm_shape, ffn_num_input, ffn_num_hiddens,</span></span>
<span class="line"><span style="color:#E1E4E8;">                        num_heads, dropout, i))</span></span>
<span class="line"><span style="color:#79B8FF;">    self</span><span style="color:#E1E4E8;">.dense </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> nn.Linear(num_hiddens, vocab_size)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583;">  def</span><span style="color:#B392F0;"> init_state</span><span style="color:#E1E4E8;">(self, enc_outputs, enc_valid_lens, </span><span style="color:#F97583;">*</span><span style="color:#E1E4E8;">args):</span></span>
<span class="line"><span style="color:#F97583;">    return</span><span style="color:#E1E4E8;"> [enc_outputs, enc_valid_lens, [</span><span style="color:#79B8FF;">None</span><span style="color:#E1E4E8;">] </span><span style="color:#F97583;">*</span><span style="color:#79B8FF;"> self</span><span style="color:#E1E4E8;">.num_layers]</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583;">  def</span><span style="color:#B392F0;"> forward</span><span style="color:#E1E4E8;">(self, X, state):</span></span>
<span class="line"><span style="color:#E1E4E8;">    X </span><span style="color:#F97583;">=</span><span style="color:#79B8FF;"> self</span><span style="color:#E1E4E8;">.pos_encoding(</span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.embedding(X) </span><span style="color:#F97583;">*</span><span style="color:#E1E4E8;"> math.sqrt(</span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.num_hiddens))</span></span>
<span class="line"><span style="color:#79B8FF;">    self</span><span style="color:#E1E4E8;">._attention_weights </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> [[</span><span style="color:#79B8FF;">None</span><span style="color:#E1E4E8;">] </span><span style="color:#F97583;">*</span><span style="color:#79B8FF;"> len</span><span style="color:#E1E4E8;">(</span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.blks) </span><span style="color:#F97583;">for</span><span style="color:#E1E4E8;"> _ </span><span style="color:#F97583;">in</span><span style="color:#79B8FF;"> range</span><span style="color:#E1E4E8;"> (</span><span style="color:#79B8FF;">2</span><span style="color:#E1E4E8;">)]</span></span>
<span class="line"><span style="color:#F97583;">    for</span><span style="color:#E1E4E8;"> i, blk </span><span style="color:#F97583;">in</span><span style="color:#79B8FF;"> enumerate</span><span style="color:#E1E4E8;">(</span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.blks):</span></span>
<span class="line"><span style="color:#E1E4E8;">      X, state </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> blk(X, state)</span></span>
<span class="line"><span style="color:#6A737D;">      # 解码器自注意力权重</span></span>
<span class="line"><span style="color:#79B8FF;">      self</span><span style="color:#E1E4E8;">._attention_weights[</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">][i] </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> blk.attention1.attention.attention_weights</span></span>
<span class="line"><span style="color:#6A737D;">      # “编码器－解码器”自注意力权重</span></span>
<span class="line"><span style="color:#79B8FF;">      self</span><span style="color:#E1E4E8;">._attention_weights[</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">][i] </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> blk.attention2.attention.attention_weights</span></span>
<span class="line"><span style="color:#F97583;">    return</span><span style="color:#79B8FF;"> self</span><span style="color:#E1E4E8;">.dense(X), state</span></span>
<span class="line"></span>
<span class="line"><span style="color:#B392F0;">  @</span><span style="color:#79B8FF;">property</span></span>
<span class="line"><span style="color:#F97583;">  def</span><span style="color:#B392F0;"> attention_weights</span><span style="color:#E1E4E8;">(self):</span></span>
<span class="line"><span style="color:#F97583;">    return</span><span style="color:#79B8FF;"> self</span><span style="color:#E1E4E8;">._attention_weights</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br><span class="line-number">43</span><br><span class="line-number">44</span><br><span class="line-number">45</span><br><span class="line-number">46</span><br><span class="line-number">47</span><br><span class="line-number">48</span><br><span class="line-number">49</span><br><span class="line-number">50</span><br><span class="line-number">51</span><br><span class="line-number">52</span><br><span class="line-number">53</span><br><span class="line-number">54</span><br><span class="line-number">55</span><br><span class="line-number">56</span><br><span class="line-number">57</span><br><span class="line-number">58</span><br><span class="line-number">59</span><br><span class="line-number">60</span><br><span class="line-number">61</span><br><span class="line-number">62</span><br><span class="line-number">63</span><br><span class="line-number">64</span><br><span class="line-number">65</span><br><span class="line-number">66</span><br><span class="line-number">67</span><br><span class="line-number">68</span><br><span class="line-number">69</span><br><span class="line-number">70</span><br><span class="line-number">71</span><br><span class="line-number">72</span><br><span class="line-number">73</span><br><span class="line-number">74</span><br><span class="line-number">75</span><br><span class="line-number">76</span><br><span class="line-number">77</span><br><span class="line-number">78</span><br><span class="line-number">79</span><br></div></div><h2 id="训练" tabindex="-1">训练 <a class="header-anchor" href="#训练" aria-label="Permalink to &quot;训练&quot;">​</a></h2><div class="language-py line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">py</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#E1E4E8;">num_hiddens, num_layers, dropout, batch_size, num_steps </span><span style="color:#F97583;">=</span><span style="color:#79B8FF;"> 32</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">2</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">0.1</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">64</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">10</span></span>
<span class="line"><span style="color:#E1E4E8;">lr, num_epochs, device </span><span style="color:#F97583;">=</span><span style="color:#79B8FF;"> 0.005</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">200</span><span style="color:#E1E4E8;">, d2l.try_gpu()</span></span>
<span class="line"><span style="color:#E1E4E8;">ffn_num_input, ffn_num_hiddens, num_heads </span><span style="color:#F97583;">=</span><span style="color:#79B8FF;"> 32</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">64</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">4</span></span>
<span class="line"><span style="color:#E1E4E8;">key_size, query_size, value_size </span><span style="color:#F97583;">=</span><span style="color:#79B8FF;"> 32</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">32</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">32</span></span>
<span class="line"><span style="color:#E1E4E8;">norm_shape </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> [</span><span style="color:#79B8FF;">32</span><span style="color:#E1E4E8;">]</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">train_iter, src_vocab, tgt_vocab </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> d2l.load_data_nmt(batch_size, num_steps)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">encoder </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> TransformerEncoder(</span></span>
<span class="line"><span style="color:#79B8FF;">    len</span><span style="color:#E1E4E8;">(src_vocab), key_size, query_size, value_size, num_hiddens,</span></span>
<span class="line"><span style="color:#E1E4E8;">    norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,</span></span>
<span class="line"><span style="color:#E1E4E8;">    num_layers, dropout)</span></span>
<span class="line"><span style="color:#E1E4E8;">decoder </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> TransformerDecoder(</span></span>
<span class="line"><span style="color:#79B8FF;">    len</span><span style="color:#E1E4E8;">(tgt_vocab), key_size, query_size, value_size, num_hiddens,</span></span>
<span class="line"><span style="color:#E1E4E8;">    norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,</span></span>
<span class="line"><span style="color:#E1E4E8;">    num_layers, dropout)</span></span>
<span class="line"><span style="color:#E1E4E8;">net </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> d2l.EncoderDecoder(encoder, decoder)</span></span>
<span class="line"><span style="color:#E1E4E8;">d2l.train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br></div></div><p><code>d2l.EncoderDecoder</code> 的 <a href="/aiart/deep-learning/rnn-modern.html#合并">EncoderDecoder</a></p></div></div><!----></main><!--[--><!--]--><footer class="VPContentDocFooter" data-v-c9771c35 data-v-5413c223><a class="prev-link" href="/aiart/deep-learning/attention-mechanisms.html" data-v-5413c223><span class="desc" data-v-5413c223><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="vt-link-icon" data-v-5413c223><path d="M15,19c-0.3,0-0.5-0.1-0.7-0.3l-6-6c-0.4-0.4-0.4-1,0-1.4l6-6c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4L10.4,12l5.3,5.3c0.4,0.4,0.4,1,0,1.4C15.5,18.9,15.3,19,15,19z"></path></svg> 上一篇</span><span class="title" data-v-5413c223>注意力机制(attention)</span></a><a class="next-link" href="/aiart/deep-learning/nlp.html" data-v-5413c223><span class="desc" data-v-5413c223>下一篇 <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="vt-link-icon" data-v-5413c223><path d="M9,19c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l5.3-5.3L8.3,6.7c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l6,6c0.4,0.4,0.4,1,0,1.4l-6,6C9.5,18.9,9.3,19,9,19z"></path></svg></span><span class="title" data-v-5413c223>自然语言处理(NLP)</span></a></footer></div></div></div></div><div class="visually-hidden" aria-live="polite" data-v-66aaf361>Transformer 架构 has loaded</div></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"aiart_chat-gpt.md\":\"D_AjEP0z\",\"aiart_comfyui_basic.md\":\"wvs4BTKB\",\"aiart_deep-learning_attention-mechanisms.md\":\"tynifhfg\",\"aiart_deep-learning_basic-concept.md\":\"BoQsdTl2\",\"aiart_deep-learning_cnn.md\":\"ByWdOvLt\",\"aiart_deep-learning_computer-vision.md\":\"BKMW-wkO\",\"aiart_deep-learning_gpt.md\":\"aUIEREkr\",\"aiart_deep-learning_linear-regression.md\":\"B9n7BHu3\",\"aiart_deep-learning_mathematics.md\":\"CJD4gT55\",\"aiart_deep-learning_nlp.md\":\"BUtBRvC0\",\"aiart_deep-learning_overview.md\":\"h0ZXP5gr\",\"aiart_deep-learning_rnn-modern.md\":\"LBPBzVc1\",\"aiart_deep-learning_rnn.md\":\"GxNcCd8u\",\"aiart_deep-learning_transformer.md\":\"9gX9HOBB\",\"aiart_huggingface_accelerate.md\":\"NAYj45Rl\",\"aiart_huggingface_config.md\":\"8Ccnmq6p\",\"aiart_huggingface_diffusers.md\":\"Dstc-3FX\",\"aiart_huggingface_pytorch.md\":\"CtJkazgT\",\"aiart_machine-learning_conda.md\":\"BHjV57wW\",\"aiart_machine-learning_kaggle.md\":\"CqlPytEC\",\"aiart_machine-learning_overview.md\":\"TBKTR3yP\",\"aiart_python_basic-info.md\":\"D3FB6PZ4\",\"aiart_python_oop.md\":\"D3hq_q3Y\",\"aiart_python_pandas.md\":\"BPgU6q3y\",\"aiart_python_pil.md\":\"BDGrusd1\",\"aiart_python_pip.md\":\"C0D7-mD4\",\"aiart_sunzi-war-art.md\":\"BPc2mfHw\",\"aiart_three-body.md\":\"-En2Kayv\",\"basic_javascript_array.md\":\"CSBeCnuV\",\"basic_javascript_bitwise-operators.md\":\"viKVg3CL\",\"basic_javascript_function.md\":\"GI9zvYnj\",\"basic_javascript_index.md\":\"B0tkDH6y\",\"basic_javascript_object.md\":\"CJojbMel\",\"basic_javascript_operators.md\":\"DaYvd2jS\",\"basic_javascript_promise.md\":\"hubFsCC_\",\"basic_javascript_proxy.md\":\"HJMZbDEW\",\"basic_javascript_reflect.md\":\"D4_FU9nW\",\"basic_javascript_symbol.md\":\"Xuj6DE1f\",\"basic_tools_cold-code.md\":\"DWZZK4MJ\",\"basic_tools_markdown.md\":\"Bru_pebV\",\"basic_tools_worker.md\":\"ENEuMZFx\",\"basic_typescript_config-file.md\":\"DMKa7uoL\",\"basic_typescript_declare.md\":\"C0Jo0HK7\",\"basic_typescript_index.md\":\"Co7Igk2U\",\"basic_typescript_tsx.md\":\"Tp9RBIif\",\"index.md\":\"D5m4R81Z\",\"investment_stock-prediction.md\":\"FcPojRcw\",\"mac_daily.md\":\"CQ2WQVz6\",\"mac_dev-tools.md\":\"BODSgj13\",\"mac_linux_os-command.md\":\"DXCaIPTI\",\"mac_linux_vim.md\":\"CFwtOTwl\",\"mac_setting.md\":\"C1KVPX_A\",\"mac_vscode.md\":\"87KvseEo\",\"mathstat_math_calculus.md\":\"go9hd_2-\",\"mathstat_math_math-symbol.md\":\"Tf5aAx9p\",\"mathstat_math_number-theory.md\":\"DQSKLiDD\",\"node_core_fs.md\":\"CGkxYZNW\",\"node_core_http.md\":\"C0QsVx7g\",\"node_core_index.md\":\"DeHpTNRY\",\"node_core_path.md\":\"DLFTIxUp\",\"node_core_process.md\":\"-GJxI6kJ\",\"node_core_url.md\":\"BEichTRH\",\"node_external_cli.md\":\"Bmdoyuag\",\"node_external_debug.md\":\"CDEtSMpb\",\"node_external_git.md\":\"DUu99lZY\",\"node_external_github.md\":\"C0xlJXQy\",\"node_external_npm-run-all.md\":\"DCLm_PL-\",\"node_external_npm.md\":\"BmJ7RRF5\",\"node_external_npx.md\":\"rUf7vhXu\",\"node_external_nvm.md\":\"pH9_ahj3\",\"node_external_package-json.md\":\"C3jzeRk7\",\"node_external_pnpm-monorepo.md\":\"CkKDDucK\",\"node_external_server.md\":\"CdIfxbvf\",\"node_external_websockets.md\":\"quDqJU4q\",\"vite_command-cli.md\":\"LcYeudqq\",\"vite_create-vite.md\":\"DKPTwAJ9\",\"vite_extend_rollup-source-build.md\":\"CG1aJcoF\",\"vite_extend_rollup-source-write.md\":\"D1ufq2-r\",\"vite_index.md\":\"C9DwhD1E\",\"vite_plugin.md\":\"BjcLhQ_1\",\"vite_press_press-command.md\":\"Ch788X_b\",\"vite_press_theme-default.md\":\"d-Gd1b_e\",\"vite_press_theme-vue.md\":\"C_Slhvbn\",\"vite_press_vitepress.md\":\"DyTM1l-K\",\"vite_rollup-source.md\":\"CP8Os-ci\",\"vite_rollup.md\":\"pQ9hgj0T\",\"vue_vue2_defineproperty.md\":\"BeXzVSVn\",\"vue_vue3_pinia-code.md\":\"BZrhMgGY\",\"vue_vue3_reactive.md\":\"q6pQL-Hx\",\"vue_vue3_router-code.md\":\"Cq7_XPU7\",\"vue_vue3_source-code.md\":\"D5bnbzxp\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"AI边城\",\"description\":\"A VitePress site\",\"base\":\"/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"nav\":[{\"text\":\"首页\",\"link\":\"/\"},{\"text\":\"AI·文艺\",\"activeMatch\":\"^/aiart\",\"link\":\"/aiart/machine-learning/overview\"},{\"text\":\"Vue·周边\",\"activeMatch\":\"^/vue/\",\"link\":\"/vue/vue3/source-code\"},{\"text\":\"Vite·Press\",\"activeMatch\":\"^/vite/\",\"link\":\"/vite/\"},{\"text\":\"Node·周边\",\"activeMatch\":\"^/node/\",\"link\":\"/node/core/\"},{\"text\":\"Js·Ts\",\"activeMatch\":\"^/basic\",\"link\":\"/basic/javascript/\"},{\"text\":\"Mac·Linux\",\"activeMatch\":\"^/mac\",\"link\":\"/mac/setting\"},{\"text\":\"数学·统计\",\"activeMatch\":\"^/mathstat\",\"link\":\"/mathstat/math/number-theory\"},{\"text\":\"投资·金融\",\"activeMatch\":\"^/investment\",\"link\":\"/investment/stock-prediction\"}],\"sidebar\":{\"/vue/\":[{\"text\":\"Vue3.x系列集锦\",\"items\":[{\"text\":\"Core流程源码摘要\",\"link\":\"/vue/vue3/source-code\"},{\"text\":\"Reactive响应式系统\",\"link\":\"/vue/vue3/reactive\"},{\"text\":\"Router源码摘要\",\"link\":\"/vue/vue3/router-code\"},{\"text\":\"Pinia源码摘要\",\"link\":\"/vue/vue3/pinia-code\"}]},{\"text\":\"Vue2原理\",\"items\":[{\"text\":\"DefineProperty\",\"link\":\"/vue/vue2/defineProperty\"}]}],\"/vite/\":[{\"text\":\"Vite原理与周边\",\"items\":[{\"text\":\"Vite基础\",\"link\":\"/vite/\"},{\"text\":\"vite命令源码摘要\",\"link\":\"/vite/command-cli\"},{\"text\":\"Vite自动生成项目原理\",\"link\":\"/vite/create-vite\"},{\"text\":\"Vite插件怎么写\",\"link\":\"/vite/plugin\"}]},{\"text\":\"Rollup.js\",\"items\":[{\"text\":\"安装命令及配置文件\",\"link\":\"/vite/rollup\"},{\"text\":\"rollup.js源码摘要\",\"link\":\"/vite/rollup-source\"}]},{\"text\":\"Vitepress\",\"items\":[{\"text\":\"简介与功能\",\"link\":\"/vite/press/vitepress\"},{\"text\":\"vitepress命令源码摘要\",\"link\":\"/vite/press/press-command\"},{\"text\":\"默认主题(default)\",\"link\":\"/vite/press/theme-default\"},{\"text\":\"Vue官方主题(Vue3)\",\"link\":\"/vite/press/theme-vue\"}]}],\"/node/\":[{\"text\":\"Node基础\",\"items\":[{\"text\":\"简介\",\"link\":\"/node/core/\"},{\"text\":\"fs文件系统及扩展\",\"link\":\"/node/core/fs\"},{\"text\":\"url模块\",\"link\":\"/node/core/url\"},{\"text\":\"path路径\",\"link\":\"/node/core/path\"},{\"text\":\"http模块\",\"link\":\"/node/core/http\"},{\"text\":\"process进程\",\"link\":\"/node/core/process\"}]},{\"text\":\"周边工具\",\"items\":[{\"text\":\"本地调试Node项目文件\",\"link\":\"/node/external/debug\"},{\"text\":\"写个Node命令行工具\",\"link\":\"/node/external/cli\"},{\"text\":\"npm\",\"link\":\"/node/external/npm\"},{\"text\":\"npx\",\"link\":\"/node/external/npx\"},{\"text\":\"package.json\",\"link\":\"/node/external/package-json\"},{\"text\":\"pnpm monorepo\",\"link\":\"/node/external/pnpm-monorepo\"},{\"text\":\"nvm 和 nrm\",\"link\":\"/node/external/nvm\"},{\"text\":\"git\",\"link\":\"/node/external/git\"},{\"text\":\"Github\",\"link\":\"/node/external/github\"},{\"text\":\"npm-run-all\",\"link\":\"/node/external/npm-run-all\"},{\"text\":\"服务器类型\",\"link\":\"/node/external/server\"},{\"text\":\"WebSocket\",\"link\":\"/node/external/websockets\"}]}],\"/basic/\":[{\"text\":\"JavaScript基础\",\"items\":[{\"text\":\"简介\",\"link\":\"/basic/javascript/\"},{\"text\":\"Array数组\",\"link\":\"/basic/javascript/array\"},{\"text\":\"Object对象\",\"link\":\"/basic/javascript/object\"},{\"text\":\"Function函数\",\"link\":\"/basic/javascript/function\"},{\"text\":\"Symbol标识符\",\"link\":\"/basic/javascript/symbol\"},{\"text\":\"Promise\",\"link\":\"/basic/javascript/promise\"},{\"text\":\"Proxy\",\"link\":\"/basic/javascript/proxy\"},{\"text\":\"Reflect\",\"link\":\"/basic/javascript/reflect\"},{\"text\":\"表达式与运算符\",\"link\":\"/basic/javascript/operators\"},{\"text\":\"位运算\",\"link\":\"/basic/javascript/bitwise-operators\"}]},{\"text\":\"TypeScript基础\",\"items\":[{\"text\":\"基本认知与区别\",\"link\":\"/basic/typescript/\"},{\"text\":\"tsconfig.json字段详解\",\"link\":\"/basic/typescript/config-file\"},{\"text\":\"declare及声明文件\",\"link\":\"/basic/typescript/declare\"},{\"text\":\"tsx执行ts及VSCode调试ts\",\"link\":\"/basic/typescript/tsx\"}]},{\"text\":\"周边及工具\",\"items\":[{\"text\":\"Markdown及生成目录\",\"link\":\"/basic/tools/markdown\"},{\"text\":\"工具函数-冷知识-酷代码\",\"link\":\"/basic/tools/cold-code\"},{\"text\":\"Web Worker\",\"link\":\"/basic/tools/worker\"}]}],\"/mac/\":[{\"text\":\"Mac那点事\",\"items\":[{\"text\":\"基本设置\",\"link\":\"/mac/setting\"},{\"text\":\"日常操作\",\"link\":\"/mac/daily\"},{\"text\":\"工具:brew/iTerm2\",\"link\":\"/mac/dev-tools\"},{\"text\":\"VSCode编辑器\",\"link\":\"/mac/vscode\"}]},{\"text\":\"Linux\",\"items\":[{\"text\":\"基础命令\",\"link\":\"/mac/linux/os-command\"},{\"text\":\"vim编辑器\",\"link\":\"/mac/linux/vim\"}]}],\"/aiart/\":[{\"text\":\"机器学习\",\"items\":[{\"text\":\"概览\",\"link\":\"/aiart/machine-learning/overview\"},{\"text\":\"conda\",\"link\":\"/aiart/machine-learning/conda\"},{\"text\":\"kaggle\",\"link\":\"/aiart/machine-learning/kaggle\"}]},{\"text\":\"深度学习\",\"items\":[{\"text\":\"概览\",\"link\":\"/aiart/deep-learning/overview\"},{\"text\":\"数学基础\",\"link\":\"/aiart/deep-learning/mathematics\"},{\"text\":\"基本概念\",\"link\":\"/aiart/deep-learning/basic-concept\"},{\"text\":\"线性回归\",\"link\":\"/aiart/deep-learning/linear-regression\"},{\"text\":\"卷积神经网络(CNN)\",\"link\":\"/aiart/deep-learning/cnn\"},{\"text\":\"循环神经网络(RNN)\",\"link\":\"/aiart/deep-learning/rnn\"},{\"text\":\"现代RNN\",\"link\":\"/aiart/deep-learning/rnn-modern\"},{\"text\":\"注意力机制(attention)\",\"link\":\"/aiart/deep-learning/attention-mechanisms\"},{\"text\":\"Transformer架构\",\"link\":\"/aiart/deep-learning/transformer\"},{\"text\":\"自然语言处理(NLP)\",\"link\":\"/aiart/deep-learning/nlp\"},{\"text\":\"GPT\",\"link\":\"/aiart/deep-learning/gpt\"},{\"text\":\"计算机视觉(CV)\",\"link\":\"/aiart/deep-learning/computer-vision\"}]},{\"text\":\"Huggingface\",\"items\":[{\"text\":\"基础配置\",\"link\":\"/aiart/huggingface/config\"},{\"text\":\"Diffusers\",\"link\":\"/aiart/huggingface/diffusers\"},{\"text\":\"Accelerate\",\"link\":\"/aiart/huggingface/accelerate\"},{\"text\":\"Pytorch\",\"link\":\"/aiart/huggingface/pytorch\"}]},{\"text\":\"ComfyUI\",\"items\":[{\"text\":\"基本概念\",\"link\":\"/aiart/comfyui/basic\"}]},{\"text\":\"Python\",\"items\":[{\"text\":\"安装与设置\",\"link\":\"/aiart/python/basic-info\"},{\"text\":\"pip\",\"link\":\"/aiart/python/pip\"},{\"text\":\"面向对象编程(OOP)\",\"link\":\"/aiart/python/oop\"},{\"text\":\"Pandas\",\"link\":\"/aiart/python/pandas\"},{\"text\":\"PIL\",\"link\":\"/aiart/python/pil\"}]},{\"text\":\"AI语录\",\"items\":[{\"text\":\"概率、记忆与意识\",\"link\":\"/aiart/three-body\"},{\"text\":\"ChatGPT问答集锦\",\"link\":\"/aiart/chat-gpt\"}]},{\"text\":\"文艺\",\"items\":[{\"text\":\"孙子兵法\",\"link\":\"/aiart/sunzi-war-art\"}]}],\"/mathstat\":[{\"text\":\"理论\",\"items\":[{\"text\":\"数的概念\",\"link\":\"/mathstat/math/number-theory\"},{\"text\":\"数学符号\",\"link\":\"/mathstat/math/math-symbol\"}]},{\"text\":\"微积分\",\"items\":[{\"text\":\"基本定理\",\"link\":\"/mathstat/math/calculus\"}]},{\"text\":\"统计学\",\"items\":[]}],\"/investment\":[{\"text\":\"投资与股票\",\"items\":[{\"text\":\"预测 or 投机\",\"link\":\"/investment/stock-prediction\"}]},{\"text\":\"金融与人性\",\"items\":[]}]},\"i18n\":{\"toc\":\"本页目录\",\"previous\":\"上一篇\",\"next\":\"下一篇\",\"pageNotFound\":\"页面未找到\"},\"socialLinks\":[{\"icon\":\"github\",\"link\":\"https://github.com/wswplay/wswplay.github.io\"}],\"footer\":{\"copyright\":\"Copyright © 2020-2025 边城\"}},\"locales\":{},\"scrollOffset\":[\"header\",\".VPLocalNav\"],\"cleanUrls\":false}");</script>
    
  </body>
</html>