<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>自然语言处理、NLP | AI边城</title>
    <meta name="description" content="A VitePress site">
    <meta name="generator" content="VitePress v1.6.4">
    <link rel="preload stylesheet" href="/assets/style.D0NeadeN.css" as="style">
    <link rel="preload stylesheet" href="/vp-icons.css" as="style">
    
    <script type="module" src="/assets/app.W6IxTJbf.js"></script>
    <link rel="modulepreload" href="/assets/chunks/theme.DGQczpCg.js">
    <link rel="modulepreload" href="/assets/chunks/framework.DUr976bL.js">
    <link rel="modulepreload" href="/assets/aiart_deep-learning_nlp.md.CNOa76hL.lean.js">
    <link rel="icon" href="/logo.svg">
    <link rel="icon" href="https://avatars.githubusercontent.com/u/13958395?v=4">
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
    <link rel="preload" href="/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
  </head>
  <body>
    <div id="app"><div class="VPApp" data-v-ed1e6441><!--[--><span tabindex="-1" data-v-ee79f3b4></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-ee79f3b4>Skip to content</a><!--]--><!----><!--[--><!--]--><header class="VPNav nav-bar" data-v-ed1e6441 data-v-047273c7><div class="VPNavBar" data-v-047273c7 data-v-3553c024><div class="container" data-v-3553c024><a class="VPNavBarTitle" href="/" data-v-3553c024 data-v-3dacf9b5><!--[--><!--[--><!--[--><!--[--><div class="nav-bar-title" data-v-429880b3><div class="bar-logo" data-v-429880b3><img src="https://avatars.githubusercontent.com/u/13958395?v=4" alt="心水喵论" data-v-429880b3></div><div class="title-text" data-v-429880b3>AI边城</div></div><!--]--><!--]--><!--]--><!--]--></a><div class="content" data-v-3553c024><!----><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-3553c024 data-v-12925b72><span id="main-nav-aria-label" class="visually-hidden" data-v-12925b72>Main Navigation</span><!--[--><!--[--><a class="vt-link link VPNavBarMenuLink" href="/" data-v-12925b72 data-v-e1703463><!--[-->首页<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link VPNavBarMenuLink active" href="/aiart/machine-learning/overview.html" data-v-12925b72 data-v-e1703463><!--[-->AI·文艺<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link VPNavBarMenuLink" href="/vue/vue3/source-code.html" data-v-12925b72 data-v-e1703463><!--[-->Vue·周边<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link VPNavBarMenuLink" href="/vite/" data-v-12925b72 data-v-e1703463><!--[-->Vite·Press<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link VPNavBarMenuLink" href="/node/core/" data-v-12925b72 data-v-e1703463><!--[-->Node·周边<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link VPNavBarMenuLink" href="/basic/javascript/" data-v-12925b72 data-v-e1703463><!--[-->Js·Ts<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link VPNavBarMenuLink" href="/mac/setting.html" data-v-12925b72 data-v-e1703463><!--[-->Mac·Linux<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link VPNavBarMenuLink" href="/mathstat/math/number-theory.html" data-v-12925b72 data-v-e1703463><!--[-->数学·统计<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link VPNavBarMenuLink" href="/investment/stock-prediction.html" data-v-12925b72 data-v-e1703463><!--[-->投资·金融<!--]--><!----><!----></a><!--]--><!--]--><!----></nav><div class="VPNavBarAppearance appearance" data-v-3553c024 data-v-83e4122c><button class="vt-switch vt-switch-appearance" type="button" role="switch" aria-label="Toggle dark mode" aria-checked="false" data-v-83e4122c><span class="vt-switch-check"><span class="vt-switch-icon"><!--[--><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="vt-switch-appearance-sun"><path d="M12,18c-3.3,0-6-2.7-6-6s2.7-6,6-6s6,2.7,6,6S15.3,18,12,18zM12,8c-2.2,0-4,1.8-4,4c0,2.2,1.8,4,4,4c2.2,0,4-1.8,4-4C16,9.8,14.2,8,12,8z"></path><path d="M12,4c-0.6,0-1-0.4-1-1V1c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,3.6,12.6,4,12,4z"></path><path d="M12,24c-0.6,0-1-0.4-1-1v-2c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,23.6,12.6,24,12,24z"></path><path d="M5.6,6.6c-0.3,0-0.5-0.1-0.7-0.3L3.5,4.9c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C6.2,6.5,5.9,6.6,5.6,6.6z"></path><path d="M19.8,20.8c-0.3,0-0.5-0.1-0.7-0.3l-1.4-1.4c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C20.3,20.7,20,20.8,19.8,20.8z"></path><path d="M3,13H1c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S3.6,13,3,13z"></path><path d="M23,13h-2c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S23.6,13,23,13z"></path><path d="M4.2,20.8c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C4.7,20.7,4.5,20.8,4.2,20.8z"></path><path d="M18.4,6.6c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C18.9,6.5,18.6,6.6,18.4,6.6z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="vt-switch-appearance-moon"><path d="M12.1,22c-0.3,0-0.6,0-0.9,0c-5.5-0.5-9.5-5.4-9-10.9c0.4-4.8,4.2-8.6,9-9c0.4,0,0.8,0.2,1,0.5c0.2,0.3,0.2,0.8-0.1,1.1c-2,2.7-1.4,6.4,1.3,8.4c2.1,1.6,5,1.6,7.1,0c0.3-0.2,0.7-0.3,1.1-0.1c0.3,0.2,0.5,0.6,0.5,1c-0.2,2.7-1.5,5.1-3.6,6.8C16.6,21.2,14.4,22,12.1,22zM9.3,4.4c-2.9,1-5,3.6-5.2,6.8c-0.4,4.4,2.8,8.3,7.2,8.7c2.1,0.2,4.2-0.4,5.8-1.8c1.1-0.9,1.9-2.1,2.4-3.4c-2.5,0.9-5.3,0.5-7.5-1.1C9.2,11.4,8.1,7.7,9.3,4.4z"></path></svg><!--]--></span></span></button></div><div class="vt-social-links VPNavBarSocialLinks social-links" data-v-3553c024 data-v-32aadcb8><!--[--><a class="vt-social-link is-small" href="https://github.com/wswplay/wswplay.github.io" title="github" target="_blank" rel="noopener noreferrer"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="vt-social-link-icon"><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"></path></svg><span class="visually-hidden">github</span></a><!--]--></div><div class="vt-flyout VPNavBarExtra extra" data-v-3553c024 data-v-100e400a><button type="button" class="vt-flyout-button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation"><!--[--><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="vt-flyout-button-icon"><circle cx="12" cy="12" r="2"></circle><circle cx="19" cy="12" r="2"></circle><circle cx="5" cy="12" r="2"></circle></svg><!--]--></button><div class="vt-flyout-menu"><div class="vt-menu"><!----><!--[--><!--[--><div class="vt-menu-group" data-v-100e400a><div class="vt-menu-item item" data-v-100e400a><p class="vt-menu-label" data-v-100e400a>Appearance</p><div class="vt-menu-action action" data-v-100e400a><button class="vt-switch vt-switch-appearance" type="button" role="switch" aria-label="Toggle dark mode" aria-checked="false" data-v-100e400a><span class="vt-switch-check"><span class="vt-switch-icon"><!--[--><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="vt-switch-appearance-sun"><path d="M12,18c-3.3,0-6-2.7-6-6s2.7-6,6-6s6,2.7,6,6S15.3,18,12,18zM12,8c-2.2,0-4,1.8-4,4c0,2.2,1.8,4,4,4c2.2,0,4-1.8,4-4C16,9.8,14.2,8,12,8z"></path><path d="M12,4c-0.6,0-1-0.4-1-1V1c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,3.6,12.6,4,12,4z"></path><path d="M12,24c-0.6,0-1-0.4-1-1v-2c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,23.6,12.6,24,12,24z"></path><path d="M5.6,6.6c-0.3,0-0.5-0.1-0.7-0.3L3.5,4.9c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C6.2,6.5,5.9,6.6,5.6,6.6z"></path><path d="M19.8,20.8c-0.3,0-0.5-0.1-0.7-0.3l-1.4-1.4c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C20.3,20.7,20,20.8,19.8,20.8z"></path><path d="M3,13H1c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S3.6,13,3,13z"></path><path d="M23,13h-2c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S23.6,13,23,13z"></path><path d="M4.2,20.8c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C4.7,20.7,4.5,20.8,4.2,20.8z"></path><path d="M18.4,6.6c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C18.9,6.5,18.6,6.6,18.4,6.6z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="vt-switch-appearance-moon"><path d="M12.1,22c-0.3,0-0.6,0-0.9,0c-5.5-0.5-9.5-5.4-9-10.9c0.4-4.8,4.2-8.6,9-9c0.4,0,0.8,0.2,1,0.5c0.2,0.3,0.2,0.8-0.1,1.1c-2,2.7-1.4,6.4,1.3,8.4c2.1,1.6,5,1.6,7.1,0c0.3-0.2,0.7-0.3,1.1-0.1c0.3,0.2,0.5,0.6,0.5,1c-0.2,2.7-1.5,5.1-3.6,6.8C16.6,21.2,14.4,22,12.1,22zM9.3,4.4c-2.9,1-5,3.6-5.2,6.8c-0.4,4.4,2.8,8.3,7.2,8.7c2.1,0.2,4.2-0.4,5.8-1.8c1.1-0.9,1.9-2.1,2.4-3.4c-2.5,0.9-5.3,0.5-7.5-1.1C9.2,11.4,8.1,7.7,9.3,4.4z"></path></svg><!--]--></span></span></button></div></div></div><div class="vt-menu-group" data-v-100e400a><div class="vt-menu-item item" data-v-100e400a><div class="vt-social-links social-links" data-v-100e400a><!--[--><a class="vt-social-link is-small" href="https://github.com/wswplay/wswplay.github.io" title="github" target="_blank" rel="noopener noreferrer"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="vt-social-link-icon"><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"></path></svg><span class="visually-hidden">github</span></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><button type="button" class="vt-hamburger VPNavBarHamburger hamburger" aria-label="Mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-3553c024 data-v-00a3c66f><span class="vt-hamburger-container"><span class="vt-hamburger-top"></span><span class="vt-hamburger-middle"></span><span class="vt-hamburger-bottom"></span></span></button></div></div></div><!----></header><div class="VPLocalNav" data-v-ed1e6441 data-v-2d570dc4><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-2d570dc4><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="menu-icon" data-v-2d570dc4><path d="M17,11H3c-0.6,0-1-0.4-1-1s0.4-1,1-1h14c0.6,0,1,0.4,1,1S17.6,11,17,11z"></path><path d="M21,7H3C2.4,7,2,6.6,2,6s0.4-1,1-1h18c0.6,0,1,0.4,1,1S21.6,7,21,7z"></path><path d="M21,15H3c-0.6,0-1-0.4-1-1s0.4-1,1-1h18c0.6,0,1,0.4,1,1S21.6,15,21,15z"></path><path d="M17,19H3c-0.6,0-1-0.4-1-1s0.4-1,1-1h14c0.6,0,1,0.4,1,1S17.6,19,17,19z"></path></svg><span class="menu-text" data-v-2d570dc4>Menu</span></button><div class="VPLocalNavOutlineDropdown" style="--vt-vh:0px;" data-v-2d570dc4 data-v-0a582018><button class="" data-v-0a582018>本页目录 <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="icon" data-v-0a582018><path d="M9,19c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l5.3-5.3L8.3,6.7c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l6,6c0.4,0.4,0.4,1,0,1.4l-6,6C9.5,18.9,9.3,19,9,19z"></path></svg></button><!----></div></div><aside class="VPSidebar" data-v-ed1e6441 data-v-b9a24e11><nav id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-b9a24e11><!--[--><!--]--><span id="sidebar-aria-label" class="visually-hidden" data-v-b9a24e11>Sidebar Navigation</span><!--[--><div class="group" data-v-b9a24e11><section class="VPSidebarGroup" data-v-b9a24e11 data-v-df257b7a><div class="title" data-v-df257b7a><h2 class="title-text" data-v-df257b7a>机器学习<!----></h2></div><!--[--><a class="link" href="/aiart/machine-learning/overview.html" data-v-df257b7a data-v-8ed2dcd9><p class="link-text" data-v-8ed2dcd9>概览<!----></p></a><a class="link" href="/aiart/machine-learning/conda.html" data-v-df257b7a data-v-8ed2dcd9><p class="link-text" data-v-8ed2dcd9>conda<!----></p></a><a class="link" href="/aiart/machine-learning/kaggle.html" data-v-df257b7a data-v-8ed2dcd9><p class="link-text" data-v-8ed2dcd9>kaggle<!----></p></a><!--]--></section></div><div class="group" data-v-b9a24e11><section class="VPSidebarGroup" data-v-b9a24e11 data-v-df257b7a><div class="title" data-v-df257b7a><h2 class="active title-text" data-v-df257b7a>深度学习<!----></h2></div><!--[--><a class="link" href="/aiart/deep-learning/overview.html" data-v-df257b7a data-v-8ed2dcd9><p class="link-text" data-v-8ed2dcd9>概览<!----></p></a><a class="link" href="/aiart/deep-learning/mathematics.html" data-v-df257b7a data-v-8ed2dcd9><p class="link-text" data-v-8ed2dcd9>数学基础<!----></p></a><a class="link" href="/aiart/deep-learning/pytorch.html" data-v-df257b7a data-v-8ed2dcd9><p class="link-text" data-v-8ed2dcd9>Pytorch<!----></p></a><a class="link" href="/aiart/deep-learning/basic-concept.html" data-v-df257b7a data-v-8ed2dcd9><p class="link-text" data-v-8ed2dcd9>基本概念<!----></p></a><a class="link" href="/aiart/deep-learning/linear-regression.html" data-v-df257b7a data-v-8ed2dcd9><p class="link-text" data-v-8ed2dcd9>线性回归<!----></p></a><a class="link" href="/aiart/deep-learning/cnn.html" data-v-df257b7a data-v-8ed2dcd9><p class="link-text" data-v-8ed2dcd9>卷积神经网络(CNN)<!----></p></a><a class="link" href="/aiart/deep-learning/rnn.html" data-v-df257b7a data-v-8ed2dcd9><p class="link-text" data-v-8ed2dcd9>循环神经网络(RNN)<!----></p></a><a class="link" href="/aiart/deep-learning/rnn-modern.html" data-v-df257b7a data-v-8ed2dcd9><p class="link-text" data-v-8ed2dcd9>现代RNN<!----></p></a><a class="link" href="/aiart/deep-learning/attention-mechanisms.html" data-v-df257b7a data-v-8ed2dcd9><p class="link-text" data-v-8ed2dcd9>注意力机制(attention)<!----></p></a><a class="link" href="/aiart/deep-learning/transformer.html" data-v-df257b7a data-v-8ed2dcd9><p class="link-text" data-v-8ed2dcd9>Transformer架构<!----></p></a><a class="link" href="/aiart/deep-learning/nlp.html" data-v-df257b7a data-v-8ed2dcd9><p class="link-text" data-v-8ed2dcd9>自然语言处理(NLP)<!----></p></a><a class="link" href="/aiart/deep-learning/gpt.html" data-v-df257b7a data-v-8ed2dcd9><p class="link-text" data-v-8ed2dcd9>GPT<!----></p></a><a class="link" href="/aiart/deep-learning/computer-vision.html" data-v-df257b7a data-v-8ed2dcd9><p class="link-text" data-v-8ed2dcd9>计算机视觉(CV)<!----></p></a><!--]--></section></div><div class="group" data-v-b9a24e11><section class="VPSidebarGroup" data-v-b9a24e11 data-v-df257b7a><div class="title" data-v-df257b7a><h2 class="title-text" data-v-df257b7a>Huggingface<!----></h2></div><!--[--><a class="link" href="/aiart/huggingface/config.html" data-v-df257b7a data-v-8ed2dcd9><p class="link-text" data-v-8ed2dcd9>基础配置<!----></p></a><a class="link" href="/aiart/huggingface/diffusers.html" data-v-df257b7a data-v-8ed2dcd9><p class="link-text" data-v-8ed2dcd9>Diffusers<!----></p></a><a class="link" href="/aiart/huggingface/accelerate.html" data-v-df257b7a data-v-8ed2dcd9><p class="link-text" data-v-8ed2dcd9>Accelerate<!----></p></a><!--]--></section></div><div class="group" data-v-b9a24e11><section class="VPSidebarGroup" data-v-b9a24e11 data-v-df257b7a><div class="title" data-v-df257b7a><h2 class="title-text" data-v-df257b7a>ComfyUI<!----></h2></div><!--[--><a class="link" href="/aiart/comfyui/basic.html" data-v-df257b7a data-v-8ed2dcd9><p class="link-text" data-v-8ed2dcd9>基本概念<!----></p></a><!--]--></section></div><div class="group" data-v-b9a24e11><section class="VPSidebarGroup" data-v-b9a24e11 data-v-df257b7a><div class="title" data-v-df257b7a><h2 class="title-text" data-v-df257b7a>Python<!----></h2></div><!--[--><a class="link" href="/aiart/python/basic-info.html" data-v-df257b7a data-v-8ed2dcd9><p class="link-text" data-v-8ed2dcd9>安装与设置<!----></p></a><a class="link" href="/aiart/python/pip.html" data-v-df257b7a data-v-8ed2dcd9><p class="link-text" data-v-8ed2dcd9>pip<!----></p></a><a class="link" href="/aiart/python/oop.html" data-v-df257b7a data-v-8ed2dcd9><p class="link-text" data-v-8ed2dcd9>面向对象编程(OOP)<!----></p></a><a class="link" href="/aiart/python/pandas.html" data-v-df257b7a data-v-8ed2dcd9><p class="link-text" data-v-8ed2dcd9>Pandas<!----></p></a><a class="link" href="/aiart/python/pil.html" data-v-df257b7a data-v-8ed2dcd9><p class="link-text" data-v-8ed2dcd9>PIL<!----></p></a><!--]--></section></div><div class="group" data-v-b9a24e11><section class="VPSidebarGroup" data-v-b9a24e11 data-v-df257b7a><div class="title" data-v-df257b7a><h2 class="title-text" data-v-df257b7a>AI语录<!----></h2></div><!--[--><a class="link" href="/aiart/three-body.html" data-v-df257b7a data-v-8ed2dcd9><p class="link-text" data-v-8ed2dcd9>概率、记忆与意识<!----></p></a><a class="link" href="/aiart/chat-gpt.html" data-v-df257b7a data-v-8ed2dcd9><p class="link-text" data-v-8ed2dcd9>ChatGPT问答集锦<!----></p></a><!--]--></section></div><div class="group" data-v-b9a24e11><section class="VPSidebarGroup" data-v-b9a24e11 data-v-df257b7a><div class="title" data-v-df257b7a><h2 class="title-text" data-v-df257b7a>文艺<!----></h2></div><!--[--><a class="link" href="/aiart/sunzi-war-art.html" data-v-df257b7a data-v-8ed2dcd9><p class="link-text" data-v-8ed2dcd9>孙子兵法<!----></p></a><!--]--></section></div><!--]--><!--[--><!--]--></nav></aside><div id="VPContent" class="VPContent has-sidebar" data-v-ed1e6441 data-v-5152f2e0><div class="VPContentDoc has-aside has-sidebar" data-v-5152f2e0 data-v-5d3992ac><div class="container" data-v-5d3992ac><div class="aside" data-v-5d3992ac><div class="aside-container" data-v-5d3992ac><!--[--><!--]--><div class="VPContentDocOutline" data-v-5d3992ac data-v-f519e333><div class="outline-marker" data-v-f519e333></div><div class="outline-title" data-v-f519e333>本页目录</div><nav aria-labelledby="doc-outline-aria-label" data-v-f519e333><span id="doc-outline-aria-label" class="visually-hidden" data-v-f519e333>Table of Contents for current page</span><ul class="root" data-v-f519e333 data-v-5b6e5798><!--[--><li data-v-5b6e5798><a class="outline-link" href="#预训练-pre-training" style="" data-v-5b6e5798>预训练(pre-training)</a><ul class="nested" data-v-5b6e5798 data-v-5b6e5798><!--[--><li data-v-5b6e5798><a class="outline-link" href="#子词嵌入" style="" data-v-5b6e5798>子词嵌入</a><!----></li><li data-v-5b6e5798><a class="outline-link" href="#字节对编码-bpe" style="" data-v-5b6e5798>字节对编码(BPE)</a><!----></li><li data-v-5b6e5798><a class="outline-link" href="#transformers-的-bert" style="" data-v-5b6e5798>Transformers 的 BERT</a><!----></li><!--]--></ul></li><!--]--></ul></nav></div><!--[--><!--[--><!--[--><div class="poetry" data-v-5b0a57b7>给每一条河每一座山取一个温暖的名字，我有一所房子，面朝大海，春暖花开。</div><!--]--><!--]--><!--]--><!----><!--[--><!--]--></div></div><div class="content" data-v-5d3992ac><!--[--><!--]--><main data-v-5d3992ac><div style="position:relative;" class="vt-doc aiart" data-v-5d3992ac><div><h1 id="自然语言处理-nlp" tabindex="-1">自然语言处理(NLP) <a class="header-anchor" href="#自然语言处理-nlp" aria-label="Permalink to &quot;自然语言处理(NLP)&quot;">​</a></h1><p><strong>NLP</strong>：Natural Language Processing，是指研究使用自然语言的<strong>计算机和人类</strong>之间的<strong>交互</strong>。</p><h2 id="预训练-pre-training" tabindex="-1">预训练(pre-training) <a class="header-anchor" href="#预训练-pre-training" aria-label="Permalink to &quot;预训练(pre-training)&quot;">​</a></h2><p><strong>自监督学习</strong><sup>self-supervised learning</sup>已被广泛用于<strong>预训练</strong>文本表示，例如通过使用周围文本的其它部分来预测文本的隐藏部分。</p><h3 id="子词嵌入" tabindex="-1">子词嵌入 <a class="header-anchor" href="#子词嵌入" aria-label="Permalink to &quot;子词嵌入&quot;">​</a></h3><div class="language-py line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">py</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#6A737D;"># 代码示例（Hugging Face 库）</span></span>
<span class="line"><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> transformers </span><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> AutoTokenizer</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 加载BERT的WordPiece分词器</span></span>
<span class="line"><span style="color:#E1E4E8;">tokenizer </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> AutoTokenizer.from_pretrained(</span><span style="color:#9ECBFF;">&quot;bert-base-uncased&quot;</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">tokens </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> tokenizer.tokenize(</span><span style="color:#9ECBFF;">&quot;unhappiness&quot;</span><span style="color:#E1E4E8;">)  </span><span style="color:#6A737D;"># 输出：[&#39;un&#39;, &#39;##happy&#39;, &#39;##ness&#39;]</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 加载BPE分词器（GPT-2）</span></span>
<span class="line"><span style="color:#E1E4E8;">tokenizer </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> AutoTokenizer.from_pretrained(</span><span style="color:#9ECBFF;">&quot;gpt2&quot;</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">tokens </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> tokenizer.tokenize(</span><span style="color:#9ECBFF;">&quot;unhappiness&quot;</span><span style="color:#E1E4E8;">)  </span><span style="color:#6A737D;"># 输出：[&#39;un&#39;, &#39;happiness&#39;]</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><p>自然语言是用来表达人脑思维的复杂系统，<strong>词</strong>是意义的基本单元。</p><ul><li><strong>词向量</strong>：用于表示<strong>单词意义</strong>的向量，并且还可以被认为是单词<strong>特征向量或表示</strong>。</li><li><strong>词嵌入</strong>：将<strong>单词映射到实向量</strong>的技术。</li></ul><p><strong>子词嵌入</strong><sup>Subword Embedding</sup>通过<strong>将单词拆分为更小的单元</strong>（子词）来解决传统词嵌入局限性。</p><ul><li>未登录词（OOV）问题：传统词嵌入（如 Word2Vec）无法处理词汇表外的单词。</li><li>形态学相似性：子词能捕捉单词间的结构关系（如&quot;running&quot; → &quot;run&quot; + &quot;ing&quot;）。</li><li>多语言支持：共享子词可跨语言建模（如拉丁语系的共同词根）。</li></ul><p><strong>子词嵌入</strong> = <strong>子词分词</strong>(<code>BPE</code>等) + <strong>向量化</strong><sup>Embedding</sup></p><h3 id="字节对编码-bpe" tabindex="-1">字节对编码(BPE) <a class="header-anchor" href="#字节对编码-bpe" aria-label="Permalink to &quot;字节对编码(BPE)&quot;">​</a></h3><p><strong>BPE</strong>：Byte Pair Encoding，是一种<strong>子词分词算法</strong><sup>Subword Tokenization</sup>。</p><p><strong>核心思想</strong>：通过迭代<strong>合并高频符号对</strong>(字节对)构建词汇表，有效平衡词表大小与语义覆盖能力。</p><p><strong>基本步骤</strong>：</p><ol><li><strong>初始化词汇表</strong>：将所有基本字符（如字母、标点）加入词表。</li><li><strong>统计符号对频率</strong>：在训练语料中统计所有相邻符号对的共现频率。</li><li><strong>合并最高频对</strong>：将出现频率最高的符号对合并为一个新符号，加入词表。</li><li><strong>重复迭代</strong>：持续合并直到达到预设的词表大小或迭代次数。</li></ol><p><strong>示例演示</strong>：</p><p>假设语料为 <code>&quot;low low low lower newest widest&quot;</code>：</p><ul><li>初始词表：<code>{l, o, w, e, r, n, s, t, d, i}</code></li><li>第 1 步：最高频对 <code>&quot;lo&quot;</code>（出现 6 次）→ 合并为 <code>&quot;lo&quot;</code>，词表新增 <code>&quot;lo&quot;</code>。</li><li>第 2 步：最高频对 <code>&quot;low&quot;</code>（出现 4 次）→ 合并为 <code>&quot;low&quot;</code>，词表新增 <code>&quot;low&quot;</code>。</li><li>后续可能合并 <code>&quot;er&quot;</code>、<code>&quot;est&quot;</code>等。</li><li>最终词表可能包含子词如：<code>low, er, est, newer, wid</code>。</li></ul><p><strong>优点优势</strong>：</p><ul><li><strong>解决未登录词（OOV）</strong>：通过子词组合表示罕见词（如 <code>&quot;unhappiness&quot;</code> → <code>&quot;un&quot; + &quot;happy&quot; + &quot;ness&quot;</code>）。</li><li><strong>压缩词表大小</strong>：避免维护超大词表（如英语常用词约 20 万，BPE 词表可压缩到 1 万~3 万）。</li><li><strong>保留语义</strong>：相似词共享子词（如 <code>&quot;playing&quot;</code> 和 <code>&quot;played&quot;</code> 共享 <code>&quot;play&quot;</code>）。</li></ul><p><strong>代码示例</strong>：</p><div class="language-py line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">py</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#6A737D;"># Hugging Face 库调用</span></span>
<span class="line"><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> transformers </span><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> GPT2Tokenizer</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># GPT-2使用BPE分词</span></span>
<span class="line"><span style="color:#E1E4E8;">tokenizer </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> GPT2Tokenizer.from_pretrained(</span><span style="color:#9ECBFF;">&quot;gpt2&quot;</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">tokens </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> tokenizer.tokenize(</span><span style="color:#9ECBFF;">&quot;unhappiness&quot;</span><span style="color:#E1E4E8;">)  </span><span style="color:#6A737D;"># 输出：[&#39;un&#39;, &#39;happiness&#39;]</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p><strong>代码实现</strong>：</p><div class="language-py line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">py</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> collections</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">symbols </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> [</span><span style="color:#9ECBFF;">&#39;a&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;b&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;c&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;d&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;e&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;f&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;g&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;h&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;i&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;j&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;k&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;l&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;m&#39;</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#9ECBFF;">           &#39;n&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;o&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;p&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;q&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;r&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;s&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;t&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;u&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;v&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;w&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;x&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;y&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;z&#39;</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#9ECBFF;">           &#39;_&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;[UNK]&#39;</span><span style="color:#E1E4E8;">]</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 频率字典</span></span>
<span class="line"><span style="color:#E1E4E8;">raw_token_freqs </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> {</span><span style="color:#9ECBFF;">&#39;fast_&#39;</span><span style="color:#E1E4E8;">: </span><span style="color:#79B8FF;">4</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;faster_&#39;</span><span style="color:#E1E4E8;">: </span><span style="color:#79B8FF;">3</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;tall_&#39;</span><span style="color:#E1E4E8;">: </span><span style="color:#79B8FF;">5</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;taller_&#39;</span><span style="color:#E1E4E8;">: </span><span style="color:#79B8FF;">4</span><span style="color:#E1E4E8;">}</span></span>
<span class="line"><span style="color:#E1E4E8;">token_freqs </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> {}</span></span>
<span class="line"><span style="color:#F97583;">for</span><span style="color:#E1E4E8;"> token, freq </span><span style="color:#F97583;">in</span><span style="color:#E1E4E8;"> raw_token_freqs.items():</span></span>
<span class="line"><span style="color:#E1E4E8;">  token_freqs[</span><span style="color:#9ECBFF;">&#39; &#39;</span><span style="color:#E1E4E8;">.join(</span><span style="color:#79B8FF;">list</span><span style="color:#E1E4E8;">(token))] </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> raw_token_freqs[token]</span></span>
<span class="line"><span style="color:#E1E4E8;">token_freqs</span></span>
<span class="line"><span style="color:#6A737D;"># {&#39;f a s t _&#39;: 4, &#39;f a s t e r _&#39;: 3, &#39;t a l l _&#39;: 5, &#39;t a l l e r _&#39;: 4}</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 返回词内最频繁的连续符号对，其中词来自输入词典token_freqs的键</span></span>
<span class="line"><span style="color:#F97583;">def</span><span style="color:#B392F0;"> get_max_freq_pair</span><span style="color:#E1E4E8;">(token_freqs):</span></span>
<span class="line"><span style="color:#E1E4E8;">  pairs </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> collections.defaultdict(</span><span style="color:#79B8FF;">int</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#F97583;">  for</span><span style="color:#E1E4E8;"> token, freq </span><span style="color:#F97583;">in</span><span style="color:#E1E4E8;"> token_freqs.items():</span></span>
<span class="line"><span style="color:#E1E4E8;">    symbols </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> token.split()</span></span>
<span class="line"><span style="color:#F97583;">    for</span><span style="color:#E1E4E8;"> i </span><span style="color:#F97583;">in</span><span style="color:#79B8FF;"> range</span><span style="color:#E1E4E8;">(</span><span style="color:#79B8FF;">len</span><span style="color:#E1E4E8;">(symbols) </span><span style="color:#F97583;">-</span><span style="color:#79B8FF;"> 1</span><span style="color:#E1E4E8;">):</span></span>
<span class="line"><span style="color:#6A737D;">      # “pairs”的键是两个连续符号的元组</span></span>
<span class="line"><span style="color:#E1E4E8;">      pairs[symbols[i], symbols[i </span><span style="color:#F97583;">+</span><span style="color:#79B8FF;"> 1</span><span style="color:#E1E4E8;">]] </span><span style="color:#F97583;">+=</span><span style="color:#E1E4E8;"> freq</span></span>
<span class="line"><span style="color:#F97583;">  return</span><span style="color:#79B8FF;"> max</span><span style="color:#E1E4E8;">(pairs, </span><span style="color:#FFAB70;">key</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">pairs.get)  </span><span style="color:#6A737D;"># 具有最大值的“pairs”键</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 基于连续符号频率的贪心方法，合并最频繁的连续符号对以产生新符号</span></span>
<span class="line"><span style="color:#F97583;">def</span><span style="color:#B392F0;"> merge_symbols</span><span style="color:#E1E4E8;">(max_freq_pair, token_freqs, symbols):</span></span>
<span class="line"><span style="color:#E1E4E8;">  symbols.append(</span><span style="color:#9ECBFF;">&#39;&#39;</span><span style="color:#E1E4E8;">.join(max_freq_pair))</span></span>
<span class="line"><span style="color:#E1E4E8;">  new_token_freqs </span><span style="color:#F97583;">=</span><span style="color:#79B8FF;"> dict</span><span style="color:#E1E4E8;">()</span></span>
<span class="line"><span style="color:#F97583;">  for</span><span style="color:#E1E4E8;"> token, freq </span><span style="color:#F97583;">in</span><span style="color:#E1E4E8;"> token_freqs.items():</span></span>
<span class="line"><span style="color:#E1E4E8;">    new_token </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> token.replace(</span><span style="color:#9ECBFF;">&#39; &#39;</span><span style="color:#E1E4E8;">.join(max_freq_pair), </span><span style="color:#9ECBFF;">&#39;&#39;</span><span style="color:#E1E4E8;">.join(max_freq_pair))</span></span>
<span class="line"><span style="color:#E1E4E8;">    new_token_freqs[new_token] </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> token_freqs[token]</span></span>
<span class="line"><span style="color:#F97583;">  return</span><span style="color:#E1E4E8;"> new_token_freqs</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 迭代</span></span>
<span class="line"><span style="color:#E1E4E8;">num_merges </span><span style="color:#F97583;">=</span><span style="color:#79B8FF;"> 10</span></span>
<span class="line"><span style="color:#F97583;">for</span><span style="color:#E1E4E8;"> i </span><span style="color:#F97583;">in</span><span style="color:#79B8FF;"> range</span><span style="color:#E1E4E8;">(num_merges):</span></span>
<span class="line"><span style="color:#E1E4E8;">  max_freq_pair </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> get_max_freq_pair(token_freqs)</span></span>
<span class="line"><span style="color:#E1E4E8;">  token_freqs </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> merge_symbols(max_freq_pair, token_freqs, symbols)</span></span>
<span class="line"><span style="color:#79B8FF;">  print</span><span style="color:#E1E4E8;">(</span><span style="color:#F97583;">f</span><span style="color:#9ECBFF;">&#39;合并# </span><span style="color:#79B8FF;">{</span><span style="color:#E1E4E8;">i</span><span style="color:#F97583;">+</span><span style="color:#79B8FF;">1}</span><span style="color:#9ECBFF;">:&#39;</span><span style="color:#E1E4E8;">,max_freq_pair)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 合并# 1: (&#39;t&#39;, &#39;a&#39;)</span></span>
<span class="line"><span style="color:#6A737D;"># 合并# 2: (&#39;ta&#39;, &#39;l&#39;)</span></span>
<span class="line"><span style="color:#6A737D;"># 合并# 3: (&#39;tal&#39;, &#39;l&#39;)</span></span>
<span class="line"><span style="color:#6A737D;"># 合并# 4: (&#39;f&#39;, &#39;a&#39;)</span></span>
<span class="line"><span style="color:#6A737D;"># 合并# 5: (&#39;fa&#39;, &#39;s&#39;)</span></span>
<span class="line"><span style="color:#6A737D;"># 合并# 6: (&#39;fas&#39;, &#39;t&#39;)</span></span>
<span class="line"><span style="color:#6A737D;"># 合并# 7: (&#39;e&#39;, &#39;r&#39;)</span></span>
<span class="line"><span style="color:#6A737D;"># 合并# 8: (&#39;er&#39;, &#39;_&#39;)</span></span>
<span class="line"><span style="color:#6A737D;"># 合并# 9: (&#39;tall&#39;, &#39;_&#39;)</span></span>
<span class="line"><span style="color:#6A737D;"># 合并# 10: (&#39;fast&#39;, &#39;_&#39;)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#79B8FF;">print</span><span style="color:#E1E4E8;">(symbols)</span></span>
<span class="line"><span style="color:#6A737D;"># [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;, &#39;f&#39;, &#39;g&#39;, &#39;h&#39;, &#39;i&#39;, &#39;j&#39;, &#39;k&#39;, &#39;l&#39;, &#39;m&#39;, &#39;n&#39;, &#39;o&#39;, &#39;p&#39;, &#39;q&#39;, &#39;r&#39;, &#39;s&#39;, &#39;t&#39;, &#39;u&#39;, &#39;v&#39;, &#39;w&#39;, &#39;x&#39;, &#39;y&#39;, &#39;z&#39;, &#39;_&#39;, &#39;[UNK]&#39;, &#39;ta&#39;, &#39;tal&#39;, &#39;tall&#39;, &#39;fa&#39;, &#39;fas&#39;, &#39;fast&#39;, &#39;er&#39;, &#39;er_&#39;, &#39;tall_&#39;, &#39;fast_&#39;]</span></span>
<span class="line"></span>
<span class="line"><span style="color:#79B8FF;">print</span><span style="color:#E1E4E8;">(</span><span style="color:#79B8FF;">list</span><span style="color:#E1E4E8;">(token_freqs.keys()))</span></span>
<span class="line"><span style="color:#6A737D;"># [&#39;fast_&#39;, &#39;fast er_&#39;, &#39;tall_&#39;, &#39;tall er_&#39;]</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 用从一个数据集学习的子词来切分另一个数据集的单词</span></span>
<span class="line"><span style="color:#6A737D;"># 尝试将单词从输入参数symbols分成可能最长的子词</span></span>
<span class="line"><span style="color:#F97583;">def</span><span style="color:#B392F0;"> segment_BPE</span><span style="color:#E1E4E8;">(tokens, symbols):</span></span>
<span class="line"><span style="color:#E1E4E8;">  outputs </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> []</span></span>
<span class="line"><span style="color:#F97583;">  for</span><span style="color:#E1E4E8;"> token </span><span style="color:#F97583;">in</span><span style="color:#E1E4E8;"> tokens:</span></span>
<span class="line"><span style="color:#E1E4E8;">    start, end </span><span style="color:#F97583;">=</span><span style="color:#79B8FF;"> 0</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">len</span><span style="color:#E1E4E8;">(token)</span></span>
<span class="line"><span style="color:#E1E4E8;">    cur_output </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> []</span></span>
<span class="line"><span style="color:#6A737D;">    # 具有符号中可能最长子字的词元段</span></span>
<span class="line"><span style="color:#F97583;">    while</span><span style="color:#E1E4E8;"> start </span><span style="color:#F97583;">&lt;</span><span style="color:#79B8FF;"> len</span><span style="color:#E1E4E8;">(token) </span><span style="color:#F97583;">and</span><span style="color:#E1E4E8;"> start </span><span style="color:#F97583;">&lt;</span><span style="color:#E1E4E8;"> end:</span></span>
<span class="line"><span style="color:#F97583;">      if</span><span style="color:#E1E4E8;"> token[start: end] </span><span style="color:#F97583;">in</span><span style="color:#E1E4E8;"> symbols:</span></span>
<span class="line"><span style="color:#E1E4E8;">        cur_output.append(token[start: end])</span></span>
<span class="line"><span style="color:#E1E4E8;">        start </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> end</span></span>
<span class="line"><span style="color:#E1E4E8;">        end </span><span style="color:#F97583;">=</span><span style="color:#79B8FF;"> len</span><span style="color:#E1E4E8;">(token)</span></span>
<span class="line"><span style="color:#F97583;">      else</span><span style="color:#E1E4E8;">:</span></span>
<span class="line"><span style="color:#E1E4E8;">        end </span><span style="color:#F97583;">-=</span><span style="color:#79B8FF;"> 1</span></span>
<span class="line"><span style="color:#F97583;">    if</span><span style="color:#E1E4E8;"> start </span><span style="color:#F97583;">&lt;</span><span style="color:#79B8FF;"> len</span><span style="color:#E1E4E8;">(token):</span></span>
<span class="line"><span style="color:#E1E4E8;">      cur_output.append(</span><span style="color:#9ECBFF;">&#39;[UNK]&#39;</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">    outputs.append(</span><span style="color:#9ECBFF;">&#39; &#39;</span><span style="color:#E1E4E8;">.join(cur_output))</span></span>
<span class="line"><span style="color:#F97583;">  return</span><span style="color:#E1E4E8;"> outputs</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">tokens </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> [</span><span style="color:#9ECBFF;">&#39;tallest_&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;fatter_&#39;</span><span style="color:#E1E4E8;">]</span></span>
<span class="line"><span style="color:#79B8FF;">print</span><span style="color:#E1E4E8;">(segment_BPE(tokens, symbols))</span></span>
<span class="line"><span style="color:#6A737D;"># [&#39;tall e s t _&#39;, &#39;fa t t er_&#39;]</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br><span class="line-number">43</span><br><span class="line-number">44</span><br><span class="line-number">45</span><br><span class="line-number">46</span><br><span class="line-number">47</span><br><span class="line-number">48</span><br><span class="line-number">49</span><br><span class="line-number">50</span><br><span class="line-number">51</span><br><span class="line-number">52</span><br><span class="line-number">53</span><br><span class="line-number">54</span><br><span class="line-number">55</span><br><span class="line-number">56</span><br><span class="line-number">57</span><br><span class="line-number">58</span><br><span class="line-number">59</span><br><span class="line-number">60</span><br><span class="line-number">61</span><br><span class="line-number">62</span><br><span class="line-number">63</span><br><span class="line-number">64</span><br><span class="line-number">65</span><br><span class="line-number">66</span><br><span class="line-number">67</span><br><span class="line-number">68</span><br><span class="line-number">69</span><br><span class="line-number">70</span><br><span class="line-number">71</span><br><span class="line-number">72</span><br><span class="line-number">73</span><br><span class="line-number">74</span><br><span class="line-number">75</span><br><span class="line-number">76</span><br><span class="line-number">77</span><br><span class="line-number">78</span><br><span class="line-number">79</span><br><span class="line-number">80</span><br></div></div><p>字节对编码执行训练数据集的统计分析，以发现词内的公共符号。作为一种<strong>贪心方法</strong>，字节对编码迭代地<strong>合并最频繁的连续符号对</strong>。</p><h3 id="transformers-的-bert" tabindex="-1">Transformers 的 BERT <a class="header-anchor" href="#transformers-的-bert" aria-label="Permalink to &quot;Transformers 的 BERT&quot;">​</a></h3><p><strong>词嵌入模型</strong>预训练后，输出可认为是一个<strong>矩阵</strong>，每一行都是一个表示预定义词表中词的向量。</p><p>但，这些词嵌入模型都是与<strong>上下文无关</strong>的。</p><p><strong>ELMo</strong>：从上下文无关到<strong>上下文敏感</strong>，ELMo<sup>Embeddings from Language Models</sup>为输入序列中的每个单词分配一个表示的函数。具体来说，ELMo 将来自预训练的<strong>双向长短期记忆</strong>网络的所有中间层表示<strong>组合为输出</strong>表示。然后，ELMo 的表示将作为附加特征添加到下游任务的现有监督模型中，例如通过将 ELMo 的表示和现有模型中词元的原始表示连结起来。</p><p>尽管，ELMo 显著改进了各种自然语言处理任务的解决方案，但每个解决方案仍然<strong>依赖于一个特定于任务架构</strong>。</p><p><strong>GPT</strong>：生成式预训练<sup>Generative Pre Training</sup>模型为上下文的敏感表示设计了通用<strong>任务无关</strong>模型。GPT 建立在 <code>Transformer</code> 解码器的基础上，预训练了一个用于表示文本序列的语言模型。当将 <code>GPT</code> 应用于下游任务时，语言模型的输出将被送到一个附加的线性输出层，以预测任务的标签。与 <code>ELMo</code> 冻结预训练模型的参数不同，<code>GPT</code> 在下游任务的监督学习过程中对预训练 <code>Transformer</code> 解码器中的所有参数进行<strong>微调</strong>。</p><p>然而，由于语言模型的<strong>自回归</strong>特性，GPT <strong>只能向前看</strong>（从左到右）。</p><p><strong>BERT</strong>：Google 2018 年提出基于 <code>Transformer</code> 架构预训练语言模型 BERT<sup>Bidirectional Encoder Representations from Transformers</sup>(双向编码模型)。BERT 推动了<strong>预训练模型</strong>的浪潮，后续模型如 <code>GPT-3、T5</code> 等均受其启发。当前趋势转向更大规模（如 <code>PaLM</code>）、多模态（如 <code>CLIP</code>）和高效训练（如 <code>LoRA</code>）。</p><p><strong>1. 核心思想</strong></p><ul><li>双向上下文建模：<br> 与传统单向语言模型（如 GPT）不同，BERT 通过 <strong>Masked Language Model (MLM)</strong> 同时利用左右两侧的上下文信息，更全面地理解词语含义。</li><li>预训练+微调：<br> 先在大规模语料上无监督预训练，再针对下游任务（如分类、问答）进行少量数据微调。</li></ul><p><strong>2. 关键技术创新</strong></p><ul><li>Transformer 编码器：<br> 完全基于 Transformer 的编码器堆叠（多层 Self-Attention + Feed-Forward），无需解码器。</li><li>两种预训练任务： <ul><li>MLM（掩码语言模型）：随机遮盖 15% 的单词，预测被遮盖的词。</li><li>NSP（下一句预测）：判断两个句子是否连续，增强句子间关系理解。</li></ul></li><li>输入表示：<br> 使用 <code>[CLS]</code>（分类标记）、<code>[SEP]</code>（分隔标记）和词/段/位置嵌入的三层编码。</li></ul><p><strong>3. 优缺点</strong></p><ul><li>优点： <ul><li>上下文敏感，解决多义词问题（如 &quot;bank&quot; 在金融或河岸的差异）。</li><li>通用性强，微调即可适配多种任务。</li></ul></li><li>缺点： <ul><li>计算资源消耗大（尤其是 Large 版本）。</li><li>对超长文本处理有限（最大长度通常为 512 token）。</li></ul></li></ul><p><strong>4. 代码示例(Hugging Face 库)</strong></p><div class="language-python line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> transformers </span><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> BertTokenizer, BertModel</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 加载预训练模型和分词器</span></span>
<span class="line"><span style="color:#E1E4E8;">tokenizer </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> BertTokenizer.from_pretrained(</span><span style="color:#9ECBFF;">&#39;bert-base-uncased&#39;</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">model </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> BertModel.from_pretrained(</span><span style="color:#9ECBFF;">&#39;bert-base-uncased&#39;</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 输入处理</span></span>
<span class="line"><span style="color:#E1E4E8;">inputs </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> tokenizer(</span><span style="color:#9ECBFF;">&quot;Hello, BERT!&quot;</span><span style="color:#E1E4E8;">, </span><span style="color:#FFAB70;">return_tensors</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">&quot;pt&quot;</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">outputs </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> model(</span><span style="color:#F97583;">**</span><span style="color:#E1E4E8;">inputs)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 获取词嵌入或句子表示</span></span>
<span class="line"><span style="color:#E1E4E8;">last_hidden_states </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> outputs.last_hidden_state  </span><span style="color:#6A737D;"># 词级别嵌入</span></span>
<span class="line"><span style="color:#E1E4E8;">pooler_output </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> outputs.pooler_output          </span><span style="color:#6A737D;"># [CLS] 的句子表示</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br></div></div><p><strong>5. 训练准备及预训练</strong></p><p>与原始 <code>Transformer</code> 编码器不同，BERT 使用<strong>可学习位置</strong>嵌入。 <img src="/assets/bert-input.DYIcz4yN.svg" alt="An Image"><strong>BERT 嵌入 = 词元嵌入 + 片段嵌入 + 位置嵌入</strong>。</p><div class="language-py line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">py</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> torch</span></span>
<span class="line"><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> torch </span><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> nn</span></span>
<span class="line"><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> d2l </span><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> torch </span><span style="color:#F97583;">as</span><span style="color:#E1E4E8;"> d2l</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 获取输入序列的词元及其片段索引</span></span>
<span class="line"><span style="color:#F97583;">def</span><span style="color:#B392F0;"> get_tokens_and_segments</span><span style="color:#E1E4E8;">(tokens_a, tokens_b</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">None</span><span style="color:#E1E4E8;">):</span></span>
<span class="line"><span style="color:#E1E4E8;">  tokens </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> [</span><span style="color:#9ECBFF;">&#39;&lt;cls&gt;&#39;</span><span style="color:#E1E4E8;">] </span><span style="color:#F97583;">+</span><span style="color:#E1E4E8;"> tokens_a </span><span style="color:#F97583;">+</span><span style="color:#E1E4E8;"> [</span><span style="color:#9ECBFF;">&#39;&lt;sep&gt;&#39;</span><span style="color:#E1E4E8;">]</span></span>
<span class="line"><span style="color:#6A737D;">  # 0和1分别标记片段A和B</span></span>
<span class="line"><span style="color:#E1E4E8;">  segments </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> [</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">] </span><span style="color:#F97583;">*</span><span style="color:#E1E4E8;"> (</span><span style="color:#79B8FF;">len</span><span style="color:#E1E4E8;">(tokens_a) </span><span style="color:#F97583;">+</span><span style="color:#79B8FF;"> 2</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#F97583;">  if</span><span style="color:#E1E4E8;"> tokens_b </span><span style="color:#F97583;">is</span><span style="color:#F97583;"> not</span><span style="color:#79B8FF;"> None</span><span style="color:#E1E4E8;">:</span></span>
<span class="line"><span style="color:#E1E4E8;">    tokens </span><span style="color:#F97583;">+=</span><span style="color:#E1E4E8;"> tokens_b </span><span style="color:#F97583;">+</span><span style="color:#E1E4E8;"> [</span><span style="color:#9ECBFF;">&#39;&lt;sep&gt;&#39;</span><span style="color:#E1E4E8;">]</span></span>
<span class="line"><span style="color:#E1E4E8;">    segments </span><span style="color:#F97583;">+=</span><span style="color:#E1E4E8;"> [</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">] </span><span style="color:#F97583;">*</span><span style="color:#E1E4E8;"> (</span><span style="color:#79B8FF;">len</span><span style="color:#E1E4E8;">(tokens_b) </span><span style="color:#F97583;">+</span><span style="color:#79B8FF;"> 1</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#F97583;">  return</span><span style="color:#E1E4E8;"> tokens, segments</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># BERT编码器</span></span>
<span class="line"><span style="color:#F97583;">class</span><span style="color:#B392F0;"> BERTEncoder</span><span style="color:#E1E4E8;">(</span><span style="color:#B392F0;">nn</span><span style="color:#E1E4E8;">.</span><span style="color:#B392F0;">Module</span><span style="color:#E1E4E8;">):</span></span>
<span class="line"><span style="color:#F97583;">  def</span><span style="color:#79B8FF;"> __init__</span><span style="color:#E1E4E8;">(self, vocab_size, num_hiddens, norm_shape, ffn_num_input,</span></span>
<span class="line"><span style="color:#E1E4E8;">                ffn_num_hiddens, num_heads, num_layers, dropout,</span></span>
<span class="line"><span style="color:#E1E4E8;">                max_len</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">1000</span><span style="color:#E1E4E8;">, key_size</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">768</span><span style="color:#E1E4E8;">, query_size</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">768</span><span style="color:#E1E4E8;">, value_size</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">768</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#F97583;">                **</span><span style="color:#E1E4E8;">kwargs):</span></span>
<span class="line"><span style="color:#79B8FF;">    super</span><span style="color:#E1E4E8;">(BERTEncoder, </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">).</span><span style="color:#79B8FF;">__init__</span><span style="color:#E1E4E8;">(</span><span style="color:#F97583;">**</span><span style="color:#E1E4E8;">kwargs)</span></span>
<span class="line"><span style="color:#79B8FF;">    self</span><span style="color:#E1E4E8;">.token_embedding </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> nn.Embedding(vocab_size, num_hiddens)</span></span>
<span class="line"><span style="color:#79B8FF;">    self</span><span style="color:#E1E4E8;">.segment_embedding </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> nn.Embedding(</span><span style="color:#79B8FF;">2</span><span style="color:#E1E4E8;">, num_hiddens)</span></span>
<span class="line"><span style="color:#79B8FF;">    self</span><span style="color:#E1E4E8;">.blks </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> nn.Sequential()</span></span>
<span class="line"><span style="color:#F97583;">    for</span><span style="color:#E1E4E8;"> i </span><span style="color:#F97583;">in</span><span style="color:#79B8FF;"> range</span><span style="color:#E1E4E8;">(num_layers):</span></span>
<span class="line"><span style="color:#79B8FF;">      self</span><span style="color:#E1E4E8;">.blks.add_module(</span><span style="color:#F97583;">f</span><span style="color:#9ECBFF;">&quot;</span><span style="color:#79B8FF;">{</span><span style="color:#E1E4E8;">i</span><span style="color:#79B8FF;">}</span><span style="color:#9ECBFF;">&quot;</span><span style="color:#E1E4E8;">, d2l.EncoderBlock(</span></span>
<span class="line"><span style="color:#E1E4E8;">        key_size, query_size, value_size, num_hiddens, norm_shape,</span></span>
<span class="line"><span style="color:#E1E4E8;">        ffn_num_input, ffn_num_hiddens, num_heads, dropout, </span><span style="color:#79B8FF;">True</span><span style="color:#E1E4E8;">))</span></span>
<span class="line"><span style="color:#6A737D;">    # 在BERT中，位置嵌入是可学习的，因此我们创建一个足够长的位置嵌入参数</span></span>
<span class="line"><span style="color:#79B8FF;">    self</span><span style="color:#E1E4E8;">.pos_embedding </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> nn.Parameter(torch.randn(</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">, max_len, num_hiddens))</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583;">  def</span><span style="color:#B392F0;"> forward</span><span style="color:#E1E4E8;">(self, tokens, segments, valid_lens):</span></span>
<span class="line"><span style="color:#6A737D;">    # 在以下代码段中，X的形状保持不变：（批量大小，最大序列长度，num_hiddens）</span></span>
<span class="line"><span style="color:#E1E4E8;">    X </span><span style="color:#F97583;">=</span><span style="color:#79B8FF;"> self</span><span style="color:#E1E4E8;">.token_embedding(tokens) </span><span style="color:#F97583;">+</span><span style="color:#79B8FF;"> self</span><span style="color:#E1E4E8;">.segment_embedding(segments)</span></span>
<span class="line"><span style="color:#E1E4E8;">    X </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> X </span><span style="color:#F97583;">+</span><span style="color:#79B8FF;"> self</span><span style="color:#E1E4E8;">.pos_embedding.data[:, :X.shape[</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">], :]</span></span>
<span class="line"><span style="color:#F97583;">    for</span><span style="color:#E1E4E8;"> blk </span><span style="color:#F97583;">in</span><span style="color:#79B8FF;"> self</span><span style="color:#E1E4E8;">.blks:</span></span>
<span class="line"><span style="color:#E1E4E8;">      X </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> blk(X, valid_lens)</span></span>
<span class="line"><span style="color:#F97583;">    return</span><span style="color:#E1E4E8;"> X</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 实例化</span></span>
<span class="line"><span style="color:#E1E4E8;">vocab_size, num_hiddens, ffn_num_hiddens, num_heads </span><span style="color:#F97583;">=</span><span style="color:#79B8FF;"> 10000</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">768</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">1024</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">4</span></span>
<span class="line"><span style="color:#E1E4E8;">norm_shape, ffn_num_input, num_layers, dropout </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> [</span><span style="color:#79B8FF;">768</span><span style="color:#E1E4E8;">], </span><span style="color:#79B8FF;">768</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">2</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">0.2</span></span>
<span class="line"><span style="color:#E1E4E8;">encoder </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> BERTEncoder(vocab_size, num_hiddens, norm_shape, ffn_num_input,</span></span>
<span class="line"><span style="color:#E1E4E8;">                      ffn_num_hiddens, num_heads, num_layers, dropout)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 走起</span></span>
<span class="line"><span style="color:#E1E4E8;">tokens </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torch.randint(</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">, vocab_size, (</span><span style="color:#79B8FF;">2</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">8</span><span style="color:#E1E4E8;">))</span></span>
<span class="line"><span style="color:#E1E4E8;">segments </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torch.tensor([[</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">], [</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">]])</span></span>
<span class="line"><span style="color:#E1E4E8;">encoded_X </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> encoder(tokens, segments, </span><span style="color:#79B8FF;">None</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">encoded_X.shape</span></span>
<span class="line"><span style="color:#6A737D;"># torch.Size([2, 8, 768])</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br><span class="line-number">43</span><br><span class="line-number">44</span><br><span class="line-number">45</span><br><span class="line-number">46</span><br><span class="line-number">47</span><br><span class="line-number">48</span><br><span class="line-number">49</span><br><span class="line-number">50</span><br><span class="line-number">51</span><br></div></div><p><strong>预训练</strong>包括 2 个任务：<strong>掩蔽语言模型</strong><sup>Masked Language Modeling</sup>和<strong>下一句预测</strong><sup>Next Sentence Prediction</sup>。</p><p>前者能够编码双向上下文来表示单词，而后者则显式地建模文本对之间的逻辑关系。</p><p><strong>掩蔽语言模型</strong>：语言模型使用左侧上下文预测词元。为了双向编码上下文以表示每个词元，BERT <strong>随机掩蔽词元</strong>并使用来自双向上下文的词元以<strong>自监督</strong>的方式预测掩蔽词元。</p><div class="language-py line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">py</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#6A737D;"># BERT的掩蔽语言模型任务</span></span>
<span class="line"><span style="color:#F97583;">class</span><span style="color:#B392F0;"> MaskLM</span><span style="color:#E1E4E8;">(</span><span style="color:#B392F0;">nn</span><span style="color:#E1E4E8;">.</span><span style="color:#B392F0;">Module</span><span style="color:#E1E4E8;">):</span></span>
<span class="line"><span style="color:#F97583;">  def</span><span style="color:#79B8FF;"> __init__</span><span style="color:#E1E4E8;">(self, vocab_size, num_hiddens, num_inputs</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">768</span><span style="color:#E1E4E8;">, </span><span style="color:#F97583;">**</span><span style="color:#E1E4E8;">kwargs):</span></span>
<span class="line"><span style="color:#79B8FF;">    super</span><span style="color:#E1E4E8;">(MaskLM, </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">).</span><span style="color:#79B8FF;">__init__</span><span style="color:#E1E4E8;">(</span><span style="color:#F97583;">**</span><span style="color:#E1E4E8;">kwargs)</span></span>
<span class="line"><span style="color:#79B8FF;">    self</span><span style="color:#E1E4E8;">.mlp </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> nn.Sequential(nn.Linear(num_inputs, num_hiddens),</span></span>
<span class="line"><span style="color:#E1E4E8;">                              nn.ReLU(), nn.LayerNorm(num_hiddens),</span></span>
<span class="line"><span style="color:#E1E4E8;">                              nn.Linear(num_hiddens, vocab_size))</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583;">  def</span><span style="color:#B392F0;"> forward</span><span style="color:#E1E4E8;">(self, X, pred_positions):</span></span>
<span class="line"><span style="color:#E1E4E8;">    num_pred_positions </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> pred_positions.shape[</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">]</span></span>
<span class="line"><span style="color:#E1E4E8;">    pred_positions </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> pred_positions.reshape(</span><span style="color:#F97583;">-</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">    batch_size </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> X.shape[</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">]</span></span>
<span class="line"><span style="color:#E1E4E8;">    batch_idx </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torch.arange(</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">, batch_size)</span></span>
<span class="line"><span style="color:#6A737D;">    # 假设batch_size=2，num_pred_positions=3</span></span>
<span class="line"><span style="color:#6A737D;">    # 那么batch_idx是np.array（[0,0,0,1,1,1]）</span></span>
<span class="line"><span style="color:#E1E4E8;">    batch_idx </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torch.repeat_interleave(batch_idx, num_pred_positions)</span></span>
<span class="line"><span style="color:#E1E4E8;">    masked_X </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> X[batch_idx, pred_positions]</span></span>
<span class="line"><span style="color:#E1E4E8;">    masked_X </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> masked_X.reshape((batch_size, num_pred_positions, </span><span style="color:#F97583;">-</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">))</span></span>
<span class="line"><span style="color:#E1E4E8;">    mlm_Y_hat </span><span style="color:#F97583;">=</span><span style="color:#79B8FF;"> self</span><span style="color:#E1E4E8;">.mlp(masked_X)</span></span>
<span class="line"><span style="color:#F97583;">    return</span><span style="color:#E1E4E8;"> mlm_Y_hat</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 初始化</span></span>
<span class="line"><span style="color:#E1E4E8;">mlm </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> MaskLM(vocab_size, num_hiddens)</span></span>
<span class="line"><span style="color:#E1E4E8;">mlm_positions </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torch.tensor([[</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">5</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">2</span><span style="color:#E1E4E8;">], [</span><span style="color:#79B8FF;">6</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">5</span><span style="color:#E1E4E8;">]])</span></span>
<span class="line"><span style="color:#E1E4E8;">mlm_Y_hat </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> mlm(encoded_X, mlm_positions)</span></span>
<span class="line"><span style="color:#E1E4E8;">mlm_Y_hat.shape</span></span>
<span class="line"><span style="color:#6A737D;"># torch.Size([2, 3, 10000])</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># Go!</span></span>
<span class="line"><span style="color:#E1E4E8;">mlm_Y </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torch.tensor([[</span><span style="color:#79B8FF;">7</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">8</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">9</span><span style="color:#E1E4E8;">], [</span><span style="color:#79B8FF;">10</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">20</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">30</span><span style="color:#E1E4E8;">]])</span></span>
<span class="line"><span style="color:#E1E4E8;">loss </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> nn.CrossEntropyLoss(</span><span style="color:#FFAB70;">reduction</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">&#39;none&#39;</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">mlm_l </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> loss(mlm_Y_hat.reshape((</span><span style="color:#F97583;">-</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">, vocab_size)), mlm_Y.reshape(</span><span style="color:#F97583;">-</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">))</span></span>
<span class="line"><span style="color:#E1E4E8;">mlm_l.shape</span></span>
<span class="line"><span style="color:#6A737D;"># torch.Size([6])</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br></div></div><p><strong>下一句预测</strong>：在为预训练生成句子对时，有一半时间为 <code>True</code>，一半时间为 <code>False</code>。</p><div class="language-py line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">py</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#6A737D;"># BERT的下一句预测任务</span></span>
<span class="line"><span style="color:#F97583;">class</span><span style="color:#B392F0;"> NextSentencePred</span><span style="color:#E1E4E8;">(</span><span style="color:#B392F0;">nn</span><span style="color:#E1E4E8;">.</span><span style="color:#B392F0;">Module</span><span style="color:#E1E4E8;">):</span></span>
<span class="line"><span style="color:#F97583;">  def</span><span style="color:#79B8FF;"> __init__</span><span style="color:#E1E4E8;">(self, num_inputs, </span><span style="color:#F97583;">**</span><span style="color:#E1E4E8;">kwargs):</span></span>
<span class="line"><span style="color:#79B8FF;">    super</span><span style="color:#E1E4E8;">(NextSentencePred, </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">).</span><span style="color:#79B8FF;">__init__</span><span style="color:#E1E4E8;">(</span><span style="color:#F97583;">**</span><span style="color:#E1E4E8;">kwargs)</span></span>
<span class="line"><span style="color:#79B8FF;">    self</span><span style="color:#E1E4E8;">.output </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> nn.Linear(num_inputs, </span><span style="color:#79B8FF;">2</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583;">  def</span><span style="color:#B392F0;"> forward</span><span style="color:#E1E4E8;">(self, X):</span></span>
<span class="line"><span style="color:#6A737D;">    # X的形状：(batchsize,num_hiddens)</span></span>
<span class="line"><span style="color:#F97583;">    return</span><span style="color:#79B8FF;"> self</span><span style="color:#E1E4E8;">.output(X)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">encoded_X </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torch.flatten(encoded_X, </span><span style="color:#FFAB70;">start_dim</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#6A737D;"># NSP的输入形状:(batchsize，num_hiddens)</span></span>
<span class="line"><span style="color:#E1E4E8;">nsp </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> NextSentencePred(encoded_X.shape[</span><span style="color:#F97583;">-</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">])</span></span>
<span class="line"><span style="color:#E1E4E8;">nsp_Y_hat </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> nsp(encoded_X)</span></span>
<span class="line"><span style="color:#E1E4E8;">nsp_Y_hat.shape</span></span>
<span class="line"><span style="color:#6A737D;"># torch.Size([2, 2])</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">nsp_y </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torch.tensor([</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">])</span></span>
<span class="line"><span style="color:#E1E4E8;">nsp_l </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> loss(nsp_Y_hat, nsp_y)</span></span>
<span class="line"><span style="color:#E1E4E8;">nsp_l.shape</span></span>
<span class="line"><span style="color:#6A737D;"># torch.Size([2])</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br></div></div><p><strong>整合代码</strong>：</p><p>BERT 预训练最终损失 = 遮蔽语言模型损失 + 下一句预测损失</p><div class="language-py line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">py</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#6A737D;"># BERT模型</span></span>
<span class="line"><span style="color:#F97583;">class</span><span style="color:#B392F0;"> BERTModel</span><span style="color:#E1E4E8;">(</span><span style="color:#B392F0;">nn</span><span style="color:#E1E4E8;">.</span><span style="color:#B392F0;">Module</span><span style="color:#E1E4E8;">):</span></span>
<span class="line"><span style="color:#F97583;">  def</span><span style="color:#79B8FF;"> __init__</span><span style="color:#E1E4E8;">(self, vocab_size, num_hiddens, norm_shape, ffn_num_input,</span></span>
<span class="line"><span style="color:#E1E4E8;">                ffn_num_hiddens, num_heads, num_layers, dropout,</span></span>
<span class="line"><span style="color:#E1E4E8;">                max_len</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">1000</span><span style="color:#E1E4E8;">, key_size</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">768</span><span style="color:#E1E4E8;">, query_size</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">768</span><span style="color:#E1E4E8;">, value_size</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">768</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#E1E4E8;">                hid_in_features</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">768</span><span style="color:#E1E4E8;">, mlm_in_features</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">768</span><span style="color:#E1E4E8;">, nsp_in_features</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">768</span><span style="color:#E1E4E8;">):</span></span>
<span class="line"><span style="color:#79B8FF;">    super</span><span style="color:#E1E4E8;">(BERTModel, </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">).</span><span style="color:#79B8FF;">__init__</span><span style="color:#E1E4E8;">()</span></span>
<span class="line"><span style="color:#79B8FF;">    self</span><span style="color:#E1E4E8;">.encoder </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> BERTEncoder(vocab_size, num_hiddens, norm_shape,</span></span>
<span class="line"><span style="color:#E1E4E8;">                ffn_num_input, ffn_num_hiddens, num_heads, num_layers,</span></span>
<span class="line"><span style="color:#E1E4E8;">                dropout, </span><span style="color:#FFAB70;">max_len</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">max_len, </span><span style="color:#FFAB70;">key_size</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">key_size,</span></span>
<span class="line"><span style="color:#FFAB70;">                query_size</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">query_size, </span><span style="color:#FFAB70;">value_size</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">value_size)</span></span>
<span class="line"><span style="color:#79B8FF;">    self</span><span style="color:#E1E4E8;">.hidden </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> nn.Sequential(nn.Linear(hid_in_features, num_hiddens), nn.Tanh())</span></span>
<span class="line"><span style="color:#79B8FF;">    self</span><span style="color:#E1E4E8;">.mlm </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> MaskLM(vocab_size, num_hiddens, mlm_in_features)</span></span>
<span class="line"><span style="color:#79B8FF;">    self</span><span style="color:#E1E4E8;">.nsp </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> NextSentencePred(nsp_in_features)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583;">  def</span><span style="color:#B392F0;"> forward</span><span style="color:#E1E4E8;">(self, tokens, segments, valid_lens</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">None</span><span style="color:#E1E4E8;">, pred_positions</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">None</span><span style="color:#E1E4E8;">):</span></span>
<span class="line"><span style="color:#E1E4E8;">    encoded_X </span><span style="color:#F97583;">=</span><span style="color:#79B8FF;"> self</span><span style="color:#E1E4E8;">.encoder(tokens, segments, valid_lens)</span></span>
<span class="line"><span style="color:#F97583;">    if</span><span style="color:#E1E4E8;"> pred_positions </span><span style="color:#F97583;">is</span><span style="color:#F97583;"> not</span><span style="color:#79B8FF;"> None</span><span style="color:#E1E4E8;">:</span></span>
<span class="line"><span style="color:#E1E4E8;">      mlm_Y_hat </span><span style="color:#F97583;">=</span><span style="color:#79B8FF;"> self</span><span style="color:#E1E4E8;">.mlm(encoded_X, pred_positions)</span></span>
<span class="line"><span style="color:#F97583;">    else</span><span style="color:#E1E4E8;">:</span></span>
<span class="line"><span style="color:#E1E4E8;">      mlm_Y_hat </span><span style="color:#F97583;">=</span><span style="color:#79B8FF;"> None</span></span>
<span class="line"><span style="color:#6A737D;">    # 用于下一句预测的多层感知机分类器的隐藏层，0是“&lt;cls&gt;”标记的索引</span></span>
<span class="line"><span style="color:#E1E4E8;">    nsp_Y_hat </span><span style="color:#F97583;">=</span><span style="color:#79B8FF;"> self</span><span style="color:#E1E4E8;">.nsp(</span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.hidden(encoded_X[:, </span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">, :]))</span></span>
<span class="line"><span style="color:#F97583;">    return</span><span style="color:#E1E4E8;"> encoded_X, mlm_Y_hat, nsp_Y_hat</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br></div></div><p><strong>附录：双向 vs 单向</strong></p><table tabindex="0"><thead><tr><th>特性</th><th>BERT (Transformer 编码器)</th><th>GPT (Transformer 解码器)</th></tr></thead><tbody><tr><td><strong>上下文方向</strong></td><td>双向（同时看左右上下文）</td><td>单向（仅看左侧历史上下文）</td></tr><tr><td><strong>架构模块</strong></td><td>仅使用 Transformer <strong>编码器</strong></td><td>仅使用 Transformer <strong>解码器</strong>（带掩码注意力）</td></tr><tr><td><strong>预训练目标</strong></td><td>Masked Language Model (MLM) + NSP</td><td>自回归语言建模（预测下一个词）</td></tr><tr><td><strong>典型应用</strong></td><td>文本分类/实体识别/问答</td><td>文本生成/续写/翻译</td></tr></tbody></table></div></div><!----></main><!--[--><!--]--><footer class="VPContentDocFooter" data-v-5d3992ac data-v-b1808168><a class="prev-link" href="/aiart/deep-learning/transformer.html" data-v-b1808168><span class="desc" data-v-b1808168><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="vt-link-icon" data-v-b1808168><path d="M15,19c-0.3,0-0.5-0.1-0.7-0.3l-6-6c-0.4-0.4-0.4-1,0-1.4l6-6c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4L10.4,12l5.3,5.3c0.4,0.4,0.4,1,0,1.4C15.5,18.9,15.3,19,15,19z"></path></svg> 上一篇</span><span class="title" data-v-b1808168>Transformer架构</span></a><a class="next-link" href="/aiart/deep-learning/gpt.html" data-v-b1808168><span class="desc" data-v-b1808168>下一篇 <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="vt-link-icon" data-v-b1808168><path d="M9,19c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l5.3-5.3L8.3,6.7c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l6,6c0.4,0.4,0.4,1,0,1.4l-6,6C9.5,18.9,9.3,19,9,19z"></path></svg></span><span class="title" data-v-b1808168>GPT</span></a></footer></div></div></div></div><div class="visually-hidden" aria-live="polite" data-v-ed1e6441>自然语言处理、NLP has loaded</div></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"aiart_chat-gpt.md\":\"N0KWoXR0\",\"aiart_comfyui_basic.md\":\"yiS-gea9\",\"aiart_deep-learning_attention-mechanisms.md\":\"Ccu-XPdt\",\"aiart_deep-learning_basic-concept.md\":\"PnWSarrd\",\"aiart_deep-learning_cnn.md\":\"C7NLqIZ0\",\"aiart_deep-learning_computer-vision.md\":\"ByuKyrGI\",\"aiart_deep-learning_gpt.md\":\"Bjrvinyb\",\"aiart_deep-learning_linear-regression.md\":\"BF3NeW_h\",\"aiart_deep-learning_mathematics.md\":\"gApASmDd\",\"aiart_deep-learning_nlp.md\":\"CNOa76hL\",\"aiart_deep-learning_overview.md\":\"DWusOyFF\",\"aiart_deep-learning_pytorch.md\":\"BZZFhUgP\",\"aiart_deep-learning_rnn-modern.md\":\"TP1JCyVE\",\"aiart_deep-learning_rnn.md\":\"D3qUO5FY\",\"aiart_deep-learning_transformer.md\":\"XY061JD_\",\"aiart_huggingface_accelerate.md\":\"DVotXi9i\",\"aiart_huggingface_config.md\":\"Nqa2M6Jq\",\"aiart_huggingface_diffusers.md\":\"Hf3cFwwy\",\"aiart_machine-learning_conda.md\":\"0sr_b5Nw\",\"aiart_machine-learning_kaggle.md\":\"TghmnFDx\",\"aiart_machine-learning_overview.md\":\"GvJZfTwm\",\"aiart_python_basic-info.md\":\"COlde5yp\",\"aiart_python_oop.md\":\"C3pWTViD\",\"aiart_python_pandas.md\":\"BSPyWinL\",\"aiart_python_pil.md\":\"zAS8WQd4\",\"aiart_python_pip.md\":\"C0o894rF\",\"aiart_sunzi-war-art.md\":\"-FzWbhIV\",\"aiart_three-body.md\":\"BiRvl00u\",\"basic_javascript_array.md\":\"Bi9w5opY\",\"basic_javascript_bitwise-operators.md\":\"DAQVVFL7\",\"basic_javascript_function.md\":\"qO8HTv3T\",\"basic_javascript_index.md\":\"DL3KG0vt\",\"basic_javascript_object.md\":\"ByvK_cvR\",\"basic_javascript_operators.md\":\"BLq7S7wq\",\"basic_javascript_promise.md\":\"DtPJA1Wz\",\"basic_javascript_proxy.md\":\"B1k8xFmq\",\"basic_javascript_reflect.md\":\"B7o61Vou\",\"basic_javascript_symbol.md\":\"qF0Bkhg9\",\"basic_tools_cold-code.md\":\"3qk2TpHi\",\"basic_tools_markdown.md\":\"CsoEtW9O\",\"basic_tools_worker.md\":\"DEKTO6RP\",\"basic_typescript_config-file.md\":\"BJLx862P\",\"basic_typescript_declare.md\":\"Sc_xgBlM\",\"basic_typescript_index.md\":\"QEkJkoOd\",\"basic_typescript_tsx.md\":\"i20wGBG8\",\"index.md\":\"DPEv5U2T\",\"investment_jabber.md\":\"n43KYWM1\",\"investment_stock-prediction.md\":\"B1M3jJA7\",\"mac_daily.md\":\"smp_6Cqf\",\"mac_dev-tools.md\":\"SBU-q55d\",\"mac_linux_os-command.md\":\"CtwI0_oY\",\"mac_linux_vim.md\":\"CDladIQ3\",\"mac_setting.md\":\"BldySgnd\",\"mac_vscode.md\":\"smoxE_FG\",\"mathstat_math_calculus.md\":\"sp--0BXc\",\"mathstat_math_math-symbol.md\":\"DDM9NARj\",\"mathstat_math_number-theory.md\":\"DUx7FyCd\",\"node_core_fs.md\":\"KCwkN7Xl\",\"node_core_http.md\":\"CDBlWuyg\",\"node_core_index.md\":\"CYwyDjAT\",\"node_core_path.md\":\"iIsh6XZl\",\"node_core_process.md\":\"CKkYoe6j\",\"node_core_url.md\":\"CESnLkdT\",\"node_external_cli.md\":\"DSu_6Wvi\",\"node_external_debug.md\":\"DfiEaDM9\",\"node_external_git.md\":\"Bv45nnOd\",\"node_external_github.md\":\"BN_NL6tQ\",\"node_external_npm-run-all.md\":\"BZ0XfU00\",\"node_external_npm.md\":\"DCprIcmg\",\"node_external_npx.md\":\"DgCYuRBD\",\"node_external_nvm.md\":\"B01pxVbC\",\"node_external_package-json.md\":\"DzFbMY8f\",\"node_external_pnpm-monorepo.md\":\"DbgFKhOC\",\"node_external_server.md\":\"BjHQePIs\",\"node_external_websockets.md\":\"zzzM85St\",\"vite_command-cli.md\":\"CJXlgFA_\",\"vite_create-vite.md\":\"CGvhWbZn\",\"vite_extend_rollup-source-build.md\":\"CReGQeKn\",\"vite_extend_rollup-source-write.md\":\"D8HiFzfq\",\"vite_index.md\":\"DhvE5Q4m\",\"vite_plugin.md\":\"B2g_mllf\",\"vite_press_press-command.md\":\"wMtWYO_9\",\"vite_press_theme-default.md\":\"DAXVioWK\",\"vite_press_theme-vue.md\":\"DaLKC6K5\",\"vite_press_vitepress.md\":\"hOsMTnTM\",\"vite_rollup-source.md\":\"Br9J-oNK\",\"vite_rollup.md\":\"D8rCZrLs\",\"vue_vue2_defineproperty.md\":\"D_U4svEa\",\"vue_vue3_pinia-code.md\":\"COq3ByGP\",\"vue_vue3_reactive.md\":\"B3a8st73\",\"vue_vue3_router-code.md\":\"BXGM6WGe\",\"vue_vue3_source-code.md\":\"BnfSCC5g\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"AI边城\",\"description\":\"A VitePress site\",\"base\":\"/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"nav\":[{\"text\":\"首页\",\"link\":\"/\"},{\"text\":\"AI·文艺\",\"activeMatch\":\"^/aiart\",\"link\":\"/aiart/machine-learning/overview\"},{\"text\":\"Vue·周边\",\"activeMatch\":\"^/vue/\",\"link\":\"/vue/vue3/source-code\"},{\"text\":\"Vite·Press\",\"activeMatch\":\"^/vite/\",\"link\":\"/vite/\"},{\"text\":\"Node·周边\",\"activeMatch\":\"^/node/\",\"link\":\"/node/core/\"},{\"text\":\"Js·Ts\",\"activeMatch\":\"^/basic\",\"link\":\"/basic/javascript/\"},{\"text\":\"Mac·Linux\",\"activeMatch\":\"^/mac\",\"link\":\"/mac/setting\"},{\"text\":\"数学·统计\",\"activeMatch\":\"^/mathstat\",\"link\":\"/mathstat/math/number-theory\"},{\"text\":\"投资·金融\",\"activeMatch\":\"^/investment\",\"link\":\"/investment/stock-prediction\"}],\"sidebar\":{\"/vue/\":[{\"text\":\"Vue3.x系列集锦\",\"items\":[{\"text\":\"Core流程源码摘要\",\"link\":\"/vue/vue3/source-code\"},{\"text\":\"Reactive响应式系统\",\"link\":\"/vue/vue3/reactive\"},{\"text\":\"Router源码摘要\",\"link\":\"/vue/vue3/router-code\"},{\"text\":\"Pinia源码摘要\",\"link\":\"/vue/vue3/pinia-code\"}]},{\"text\":\"Vue2原理\",\"items\":[{\"text\":\"DefineProperty\",\"link\":\"/vue/vue2/defineProperty\"}]}],\"/vite/\":[{\"text\":\"Vite原理与周边\",\"items\":[{\"text\":\"Vite基础\",\"link\":\"/vite/\"},{\"text\":\"vite命令源码摘要\",\"link\":\"/vite/command-cli\"},{\"text\":\"Vite自动生成项目原理\",\"link\":\"/vite/create-vite\"},{\"text\":\"Vite插件怎么写\",\"link\":\"/vite/plugin\"}]},{\"text\":\"Rollup.js\",\"items\":[{\"text\":\"安装命令及配置文件\",\"link\":\"/vite/rollup\"},{\"text\":\"rollup.js源码摘要\",\"link\":\"/vite/rollup-source\"}]},{\"text\":\"Vitepress\",\"items\":[{\"text\":\"简介与功能\",\"link\":\"/vite/press/vitepress\"},{\"text\":\"vitepress命令源码摘要\",\"link\":\"/vite/press/press-command\"},{\"text\":\"默认主题(default)\",\"link\":\"/vite/press/theme-default\"},{\"text\":\"Vue官方主题(Vue3)\",\"link\":\"/vite/press/theme-vue\"}]}],\"/node/\":[{\"text\":\"Node基础\",\"items\":[{\"text\":\"简介\",\"link\":\"/node/core/\"},{\"text\":\"fs文件系统及扩展\",\"link\":\"/node/core/fs\"},{\"text\":\"url模块\",\"link\":\"/node/core/url\"},{\"text\":\"path路径\",\"link\":\"/node/core/path\"},{\"text\":\"http模块\",\"link\":\"/node/core/http\"},{\"text\":\"process进程\",\"link\":\"/node/core/process\"}]},{\"text\":\"周边工具\",\"items\":[{\"text\":\"本地调试Node项目文件\",\"link\":\"/node/external/debug\"},{\"text\":\"写个Node命令行工具\",\"link\":\"/node/external/cli\"},{\"text\":\"npm\",\"link\":\"/node/external/npm\"},{\"text\":\"npx\",\"link\":\"/node/external/npx\"},{\"text\":\"package.json\",\"link\":\"/node/external/package-json\"},{\"text\":\"pnpm monorepo\",\"link\":\"/node/external/pnpm-monorepo\"},{\"text\":\"nvm 和 nrm\",\"link\":\"/node/external/nvm\"},{\"text\":\"git\",\"link\":\"/node/external/git\"},{\"text\":\"Github\",\"link\":\"/node/external/github\"},{\"text\":\"npm-run-all\",\"link\":\"/node/external/npm-run-all\"},{\"text\":\"服务器类型\",\"link\":\"/node/external/server\"},{\"text\":\"WebSocket\",\"link\":\"/node/external/websockets\"}]}],\"/basic/\":[{\"text\":\"JavaScript基础\",\"items\":[{\"text\":\"简介\",\"link\":\"/basic/javascript/\"},{\"text\":\"Array数组\",\"link\":\"/basic/javascript/array\"},{\"text\":\"Object对象\",\"link\":\"/basic/javascript/object\"},{\"text\":\"Function函数\",\"link\":\"/basic/javascript/function\"},{\"text\":\"Symbol标识符\",\"link\":\"/basic/javascript/symbol\"},{\"text\":\"Promise\",\"link\":\"/basic/javascript/promise\"},{\"text\":\"Proxy\",\"link\":\"/basic/javascript/proxy\"},{\"text\":\"Reflect\",\"link\":\"/basic/javascript/reflect\"},{\"text\":\"表达式与运算符\",\"link\":\"/basic/javascript/operators\"},{\"text\":\"位运算\",\"link\":\"/basic/javascript/bitwise-operators\"}]},{\"text\":\"TypeScript基础\",\"items\":[{\"text\":\"基本认知与区别\",\"link\":\"/basic/typescript/\"},{\"text\":\"tsconfig.json字段详解\",\"link\":\"/basic/typescript/config-file\"},{\"text\":\"declare及声明文件\",\"link\":\"/basic/typescript/declare\"},{\"text\":\"tsx执行ts及VSCode调试ts\",\"link\":\"/basic/typescript/tsx\"}]},{\"text\":\"周边及工具\",\"items\":[{\"text\":\"Markdown及生成目录\",\"link\":\"/basic/tools/markdown\"},{\"text\":\"工具函数-冷知识-酷代码\",\"link\":\"/basic/tools/cold-code\"},{\"text\":\"Web Worker\",\"link\":\"/basic/tools/worker\"}]}],\"/mac/\":[{\"text\":\"Mac那点事\",\"items\":[{\"text\":\"基本设置\",\"link\":\"/mac/setting\"},{\"text\":\"日常操作\",\"link\":\"/mac/daily\"},{\"text\":\"工具:brew/iTerm2\",\"link\":\"/mac/dev-tools\"},{\"text\":\"VSCode编辑器\",\"link\":\"/mac/vscode\"}]},{\"text\":\"Linux\",\"items\":[{\"text\":\"基础命令\",\"link\":\"/mac/linux/os-command\"},{\"text\":\"vim编辑器\",\"link\":\"/mac/linux/vim\"}]}],\"/aiart/\":[{\"text\":\"机器学习\",\"items\":[{\"text\":\"概览\",\"link\":\"/aiart/machine-learning/overview\"},{\"text\":\"conda\",\"link\":\"/aiart/machine-learning/conda\"},{\"text\":\"kaggle\",\"link\":\"/aiart/machine-learning/kaggle\"}]},{\"text\":\"深度学习\",\"items\":[{\"text\":\"概览\",\"link\":\"/aiart/deep-learning/overview\"},{\"text\":\"数学基础\",\"link\":\"/aiart/deep-learning/mathematics\"},{\"text\":\"Pytorch\",\"link\":\"/aiart/deep-learning/pytorch\"},{\"text\":\"基本概念\",\"link\":\"/aiart/deep-learning/basic-concept\"},{\"text\":\"线性回归\",\"link\":\"/aiart/deep-learning/linear-regression\"},{\"text\":\"卷积神经网络(CNN)\",\"link\":\"/aiart/deep-learning/cnn\"},{\"text\":\"循环神经网络(RNN)\",\"link\":\"/aiart/deep-learning/rnn\"},{\"text\":\"现代RNN\",\"link\":\"/aiart/deep-learning/rnn-modern\"},{\"text\":\"注意力机制(attention)\",\"link\":\"/aiart/deep-learning/attention-mechanisms\"},{\"text\":\"Transformer架构\",\"link\":\"/aiart/deep-learning/transformer\"},{\"text\":\"自然语言处理(NLP)\",\"link\":\"/aiart/deep-learning/nlp\"},{\"text\":\"GPT\",\"link\":\"/aiart/deep-learning/gpt\"},{\"text\":\"计算机视觉(CV)\",\"link\":\"/aiart/deep-learning/computer-vision\"}]},{\"text\":\"Huggingface\",\"items\":[{\"text\":\"基础配置\",\"link\":\"/aiart/huggingface/config\"},{\"text\":\"Diffusers\",\"link\":\"/aiart/huggingface/diffusers\"},{\"text\":\"Accelerate\",\"link\":\"/aiart/huggingface/accelerate\"}]},{\"text\":\"ComfyUI\",\"items\":[{\"text\":\"基本概念\",\"link\":\"/aiart/comfyui/basic\"}]},{\"text\":\"Python\",\"items\":[{\"text\":\"安装与设置\",\"link\":\"/aiart/python/basic-info\"},{\"text\":\"pip\",\"link\":\"/aiart/python/pip\"},{\"text\":\"面向对象编程(OOP)\",\"link\":\"/aiart/python/oop\"},{\"text\":\"Pandas\",\"link\":\"/aiart/python/pandas\"},{\"text\":\"PIL\",\"link\":\"/aiart/python/pil\"}]},{\"text\":\"AI语录\",\"items\":[{\"text\":\"概率、记忆与意识\",\"link\":\"/aiart/three-body\"},{\"text\":\"ChatGPT问答集锦\",\"link\":\"/aiart/chat-gpt\"}]},{\"text\":\"文艺\",\"items\":[{\"text\":\"孙子兵法\",\"link\":\"/aiart/sunzi-war-art\"}]}],\"/mathstat\":[{\"text\":\"理论\",\"items\":[{\"text\":\"数的概念\",\"link\":\"/mathstat/math/number-theory\"},{\"text\":\"数学符号\",\"link\":\"/mathstat/math/math-symbol\"}]},{\"text\":\"微积分\",\"items\":[{\"text\":\"基本定理\",\"link\":\"/mathstat/math/calculus\"}]},{\"text\":\"统计学\",\"items\":[]}],\"/investment\":[{\"text\":\"金融与人性\",\"items\":[{\"text\":\"不预测只应对\",\"link\":\"/investment/jabber\"}]},{\"text\":\"投资与股票\",\"items\":[{\"text\":\"预测 or 投机\",\"link\":\"/investment/stock-prediction\"}]}]},\"i18n\":{\"toc\":\"本页目录\",\"previous\":\"上一篇\",\"next\":\"下一篇\",\"pageNotFound\":\"页面未找到\"},\"socialLinks\":[{\"icon\":\"github\",\"link\":\"https://github.com/wswplay/wswplay.github.io\"}],\"footer\":{\"copyright\":\"Copyright © 2020-2025 边城\"}},\"locales\":{},\"scrollOffset\":[\"header\",\".VPLocalNav\"],\"cleanUrls\":false}");</script>
    
  </body>
</html>