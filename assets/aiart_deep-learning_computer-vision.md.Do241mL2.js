import{_ as o,c as p,o as e,b as l,a as s,m as a}from"./chunks/framework._eNwL97Z.js";const t="/assets/iou.BsSB6UFx.svg",r="/assets/ssd.Ddi4w6pO.svg",c="/assets/fcn.CT6AOrYf.svg",i="/assets/trans_conv.BNSxWVrg.svg",E="/assets/trans_conv_stride2.BOqPIKpo.svg",G=JSON.parse('{"title":"计算机视觉、Computer Vision","description":"","frontmatter":{"title":"计算机视觉、Computer Vision","outline":"deep"},"headers":[{"level":2,"title":"微调(fine-tuning)","slug":"微调-fine-tuning","link":"#微调-fine-tuning","children":[]},{"level":2,"title":"目标检测与边界框 🔥🔥🔥","slug":"目标检测与边界框-🔥🔥🔥","link":"#目标检测与边界框-🔥🔥🔥","children":[]},{"level":2,"title":"锚框与交并比(IoU)","slug":"锚框与交并比-iou","link":"#锚框与交并比-iou","children":[{"level":3,"title":"锚框","slug":"锚框","link":"#锚框","children":[]},{"level":3,"title":"交并比","slug":"交并比","link":"#交并比","children":[]},{"level":3,"title":"小结","slug":"小结","link":"#小结","children":[]}]},{"level":2,"title":"单发多框检测(SSD)","slug":"单发多框检测-ssd","link":"#单发多框检测-ssd","children":[{"level":3,"title":"模型","slug":"模型","link":"#模型","children":[]},{"level":3,"title":"类别预测层","slug":"类别预测层","link":"#类别预测层","children":[]},{"level":3,"title":"边界框预测层","slug":"边界框预测层","link":"#边界框预测层","children":[]},{"level":3,"title":"连结多尺度的预测","slug":"连结多尺度的预测","link":"#连结多尺度的预测","children":[]},{"level":3,"title":"高和宽减半块","slug":"高和宽减半块","link":"#高和宽减半块","children":[]}]},{"level":2,"title":"语义分割 🔥🔥🔥","slug":"语义分割-🔥🔥🔥","link":"#语义分割-🔥🔥🔥","children":[{"level":3,"title":"核心概念","slug":"核心概念","link":"#核心概念","children":[]},{"level":3,"title":"关键技术","slug":"关键技术","link":"#关键技术","children":[]},{"level":3,"title":"经典模型","slug":"经典模型","link":"#经典模型","children":[]}]},{"level":2,"title":"全卷积网络(FCN)","slug":"全卷积网络-fcn","link":"#全卷积网络-fcn","children":[{"level":3,"title":"初始化转置卷积层","slug":"初始化转置卷积层","link":"#初始化转置卷积层","children":[]},{"level":3,"title":"构造模型","slug":"构造模型","link":"#构造模型","children":[]}]},{"level":2,"title":"转置卷积(反卷积/上采样)","slug":"转置卷积-反卷积-上采样","link":"#转置卷积-反卷积-上采样","children":[]},{"level":2,"title":"风格迁移(style transfer)🔥🔥🔥","slug":"风格迁移-style-transfer-🔥🔥🔥","link":"#风格迁移-style-transfer-🔥🔥🔥","children":[]}],"relativePath":"aiart/deep-learning/computer-vision.md","filePath":"aiart/deep-learning/computer-vision.md"}'),y={name:"aiart/deep-learning/computer-vision.md"},m={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},T={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.566ex"},xmlns:"http://www.w3.org/2000/svg",width:"5.169ex",height:"2.262ex",role:"img",focusable:"false",viewBox:"0 -750 2284.7 1000","aria-hidden":"true"},Q={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},d={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.025ex"},xmlns:"http://www.w3.org/2000/svg",width:"1.294ex",height:"1.025ex",role:"img",focusable:"false",viewBox:"0 -442 572 453","aria-hidden":"true"},u={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},b={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.464ex"},xmlns:"http://www.w3.org/2000/svg",width:"1.109ex",height:"1.464ex",role:"img",focusable:"false",viewBox:"0 -442 490 647","aria-hidden":"true"},F={tabindex:"0",class:"MathJax",jax:"SVG",display:"true",style:{direction:"ltr",display:"block","text-align":"center",margin:"1em 0",position:"relative"}},h={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-2.17ex"},xmlns:"http://www.w3.org/2000/svg",width:"18.811ex",height:"5.471ex",role:"img",focusable:"false",viewBox:"0 -1459 8314.7 2418","aria-hidden":"true"},g={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},_={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.439ex"},xmlns:"http://www.w3.org/2000/svg",width:"1.041ex",height:"1.439ex",role:"img",focusable:"false",viewBox:"0 -442 460 636","aria-hidden":"true"},x={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},w={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.439ex"},xmlns:"http://www.w3.org/2000/svg",width:"4.938ex",height:"1.946ex",role:"img",focusable:"false",viewBox:"0 -666 2182.4 860","aria-hidden":"true"},f={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},B={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.566ex"},xmlns:"http://www.w3.org/2000/svg",width:"5.169ex",height:"2.262ex",role:"img",focusable:"false",viewBox:"0 -750 2284.7 1000","aria-hidden":"true"},v={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},k={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.566ex"},xmlns:"http://www.w3.org/2000/svg",width:"6.424ex",height:"2.283ex",role:"img",focusable:"false",viewBox:"0 -759 2839.6 1009","aria-hidden":"true"},H={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},A={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.025ex"},xmlns:"http://www.w3.org/2000/svg",width:"1.922ex",height:"1.742ex",role:"img",focusable:"false",viewBox:"0 -759 849.5 770","aria-hidden":"true"},M={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},L={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.464ex"},xmlns:"http://www.w3.org/2000/svg",width:"1.736ex",height:"2.181ex",role:"img",focusable:"false",viewBox:"0 -759 767.5 964","aria-hidden":"true"},C={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},D={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.566ex"},xmlns:"http://www.w3.org/2000/svg",width:"6.424ex",height:"2.283ex",role:"img",focusable:"false",viewBox:"0 -759 2839.6 1009","aria-hidden":"true"},V={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},Z={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.566ex"},xmlns:"http://www.w3.org/2000/svg",width:"5.169ex",height:"2.262ex",role:"img",focusable:"false",viewBox:"0 -750 2284.7 1000","aria-hidden":"true"},S={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},j={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.566ex"},xmlns:"http://www.w3.org/2000/svg",width:"6.424ex",height:"2.283ex",role:"img",focusable:"false",viewBox:"0 -759 2839.6 1009","aria-hidden":"true"},P={tabindex:"0",class:"MathJax",jax:"SVG",display:"true",style:{direction:"ltr",display:"block","text-align":"center",margin:"1em 0",position:"relative"}},q={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-3.006ex"},xmlns:"http://www.w3.org/2000/svg",width:"31.097ex",height:"5.155ex",role:"img",focusable:"false",viewBox:"0 -950 13744.8 2278.6","aria-hidden":"true"};function z(Y,n,I,N,X,R){return e(),p("div",null,[n[50]||(n[50]=l(`<h1 id="cv-computer-vision" tabindex="-1">CV：Computer Vision <a class="header-anchor" href="#cv-computer-vision" aria-label="Permalink to &quot;CV：Computer Vision&quot;">​</a></h1><p>计算机视觉、机器视觉。无论是医疗诊断、<strong>自动驾驶</strong>，还是智能滤波器、摄像头监控，许多计算机视觉领域的应用都与我们当前和未来的生活密切相关。</p><h2 id="微调-fine-tuning" tabindex="-1">微调(fine-tuning) <a class="header-anchor" href="#微调-fine-tuning" aria-label="Permalink to &quot;微调(fine-tuning)&quot;">​</a></h2><p><strong>迁移学习</strong>将从源数据集中学到的知识迁移到目标数据集，<strong>微调</strong>是迁移学习的常见技巧。</p><p><strong>微调</strong>包括以下四个步骤：</p><ol><li>在源数据集（例如 ImageNet 数据集）上预训练神经网络模型，即<strong>源模型</strong>。</li><li>创建一个新的神经网络模型，即<strong>目标模型</strong>。这将复制源模型上的所有模型设计及其参数（输出层除外）。我们假定这些模型参数包含从源数据集中学到的知识，这些知识也将适用于目标数据集。我们还假设源模型的输出层与源数据集的标签密切相关；因此不在目标模型中使用该层。</li><li>向目标模型添加<strong>输出层</strong>，其输出数是目标数据集中的类别数。然后随机初始化该层的模型参数。</li><li>在目标数据集（如椅子数据集）上训练目标模型。<strong>输出层将从头开始</strong>进行训练，而<strong>其他层</strong>参数将根据源模型参数进行<strong>微调</strong>。</li></ol><p>通常，<strong>微调</strong>参数使用<strong>较小学习率</strong>，而<strong>从头开始</strong>训练输出层可以使用<strong>更大学习率</strong>。</p><p><strong>示意代码</strong>：</p><div class="language-py line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">py</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> os</span></span>
<span class="line"><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> torch</span></span>
<span class="line"><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> torchvision</span></span>
<span class="line"><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> torch </span><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> nn</span></span>
<span class="line"><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> d2l </span><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> torch </span><span style="color:#F97583;">as</span><span style="color:#E1E4E8;"> d2l</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">finetune_net </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torchvision.models.resnet18(</span><span style="color:#FFAB70;">pretrained</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">True</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">finetune_net.fc </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> nn.Linear(finetune_net.fc.in_features, </span><span style="color:#79B8FF;">2</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">nn.init.xavier_uniform_(finetune_net.fc.weight)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 如果param_group=True，输出层中的模型参数将使用十倍的学习率</span></span>
<span class="line"><span style="color:#F97583;">def</span><span style="color:#B392F0;"> train_fine_tuning</span><span style="color:#E1E4E8;">(net, learning_rate, batch_size</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">128</span><span style="color:#E1E4E8;">, num_epochs</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">5</span><span style="color:#E1E4E8;">, param_group</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">True</span><span style="color:#E1E4E8;">):</span></span>
<span class="line"><span style="color:#E1E4E8;">  train_iter </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torch.utils.data.DataLoader(torchvision.datasets.ImageFolder(</span></span>
<span class="line"><span style="color:#E1E4E8;">    os.path.join(data_dir, </span><span style="color:#9ECBFF;">&#39;train&#39;</span><span style="color:#E1E4E8;">), </span><span style="color:#FFAB70;">transform</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">train_augs),</span></span>
<span class="line"><span style="color:#FFAB70;">    batch_size</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">batch_size, </span><span style="color:#FFAB70;">shuffle</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">True</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">  test_iter </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torch.utils.data.DataLoader(torchvision.datasets.ImageFolder(</span></span>
<span class="line"><span style="color:#E1E4E8;">    os.path.join(data_dir, </span><span style="color:#9ECBFF;">&#39;test&#39;</span><span style="color:#E1E4E8;">), </span><span style="color:#FFAB70;">transform</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">test_augs),</span></span>
<span class="line"><span style="color:#FFAB70;">    batch_size</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">batch_size)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">  devices </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> d2l.try_all_gpus()</span></span>
<span class="line"><span style="color:#E1E4E8;">  loss </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> nn.CrossEntropyLoss(</span><span style="color:#FFAB70;">reduction</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">&quot;none&quot;</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583;">  if</span><span style="color:#E1E4E8;"> param_group:</span></span>
<span class="line"><span style="color:#E1E4E8;">    params_1x </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> [</span></span>
<span class="line"><span style="color:#E1E4E8;">      param </span><span style="color:#F97583;">for</span><span style="color:#E1E4E8;"> name, param </span><span style="color:#F97583;">in</span><span style="color:#E1E4E8;"> net.named_parameters() </span><span style="color:#F97583;">if</span><span style="color:#E1E4E8;"> name </span><span style="color:#F97583;">not</span><span style="color:#F97583;"> in</span><span style="color:#E1E4E8;"> [</span><span style="color:#9ECBFF;">&quot;fc.weight&quot;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&quot;fc.bias&quot;</span><span style="color:#E1E4E8;">]</span></span>
<span class="line"><span style="color:#E1E4E8;">    ]</span></span>
<span class="line"><span style="color:#E1E4E8;">    trainer </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torch.optim.SGD(</span></span>
<span class="line"><span style="color:#E1E4E8;">      [{</span><span style="color:#9ECBFF;">&#39;params&#39;</span><span style="color:#E1E4E8;">: params_1x}, {</span><span style="color:#9ECBFF;">&#39;params&#39;</span><span style="color:#E1E4E8;">: net.fc.parameters(), </span><span style="color:#9ECBFF;">&#39;lr&#39;</span><span style="color:#E1E4E8;">: learning_rate </span><span style="color:#F97583;">*</span><span style="color:#79B8FF;"> 10</span><span style="color:#E1E4E8;">}],</span></span>
<span class="line"><span style="color:#FFAB70;">      lr</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">learning_rate, </span><span style="color:#FFAB70;">weight_decay</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">0.001</span></span>
<span class="line"><span style="color:#E1E4E8;">    )</span></span>
<span class="line"><span style="color:#F97583;">  else</span><span style="color:#E1E4E8;">:</span></span>
<span class="line"><span style="color:#E1E4E8;">    trainer </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torch.optim.SGD(net.parameters(), </span><span style="color:#FFAB70;">lr</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">learning_rate, </span><span style="color:#FFAB70;">weight_decay</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">0.001</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">  d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 使用较小学习率，通过微调预训练获得的模型参数</span></span>
<span class="line"><span style="color:#E1E4E8;">train_fine_tuning(finetune_net, </span><span style="color:#79B8FF;">5e-5</span><span style="color:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br></div></div><h2 id="目标检测与边界框-🔥🔥🔥" tabindex="-1">目标检测与边界框 🔥🔥🔥 <a class="header-anchor" href="#目标检测与边界框-🔥🔥🔥" aria-label="Permalink to &quot;目标检测与边界框 🔥🔥🔥&quot;">​</a></h2><p><strong>目标检测/识别</strong>：不仅可以检测<sup>object detection</sup>识别<sup>object recognition</sup>图像中所有感兴趣的物体，还能识别它们的位置，该位置通常由矩形边界框表示。</p><p><strong>边界框</strong>：在目标检测中，通常使用边界框<sup>bounding box</sup>来描述对象的空间位置。</p>`,12)),s("p",null,[n[6]||(n[6]=a("边界框通常是")),n[7]||(n[7]=s("strong",null,"矩形",-1)),n[8]||(n[8]=a("，两种常用边界框表示「中心 ")),s("mjx-container",m,[(e(),p("svg",T,n[0]||(n[0]=[l('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(961,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1405.7,0)"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(1895.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g>',1)]))),n[1]||(n[1]=s("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("mo",{stretchy:"false"},"("),s("mi",null,"x"),s("mo",null,","),s("mi",null,"y"),s("mo",{stretchy:"false"},")")])],-1))]),n[9]||(n[9]=a("，宽度，高度」和「左上 ")),s("mjx-container",Q,[(e(),p("svg",d,n[2]||(n[2]=[s("g",{stroke:"currentColor",fill:"currentColor","stroke-width":"0",transform:"scale(1,-1)"},[s("g",{"data-mml-node":"math"},[s("g",{"data-mml-node":"mi"},[s("path",{"data-c":"1D465",d:"M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z",style:{"stroke-width":"3"}})])])],-1)]))),n[3]||(n[3]=s("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("mi",null,"x")])],-1))]),n[10]||(n[10]=a("，右下 ")),s("mjx-container",u,[(e(),p("svg",b,n[4]||(n[4]=[s("g",{stroke:"currentColor",fill:"currentColor","stroke-width":"0",transform:"scale(1,-1)"},[s("g",{"data-mml-node":"math"},[s("g",{"data-mml-node":"mi"},[s("path",{"data-c":"1D466",d:"M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z",style:{"stroke-width":"3"}})])])],-1)]))),n[5]||(n[5]=s("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("mi",null,"y")])],-1))]),n[11]||(n[11]=a("」。"))]),n[51]||(n[51]=l(`<h2 id="锚框与交并比-iou" tabindex="-1">锚框与交并比(IoU) <a class="header-anchor" href="#锚框与交并比-iou" aria-label="Permalink to &quot;锚框与交并比(IoU)&quot;">​</a></h2><h3 id="锚框" tabindex="-1">锚框 <a class="header-anchor" href="#锚框" aria-label="Permalink to &quot;锚框&quot;">​</a></h3><p>目标检测算法通常会采样大量区域，判断其中是否包含目标，并调整边界更准确地预测目标真实边界框<sup>ground-truth bounding box</sup>，这些边界框被称为锚框<sup>anchor box</sup>。</p><p>不同模型采样各异。比如以每个像素为中心，生成多个缩放比和宽高比<sup>aspect ratio</sup>的不同边界框。</p><div class="language-py line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">py</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#6A737D;"># 生成以每个像素为中心具有不同形状的锚框</span></span>
<span class="line"><span style="color:#F97583;">def</span><span style="color:#B392F0;"> multibox_prior</span><span style="color:#E1E4E8;">(data, sizes, ratios):</span></span>
<span class="line"><span style="color:#E1E4E8;">  in_height, in_width </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> data.shape[</span><span style="color:#F97583;">-</span><span style="color:#79B8FF;">2</span><span style="color:#E1E4E8;">:]</span></span>
<span class="line"><span style="color:#E1E4E8;">  device, num_sizes, num_ratios </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> data.device, </span><span style="color:#79B8FF;">len</span><span style="color:#E1E4E8;">(sizes), </span><span style="color:#79B8FF;">len</span><span style="color:#E1E4E8;">(ratios)</span></span>
<span class="line"><span style="color:#E1E4E8;">  boxes_per_pixel </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> (num_sizes </span><span style="color:#F97583;">+</span><span style="color:#E1E4E8;"> num_ratios </span><span style="color:#F97583;">-</span><span style="color:#79B8FF;"> 1</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">  size_tensor </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torch.tensor(sizes, </span><span style="color:#FFAB70;">device</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">device)</span></span>
<span class="line"><span style="color:#E1E4E8;">  ratio_tensor </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torch.tensor(ratios, </span><span style="color:#FFAB70;">device</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">device)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;">  # 为了将锚点移动到像素的中心，需要设置偏移量。</span></span>
<span class="line"><span style="color:#6A737D;">  # 因为一个像素的高为1且宽为1，我们选择偏移我们的中心0.5</span></span>
<span class="line"><span style="color:#E1E4E8;">  offset_h, offset_w </span><span style="color:#F97583;">=</span><span style="color:#79B8FF;"> 0.5</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">0.5</span></span>
<span class="line"><span style="color:#E1E4E8;">  steps_h </span><span style="color:#F97583;">=</span><span style="color:#79B8FF;"> 1.0</span><span style="color:#F97583;"> /</span><span style="color:#E1E4E8;"> in_height  </span><span style="color:#6A737D;"># 在y轴上缩放步长</span></span>
<span class="line"><span style="color:#E1E4E8;">  steps_w </span><span style="color:#F97583;">=</span><span style="color:#79B8FF;"> 1.0</span><span style="color:#F97583;"> /</span><span style="color:#E1E4E8;"> in_width  </span><span style="color:#6A737D;"># 在x轴上缩放步长</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;">  # 生成锚框的所有中心点</span></span>
<span class="line"><span style="color:#E1E4E8;">  center_h </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> (torch.arange(in_height, </span><span style="color:#FFAB70;">device</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">device) </span><span style="color:#F97583;">+</span><span style="color:#E1E4E8;"> offset_h) </span><span style="color:#F97583;">*</span><span style="color:#E1E4E8;"> steps_h</span></span>
<span class="line"><span style="color:#E1E4E8;">  center_w </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> (torch.arange(in_width, </span><span style="color:#FFAB70;">device</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">device) </span><span style="color:#F97583;">+</span><span style="color:#E1E4E8;"> offset_w) </span><span style="color:#F97583;">*</span><span style="color:#E1E4E8;"> steps_w</span></span>
<span class="line"><span style="color:#E1E4E8;">  shift_y, shift_x </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torch.meshgrid(center_h, center_w, </span><span style="color:#FFAB70;">indexing</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">&#39;ij&#39;</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">  shift_y, shift_x </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> shift_y.reshape(</span><span style="color:#F97583;">-</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">), shift_x.reshape(</span><span style="color:#F97583;">-</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;">  # 生成“boxes_per_pixel”个高和宽，</span></span>
<span class="line"><span style="color:#6A737D;">  # 之后用于创建锚框的四角坐标(xmin,xmax,ymin,ymax)</span></span>
<span class="line"><span style="color:#E1E4E8;">  w </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torch.cat((size_tensor </span><span style="color:#F97583;">*</span><span style="color:#E1E4E8;"> torch.sqrt(ratio_tensor[</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">]),</span></span>
<span class="line"><span style="color:#E1E4E8;">                  sizes[</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">] </span><span style="color:#F97583;">*</span><span style="color:#E1E4E8;"> torch.sqrt(ratio_tensor[</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">:])))\\</span></span>
<span class="line"><span style="color:#F97583;">                  *</span><span style="color:#E1E4E8;"> in_height </span><span style="color:#F97583;">/</span><span style="color:#E1E4E8;"> in_width  </span><span style="color:#6A737D;"># 处理矩形输入</span></span>
<span class="line"><span style="color:#E1E4E8;">  h </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torch.cat((size_tensor </span><span style="color:#F97583;">/</span><span style="color:#E1E4E8;"> torch.sqrt(ratio_tensor[</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">]),</span></span>
<span class="line"><span style="color:#E1E4E8;">                  sizes[</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">] </span><span style="color:#F97583;">/</span><span style="color:#E1E4E8;"> torch.sqrt(ratio_tensor[</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">:])))</span></span>
<span class="line"><span style="color:#6A737D;">  # 除以2来获得半高和半宽</span></span>
<span class="line"><span style="color:#E1E4E8;">  anchor_manipulations </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torch.stack((</span><span style="color:#F97583;">-</span><span style="color:#E1E4E8;">w, </span><span style="color:#F97583;">-</span><span style="color:#E1E4E8;">h, w, h)).T.repeat(in_height </span><span style="color:#F97583;">*</span><span style="color:#E1E4E8;"> in_width, </span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">) </span><span style="color:#F97583;">/</span><span style="color:#79B8FF;"> 2</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;">  # 每个中心点都将有“boxes_per_pixel”个锚框，</span></span>
<span class="line"><span style="color:#6A737D;">  # 所以生成含所有锚框中心的网格，重复了“boxes_per_pixel”次</span></span>
<span class="line"><span style="color:#E1E4E8;">  out_grid </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torch.stack([shift_x, shift_y, shift_x, shift_y],</span></span>
<span class="line"><span style="color:#FFAB70;">              dim</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">).repeat_interleave(boxes_per_pixel, </span><span style="color:#FFAB70;">dim</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">  output </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> out_grid </span><span style="color:#F97583;">+</span><span style="color:#E1E4E8;"> anchor_manipulations</span></span>
<span class="line"><span style="color:#F97583;">  return</span><span style="color:#E1E4E8;"> output.unsqueeze(</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br></div></div><p>那么如何衡量锚框<strong>准确性</strong>呢？换言之，若已知目标真实边界框，如何衡量锚框和<strong>真实边界框</strong>之间<strong>相似性</strong>？杰卡德系数<sup>Jaccard</sup>可以衡量两者之间相似性。</p><h3 id="交并比" tabindex="-1">交并比 <a class="header-anchor" href="#交并比" aria-label="Permalink to &quot;交并比&quot;">​</a></h3><p><strong>IoU</strong>：Intersection Over Union <strong>交并比</strong>，两个边界框<strong>交集除以并集</strong>，也被称为杰卡德系数。</p>`,8)),s("mjx-container",F,[(e(),p("svg",h,n[12]||(n[12]=[l('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43D" d="M447 625Q447 637 354 637H329Q323 642 323 645T325 664Q329 677 335 683H352Q393 681 498 681Q541 681 568 681T605 682T619 682Q633 682 633 672Q633 670 630 658Q626 642 623 640T604 637Q552 637 545 623Q541 610 483 376Q420 128 419 127Q397 64 333 21T195 -22Q137 -22 97 8T57 88Q57 130 80 152T132 174Q177 174 182 130Q182 98 164 80T123 56Q115 54 115 53T122 44Q148 15 197 15Q235 15 271 47T324 130Q328 142 387 380T447 625Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(633,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1022,0)"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(1772,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(2216.7,0)"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(2975.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(3642.4,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" style="stroke-width:3;"></path></g><g data-mml-node="mfrac" transform="translate(4698.2,0)"><g data-mml-node="mrow" transform="translate(220,709.5)"><g data-mml-node="mo" transform="translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(278,0)"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(1250.2,0)"><path data-c="2229" d="M88 -21T75 -21T55 -7V200Q55 231 55 280Q56 414 60 428Q61 430 61 431Q77 500 152 549T332 598Q443 598 522 544T610 405Q611 399 611 194V-7Q604 -22 591 -22Q582 -22 572 -9L570 405Q563 433 556 449T529 485Q498 519 445 538T334 558Q251 558 179 518T96 401Q95 396 95 193V-7Q88 -21 75 -21Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(2139.4,0)"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(2898.4,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mrow" transform="translate(220,-709.5)"><g data-mml-node="mo" transform="translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(278,0)"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(1250.2,0)"><path data-c="222A" d="M591 598H592Q604 598 611 583V376Q611 345 611 296Q610 162 606 148Q605 146 605 145Q586 68 507 23T333 -22Q268 -22 209 -1T106 66T56 173Q55 180 55 384L56 585Q66 598 75 598Q85 598 95 585V378L96 172L98 162Q112 95 181 57T332 18Q415 18 487 58T570 175Q571 180 571 383V583Q579 598 591 598Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(2139.4,0)"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(2898.4,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z" style="stroke-width:3;"></path></g></g><rect width="3376.4" height="60" x="120" y="220"></rect></g></g></g>',1)]))),n[13]||(n[13]=s("mjx-assistive-mml",{unselectable:"on",display:"block",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",overflow:"hidden",width:"100%"}},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},[s("mi",null,"J"),s("mo",{stretchy:"false"},"("),s("mi",null,"A"),s("mo",null,","),s("mi",null,"B"),s("mo",{stretchy:"false"},")"),s("mo",null,"="),s("mfrac",null,[s("mrow",null,[s("mo",{"data-mjx-texclass":"ORD",stretchy:"false"},"|"),s("mi",null,"A"),s("mo",null,"∩"),s("mi",null,"B"),s("mo",{"data-mjx-texclass":"ORD",stretchy:"false"},"|")]),s("mrow",null,[s("mo",{"data-mjx-texclass":"ORD",stretchy:"false"},"|"),s("mi",null,"A"),s("mo",null,"∪"),s("mi",null,"B"),s("mo",{"data-mjx-texclass":"ORD",stretchy:"false"},"|")])])])],-1))]),n[52]||(n[52]=l('<p>交并比的取值范围在 0 和 1 之间：0 表示两个边界框无重合像素，1 表示两个边界框完全重合。 <img src="'+t+`" alt="An Image"></p><div class="language-py line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">py</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#6A737D;"># 计算两个锚框或边界框列表中成对的交并比</span></span>
<span class="line"><span style="color:#F97583;">def</span><span style="color:#B392F0;"> box_iou</span><span style="color:#E1E4E8;">(boxes1, boxes2):</span></span>
<span class="line"><span style="color:#E1E4E8;">  box_area </span><span style="color:#F97583;">=</span><span style="color:#F97583;"> lambda</span><span style="color:#E1E4E8;"> boxes: ((boxes[:, </span><span style="color:#79B8FF;">2</span><span style="color:#E1E4E8;">] </span><span style="color:#F97583;">-</span><span style="color:#E1E4E8;"> boxes[:, </span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">]) </span><span style="color:#F97583;">*</span><span style="color:#E1E4E8;"> (boxes[:, </span><span style="color:#79B8FF;">3</span><span style="color:#E1E4E8;">] </span><span style="color:#F97583;">-</span><span style="color:#E1E4E8;"> boxes[:, </span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">]))</span></span>
<span class="line"><span style="color:#6A737D;">  # boxes1,boxes2,areas1,areas2的形状:</span></span>
<span class="line"><span style="color:#6A737D;">  # boxes1：(boxes1的数量,4),</span></span>
<span class="line"><span style="color:#6A737D;">  # boxes2：(boxes2的数量,4),</span></span>
<span class="line"><span style="color:#6A737D;">  # areas1：(boxes1的数量,),</span></span>
<span class="line"><span style="color:#6A737D;">  # areas2：(boxes2的数量,)</span></span>
<span class="line"><span style="color:#E1E4E8;">  areas1 </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> box_area(boxes1)</span></span>
<span class="line"><span style="color:#E1E4E8;">  areas2 </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> box_area(boxes2)</span></span>
<span class="line"><span style="color:#6A737D;">  # inter_upperlefts,inter_lowerrights,inters的形状:</span></span>
<span class="line"><span style="color:#6A737D;">  # (boxes1的数量,boxes2的数量,2)</span></span>
<span class="line"><span style="color:#E1E4E8;">  inter_upperlefts </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torch.max(boxes1[:, </span><span style="color:#79B8FF;">None</span><span style="color:#E1E4E8;">, :</span><span style="color:#79B8FF;">2</span><span style="color:#E1E4E8;">], boxes2[:, :</span><span style="color:#79B8FF;">2</span><span style="color:#E1E4E8;">])</span></span>
<span class="line"><span style="color:#E1E4E8;">  inter_lowerrights </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torch.min(boxes1[:, </span><span style="color:#79B8FF;">None</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">2</span><span style="color:#E1E4E8;">:], boxes2[:, </span><span style="color:#79B8FF;">2</span><span style="color:#E1E4E8;">:])</span></span>
<span class="line"><span style="color:#E1E4E8;">  inters </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> (inter_lowerrights </span><span style="color:#F97583;">-</span><span style="color:#E1E4E8;"> inter_upperlefts).clamp(</span><span style="color:#FFAB70;">min</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#6A737D;">  # inter_areasandunion_areas的形状:(boxes1的数量,boxes2的数量)</span></span>
<span class="line"><span style="color:#E1E4E8;">  inter_areas </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> inters[:, :, </span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">] </span><span style="color:#F97583;">*</span><span style="color:#E1E4E8;"> inters[:, :, </span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">]</span></span>
<span class="line"><span style="color:#E1E4E8;">  union_areas </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> areas1[:, </span><span style="color:#79B8FF;">None</span><span style="color:#E1E4E8;">] </span><span style="color:#F97583;">+</span><span style="color:#E1E4E8;"> areas2 </span><span style="color:#F97583;">-</span><span style="color:#E1E4E8;"> inter_areas</span></span>
<span class="line"><span style="color:#F97583;">  return</span><span style="color:#E1E4E8;"> inter_areas </span><span style="color:#F97583;">/</span><span style="color:#E1E4E8;"> union_areas</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br></div></div><h3 id="小结" tabindex="-1">小结 <a class="header-anchor" href="#小结" aria-label="Permalink to &quot;小结&quot;">​</a></h3><p><strong>训练时</strong>：我们需要给每个锚框两种类型的标签。一个是与锚框中目标检测的类别，另一个是锚框真实相对于边界框的偏移量。</p><p><strong>预测时</strong>：可以使用<strong>非极大值抑制</strong><sup>non-maximum suppression，NMS</sup>来<strong>移除类似</strong>预测边界框，从而简化输出。</p><h2 id="单发多框检测-ssd" tabindex="-1">单发多框检测(SSD) <a class="header-anchor" href="#单发多框检测-ssd" aria-label="Permalink to &quot;单发多框检测(SSD)&quot;">​</a></h2><p><strong>SSD</strong>：Single Shot MultiBox Detector，是一种高效的目标检测算法，由<code>Wei Liu</code>等人在 2016 年<a href="https://arxiv.org/abs/1512.02325" target="_blank" rel="noreferrer">论文</a>中提出。它通过<strong>单次前向传播</strong>即可完成目标检测，具有<strong>速度快、精度高</strong>的特点，广泛应用于<strong>实时检测</strong>场景。</p><p>“单发”（<code>Single Shot</code>）是指算法仅需一次前向传播（即“单次通过神经网络”）即可直接输出检测结果，无需像传统两阶段方法（如 Faster R-CNN）那样先生成候选区域（Region Proposals），再对候选区域进行分类和回归。</p><p>在多个尺度下，生成<strong>不同尺寸锚框来检测不同尺寸目标</strong>。通过定义特征图的形状，决定任何图像上均匀采样的锚框中心。使用输入图像在某个<strong>感受野</strong>区域内信息，来预测输入图像上与该区域位置相近的锚框类别和偏移量。通过<strong>深度学习</strong>，用<strong>多层次图像分层</strong>表示进行<strong>多尺度目标检测</strong>。</p><h3 id="模型" tabindex="-1">模型 <a class="header-anchor" href="#模型" aria-label="Permalink to &quot;模型&quot;">​</a></h3><p>单发多框检测模型主要由<strong>一个基础网络</strong>块和若干<strong>多尺度特征</strong>块<strong>串联</strong>而成。 <img src="`+r+'" alt="An Image"></p><h3 id="类别预测层" tabindex="-1">类别预测层 <a class="header-anchor" href="#类别预测层" aria-label="Permalink to &quot;类别预测层&quot;">​</a></h3>',12)),s("p",null,[n[18]||(n[18]=a("设目标类别数量为 ")),s("mjx-container",g,[(e(),p("svg",_,n[14]||(n[14]=[s("g",{stroke:"currentColor",fill:"currentColor","stroke-width":"0",transform:"scale(1,-1)"},[s("g",{"data-mml-node":"math"},[s("g",{"data-mml-node":"mi"},[s("path",{"data-c":"1D45E",d:"M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z",style:{"stroke-width":"3"}})])])],-1)]))),n[15]||(n[15]=s("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("mi",null,"q")])],-1))]),n[19]||(n[19]=a("。这样一来，锚框有 ")),s("mjx-container",x,[(e(),p("svg",w,n[16]||(n[16]=[l('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(682.2,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(1682.4,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" style="stroke-width:3;"></path></g></g></g>',1)]))),n[17]||(n[17]=s("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("mi",null,"q"),s("mo",null,"+"),s("mn",null,"1")])],-1))]),n[20]||(n[20]=a(" 个类别，其中 0 类是背景。"))]),n[53]||(n[53]=l(`<div class="language-py line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">py</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> torch</span></span>
<span class="line"><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> torchvision</span></span>
<span class="line"><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> torch </span><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> nn</span></span>
<span class="line"><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> torch.nn </span><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> functional </span><span style="color:#F97583;">as</span><span style="color:#E1E4E8;"> F</span></span>
<span class="line"><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> d2l </span><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> torch </span><span style="color:#F97583;">as</span><span style="color:#E1E4E8;"> d2l</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583;">def</span><span style="color:#B392F0;"> cls_predictor</span><span style="color:#E1E4E8;">(num_inputs, num_anchors, num_classes):</span></span>
<span class="line"><span style="color:#F97583;">  return</span><span style="color:#E1E4E8;"> nn.Conv2d(num_inputs, num_anchors </span><span style="color:#F97583;">*</span><span style="color:#E1E4E8;"> (num_classes </span><span style="color:#F97583;">+</span><span style="color:#79B8FF;"> 1</span><span style="color:#E1E4E8;">), </span><span style="color:#FFAB70;">kernel_size</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">3</span><span style="color:#E1E4E8;">, </span><span style="color:#FFAB70;">padding</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><p>使用填充为 1 的 3x3 的卷积层，此卷积层的<strong>输入和输出</strong>宽度和高度<strong>保持不变</strong>。这样一来，输出和输入在特征图宽和高上的空间坐标一一对应。</p><h3 id="边界框预测层" tabindex="-1">边界框预测层 <a class="header-anchor" href="#边界框预测层" aria-label="Permalink to &quot;边界框预测层&quot;">​</a></h3><div class="language-py line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">py</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#F97583;">def</span><span style="color:#B392F0;"> bbox_predictor</span><span style="color:#E1E4E8;">(num_inputs, num_anchors):</span></span>
<span class="line"><span style="color:#F97583;">  return</span><span style="color:#E1E4E8;"> nn.Conv2d(num_inputs, num_anchors </span><span style="color:#F97583;">*</span><span style="color:#79B8FF;"> 4</span><span style="color:#E1E4E8;">, </span><span style="color:#FFAB70;">kernel_size</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">3</span><span style="color:#E1E4E8;">, </span><span style="color:#FFAB70;">padding</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>不同的是，这里需要为每个锚框预测 4 个偏移量，而不是 num_classes + 1 个类别。</p><h3 id="连结多尺度的预测" tabindex="-1">连结多尺度的预测 <a class="header-anchor" href="#连结多尺度的预测" aria-label="Permalink to &quot;连结多尺度的预测&quot;">​</a></h3><div class="language-py line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">py</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#F97583;">def</span><span style="color:#B392F0;"> forward</span><span style="color:#E1E4E8;">(x, block):</span></span>
<span class="line"><span style="color:#F97583;">  return</span><span style="color:#E1E4E8;"> block(x)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">Y1 </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> forward(torch.zeros((</span><span style="color:#79B8FF;">2</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">8</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">20</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">20</span><span style="color:#E1E4E8;">)), cls_predictor(</span><span style="color:#79B8FF;">8</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">5</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">10</span><span style="color:#E1E4E8;">))</span></span>
<span class="line"><span style="color:#E1E4E8;">Y2 </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> forward(torch.zeros((</span><span style="color:#79B8FF;">2</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">16</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">10</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">10</span><span style="color:#E1E4E8;">)), cls_predictor(</span><span style="color:#79B8FF;">16</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">3</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">10</span><span style="color:#E1E4E8;">))</span></span>
<span class="line"><span style="color:#E1E4E8;">Y1.shape, Y2.shape</span></span>
<span class="line"><span style="color:#6A737D;"># (torch.Size([2, 55, 20, 20]), torch.Size([2, 33, 10, 10]))</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p>除批量大小外，其他三个维度都不同尺寸。将预测结果转成二维(<code>批量大小，高 x 宽 x 通道数</code>)格式，后在维度 1 上连结。</p><div class="language-py line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">py</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#F97583;">def</span><span style="color:#B392F0;"> flatten_pred</span><span style="color:#E1E4E8;">(pred):</span></span>
<span class="line"><span style="color:#F97583;">  return</span><span style="color:#E1E4E8;"> torch.flatten(pred.permute(</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">2</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">3</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">), </span><span style="color:#FFAB70;">start_dim</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583;">def</span><span style="color:#B392F0;"> concat_preds</span><span style="color:#E1E4E8;">(preds):</span></span>
<span class="line"><span style="color:#F97583;">  return</span><span style="color:#E1E4E8;"> torch.cat([flatten_pred(p) </span><span style="color:#F97583;">for</span><span style="color:#E1E4E8;"> p </span><span style="color:#F97583;">in</span><span style="color:#E1E4E8;"> preds], </span><span style="color:#FFAB70;">dim</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">concat_preds([Y1, Y2]).shape</span></span>
<span class="line"><span style="color:#6A737D;"># torch.Size([2, 25300])</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><h3 id="高和宽减半块" tabindex="-1">高和宽减半块 <a class="header-anchor" href="#高和宽减半块" aria-label="Permalink to &quot;高和宽减半块&quot;">​</a></h3><div class="language-py line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">py</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#F97583;">def</span><span style="color:#B392F0;"> down_sample_blk</span><span style="color:#E1E4E8;">(in_channels, out_channels):</span></span>
<span class="line"><span style="color:#E1E4E8;">  blk </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> []</span></span>
<span class="line"><span style="color:#F97583;">  for</span><span style="color:#E1E4E8;"> _ </span><span style="color:#F97583;">in</span><span style="color:#79B8FF;"> range</span><span style="color:#E1E4E8;">(</span><span style="color:#79B8FF;">2</span><span style="color:#E1E4E8;">):</span></span>
<span class="line"><span style="color:#E1E4E8;">    blk.append(nn.Conv2d(in_channels, out_channels, </span><span style="color:#FFAB70;">kernel_size</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">3</span><span style="color:#E1E4E8;">, </span><span style="color:#FFAB70;">padding</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">))</span></span>
<span class="line"><span style="color:#E1E4E8;">    blk.append(nn.BatchNorm2d(out_channels))</span></span>
<span class="line"><span style="color:#E1E4E8;">    blk.append(nn.ReLU())</span></span>
<span class="line"><span style="color:#E1E4E8;">    in_channels </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> out_channels</span></span>
<span class="line"><span style="color:#E1E4E8;">  blk.append(nn.MaxPool2d(</span><span style="color:#79B8FF;">2</span><span style="color:#E1E4E8;">))</span></span>
<span class="line"><span style="color:#F97583;">  return</span><span style="color:#E1E4E8;"> nn.Sequential(</span><span style="color:#F97583;">*</span><span style="color:#E1E4E8;">blk)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><p>由两个填充为 1 的 3x3 卷积层(不变)、以及步幅为 2 的 2x2 最大汇聚层(减半)组成。</p><p>对于此高和宽减半块的输入和输出特征图，1x2+(3-1)+(3-1)=6，所以输出中每个单元在输入上都有一个 6x6 感受野。因此，<strong>高和宽减半</strong>块会<strong>扩大</strong>每个单元在其输出特征图中的<strong>感受野</strong>。</p><div class="language-py line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">py</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#E1E4E8;">forward(torch.zeros((</span><span style="color:#79B8FF;">2</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">3</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">20</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">20</span><span style="color:#E1E4E8;">)), down_sample_blk(</span><span style="color:#79B8FF;">3</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">10</span><span style="color:#E1E4E8;">)).shape</span></span>
<span class="line"><span style="color:#6A737D;"># torch.Size([2, 10, 10, 10])</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><h2 id="语义分割-🔥🔥🔥" tabindex="-1">语义分割 🔥🔥🔥 <a class="header-anchor" href="#语义分割-🔥🔥🔥" aria-label="Permalink to &quot;语义分割 🔥🔥🔥&quot;">​</a></h2><p>目标检测中，我们一直使用<strong>方形边界框</strong>来标注和预测图像中的目标。</p><h3 id="核心概念" tabindex="-1">核心概念 <a class="header-anchor" href="#核心概念" aria-label="Permalink to &quot;核心概念&quot;">​</a></h3><p><strong>语义分割</strong><sup>semantic segmentation</sup>，本质是密集预测<sup>Dense Prediction</sup>，为<strong>每个像素</strong>分配一个语义类别标签（如“人”“车”“天空”），实现像素级别分类。标注和预测都是<strong>像素级</strong>，比目标检测<strong>更精细</strong>。</p><p>语义分割输入图像和标签在像素上一一对应，输入图像会被<strong>随机裁剪</strong>为固定尺寸而<strong>不是缩放</strong>。</p><h3 id="关键技术" tabindex="-1">关键技术 <a class="header-anchor" href="#关键技术" aria-label="Permalink to &quot;关键技术&quot;">​</a></h3><ul><li><strong>全卷积网络(FCN)</strong>：将传统 CNN 中的全连接层替换为卷积层，使网络可以接受任意尺寸的输入并输出相应尺寸的分割图。</li><li><strong>编码器-解码器结构</strong>：编码器通过卷积和下采样提取高级特征，解码器通过上采样恢复空间分辨率。</li><li><strong>跳跃连接(Skip Connection)</strong>：将浅层特征与深层特征融合，保留更多空间细节信息。</li></ul><p>传统 CNN 全连接展平、全局池化会丢失<code>位置信息、相邻像素的梯度、区域一致性</code>等<strong>空间信息</strong>，而语义分割需要保留空间分辨率，因此必须使用<strong>全卷积结构</strong>。</p><h3 id="经典模型" tabindex="-1">经典模型 <a class="header-anchor" href="#经典模型" aria-label="Permalink to &quot;经典模型&quot;">​</a></h3><p><strong>1. FCN (Fully Convolutional Networks)</strong></p><ul><li>首个端到端的全卷积语义分割网络</li><li>使用<strong>转置卷积</strong>进行上采样</li><li>引入跳跃连接融合多层特征</li></ul><p><strong>2. <a href="https://arxiv.org/abs/1505.04597" target="_blank" rel="noreferrer">U-Net</a></strong></p><ul><li>医学图像分割的经典网络</li><li><strong>结构对称</strong>的编码器-解码器</li><li>大量跳跃连接保留细节信息</li></ul><p><strong>3. DeepLab 系列</strong></p><ul><li>使用空洞卷积(Atrous Convolution)扩大感受野</li><li>引入 ASPP(Atrous Spatial Pyramid Pooling)模块捕捉多尺度信息</li><li>使用 CRF(Conditional Random Field)后处理细化边界</li></ul><h2 id="全卷积网络-fcn" tabindex="-1">全卷积网络(FCN) <a class="header-anchor" href="#全卷积网络-fcn" aria-label="Permalink to &quot;全卷积网络(FCN)&quot;">​</a></h2><p><strong>FCN</strong>：Fully Convolutional Network，即网络<strong>完全由卷积层构成</strong>，没有任何全连接层。</p><p>这一设计使得网络能够处理任意尺寸的输入图像，并输出相应尺寸的密集预测（如图像分割中的逐像素分类）。</p><p>通过<strong>转置卷积</strong>，将中间层特征图的高和宽变换回输入图像的尺寸，输出类别预测与输入图像在像素级别上具有一一对应关系：<strong>通道维输出</strong>即该位置对应像素的<strong>类别预测</strong>。</p><h3 id="初始化转置卷积层" tabindex="-1">初始化转置卷积层 <a class="header-anchor" href="#初始化转置卷积层" aria-label="Permalink to &quot;初始化转置卷积层&quot;">​</a></h3><p>在图像处理中，我们有时需要<strong>将图像放大</strong>，即<strong>上采样</strong><sup>upsampling</sup>。</p><p><strong>双线性插值</strong><sup>bilinear interpolation</sup> 是常用上采样方法之一，它也经常用于<strong>初始化</strong>转置卷积层。</p>`,36)),s("ol",null,[s("li",null,[n[29]||(n[29]=a("将输出图像的坐标 ")),s("mjx-container",f,[(e(),p("svg",B,n[21]||(n[21]=[l('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(961,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1405.7,0)"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(1895.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g>',1)]))),n[22]||(n[22]=s("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("mo",{stretchy:"false"},"("),s("mi",null,"x"),s("mo",null,","),s("mi",null,"y"),s("mo",{stretchy:"false"},")")])],-1))]),n[30]||(n[30]=a(" 映射到输入图像的坐标 ")),s("mjx-container",v,[(e(),p("svg",k,n[23]||(n[23]=[l('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="msup" transform="translate(389,0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(605,363) scale(0.707)"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(1238.5,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="msup" transform="translate(1683.1,0)"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(523,363) scale(0.707)"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(2450.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g>',1)]))),n[24]||(n[24]=s("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("mo",{stretchy:"false"},"("),s("msup",null,[s("mi",null,"x"),s("mo",{"data-mjx-alternate":"1"},"′")]),s("mo",null,","),s("msup",null,[s("mi",null,"y"),s("mo",{"data-mjx-alternate":"1"},"′")]),s("mo",{stretchy:"false"},")")])],-1))]),n[31]||(n[31]=a(" 上。例如，根据输入与输出的尺寸之比来映射。请注意，映射后的 ")),s("mjx-container",H,[(e(),p("svg",A,n[25]||(n[25]=[l('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(605,363) scale(0.707)"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z" style="stroke-width:3;"></path></g></g></g></g>',1)]))),n[26]||(n[26]=s("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("msup",null,[s("mi",null,"x"),s("mo",{"data-mjx-alternate":"1"},"′")])])],-1))]),n[32]||(n[32]=a(" 和 ")),s("mjx-container",M,[(e(),p("svg",L,n[27]||(n[27]=[l('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(523,363) scale(0.707)"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z" style="stroke-width:3;"></path></g></g></g></g>',1)]))),n[28]||(n[28]=s("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("msup",null,[s("mi",null,"y"),s("mo",{"data-mjx-alternate":"1"},"′")])])],-1))]),n[33]||(n[33]=a(" 是实数。"))]),s("li",null,[n[36]||(n[36]=a("在输入图像上找到离坐标 ")),s("mjx-container",C,[(e(),p("svg",D,n[34]||(n[34]=[l('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="msup" transform="translate(389,0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(605,363) scale(0.707)"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(1238.5,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="msup" transform="translate(1683.1,0)"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(523,363) scale(0.707)"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(2450.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g>',1)]))),n[35]||(n[35]=s("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("mo",{stretchy:"false"},"("),s("msup",null,[s("mi",null,"x"),s("mo",{"data-mjx-alternate":"1"},"′")]),s("mo",null,","),s("msup",null,[s("mi",null,"y"),s("mo",{"data-mjx-alternate":"1"},"′")]),s("mo",{stretchy:"false"},")")])],-1))]),n[37]||(n[37]=a(" 最近的 4 个像素。"))]),s("li",null,[n[42]||(n[42]=a("输出图像在坐标 ")),s("mjx-container",V,[(e(),p("svg",Z,n[38]||(n[38]=[l('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(961,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1405.7,0)"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(1895.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g>',1)]))),n[39]||(n[39]=s("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("mo",{stretchy:"false"},"("),s("mi",null,"x"),s("mo",null,","),s("mi",null,"y"),s("mo",{stretchy:"false"},")")])],-1))]),n[43]||(n[43]=a(" 的像素依据输入图像这 4 个像素及其与 ")),s("mjx-container",S,[(e(),p("svg",j,n[40]||(n[40]=[l('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="msup" transform="translate(389,0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(605,363) scale(0.707)"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(1238.5,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="msup" transform="translate(1683.1,0)"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(523,363) scale(0.707)"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(2450.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g>',1)]))),n[41]||(n[41]=s("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("mo",{stretchy:"false"},"("),s("msup",null,[s("mi",null,"x"),s("mo",{"data-mjx-alternate":"1"},"′")]),s("mo",null,","),s("msup",null,[s("mi",null,"y"),s("mo",{"data-mjx-alternate":"1"},"′")]),s("mo",{stretchy:"false"},")")])],-1))]),n[44]||(n[44]=a(" 相对距离来计算。"))])]),n[54]||(n[54]=l(`<div class="language-py line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">py</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#F97583;">def</span><span style="color:#B392F0;"> bilinear_kernel</span><span style="color:#E1E4E8;">(in_channels, out_channels, kernel_size):</span></span>
<span class="line"><span style="color:#E1E4E8;">  factor </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> (kernel_size </span><span style="color:#F97583;">+</span><span style="color:#79B8FF;"> 1</span><span style="color:#E1E4E8;">) </span><span style="color:#F97583;">//</span><span style="color:#79B8FF;"> 2</span></span>
<span class="line"><span style="color:#F97583;">  if</span><span style="color:#E1E4E8;"> kernel_size </span><span style="color:#F97583;">%</span><span style="color:#79B8FF;"> 2</span><span style="color:#F97583;"> ==</span><span style="color:#79B8FF;"> 1</span><span style="color:#E1E4E8;">:</span></span>
<span class="line"><span style="color:#E1E4E8;">    center </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> factor </span><span style="color:#F97583;">-</span><span style="color:#79B8FF;"> 1</span></span>
<span class="line"><span style="color:#F97583;">  else</span><span style="color:#E1E4E8;">:</span></span>
<span class="line"><span style="color:#E1E4E8;">    center </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> factor </span><span style="color:#F97583;">-</span><span style="color:#79B8FF;"> 0.5</span></span>
<span class="line"><span style="color:#E1E4E8;">  og </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> (torch.arange(kernel_size).reshape(</span><span style="color:#F97583;">-</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">),</span></span>
<span class="line"><span style="color:#E1E4E8;">        torch.arange(kernel_size).reshape(</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">, </span><span style="color:#F97583;">-</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">))</span></span>
<span class="line"><span style="color:#E1E4E8;">  filt </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> (</span><span style="color:#79B8FF;">1</span><span style="color:#F97583;"> -</span><span style="color:#E1E4E8;"> torch.abs(og[</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">] </span><span style="color:#F97583;">-</span><span style="color:#E1E4E8;"> center) </span><span style="color:#F97583;">/</span><span style="color:#E1E4E8;"> factor) </span><span style="color:#F97583;">*</span><span style="color:#E1E4E8;"> \\</span></span>
<span class="line"><span style="color:#E1E4E8;">          (</span><span style="color:#79B8FF;">1</span><span style="color:#F97583;"> -</span><span style="color:#E1E4E8;"> torch.abs(og[</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">] </span><span style="color:#F97583;">-</span><span style="color:#E1E4E8;"> center) </span><span style="color:#F97583;">/</span><span style="color:#E1E4E8;"> factor)</span></span>
<span class="line"><span style="color:#E1E4E8;">  weight </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torch.zeros((in_channels, out_channels, kernel_size, kernel_size))</span></span>
<span class="line"><span style="color:#E1E4E8;">  weight[</span><span style="color:#79B8FF;">range</span><span style="color:#E1E4E8;">(in_channels), </span><span style="color:#79B8FF;">range</span><span style="color:#E1E4E8;">(out_channels), :, :] </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> filt</span></span>
<span class="line"><span style="color:#F97583;">  return</span><span style="color:#E1E4E8;"> weight</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br></div></div><h3 id="构造模型" tabindex="-1">构造模型 <a class="header-anchor" href="#构造模型" aria-label="Permalink to &quot;构造模型&quot;">​</a></h3><p><img src="`+c+`" alt="An Image"></p><ol><li>先使用<strong>卷积神经网络</strong>抽取图像特征——<strong>编码器</strong>提取特征(下采样)。</li><li>然后 1x1 卷积层<strong>将通道数变换为类别个数</strong>——调整通道数。</li><li>最后<strong>转置卷积层</strong>将特征图高和宽<strong>变换为输入图像尺寸</strong>——<strong>解码器</strong>恢复分辨率(上采样)。</li></ol><p>因此，模型输出与输入图像的高和宽相同，且最终输出通道包含了该空间位置像素类别预测。</p><div class="language-py line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">py</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> torch</span></span>
<span class="line"><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> torchvision</span></span>
<span class="line"><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> torch </span><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> nn</span></span>
<span class="line"><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> torch.nn </span><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> functional </span><span style="color:#F97583;">as</span><span style="color:#E1E4E8;"> F</span></span>
<span class="line"><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> d2l </span><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> torch </span><span style="color:#F97583;">as</span><span style="color:#E1E4E8;"> d2l</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 1.使用ImageNet数据集上预训练的ResNet-18来提取图像特征(编码器)</span></span>
<span class="line"><span style="color:#E1E4E8;">pretrained_net </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torchvision.models.resnet18(</span><span style="color:#FFAB70;">pretrained</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">True</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">net </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> nn.Sequential(</span><span style="color:#F97583;">*</span><span style="color:#79B8FF;">list</span><span style="color:#E1E4E8;">(pretrained_net.children())[:</span><span style="color:#F97583;">-</span><span style="color:#79B8FF;">2</span><span style="color:#E1E4E8;">])</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 使用Pascal VOC2012训练集</span></span>
<span class="line"><span style="color:#E1E4E8;">num_classes </span><span style="color:#F97583;">=</span><span style="color:#79B8FF;"> 21</span></span>
<span class="line"><span style="color:#6A737D;"># 2.添加1x1卷积层(调整通道数)</span></span>
<span class="line"><span style="color:#E1E4E8;">net.add_module(</span><span style="color:#9ECBFF;">&#39;final_conv&#39;</span><span style="color:#E1E4E8;">, nn.Conv2d(</span><span style="color:#79B8FF;">512</span><span style="color:#E1E4E8;">, num_classes, </span><span style="color:#FFAB70;">kernel_size</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">))</span></span>
<span class="line"><span style="color:#6A737D;"># 3.添加转置卷积层(解码器)</span></span>
<span class="line"><span style="color:#6A737D;"># 步幅为s，填充为s/2(整数)且卷积核高和宽为2s，转置卷积核会将输入高和宽分别放大s倍</span></span>
<span class="line"><span style="color:#E1E4E8;">net.add_module(</span><span style="color:#9ECBFF;">&#39;transpose_conv&#39;</span><span style="color:#E1E4E8;">, nn.ConvTranspose2d(num_classes,</span></span>
<span class="line"><span style="color:#E1E4E8;">                num_classes, </span><span style="color:#FFAB70;">kernel_size</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">64</span><span style="color:#E1E4E8;">, </span><span style="color:#FFAB70;">padding</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">16</span><span style="color:#E1E4E8;">, </span><span style="color:#FFAB70;">stride</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">32</span><span style="color:#E1E4E8;">))</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 用双线性插值的上采样初始化转置卷积层</span></span>
<span class="line"><span style="color:#E1E4E8;">W </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> bilinear_kernel(num_classes, num_classes, </span><span style="color:#79B8FF;">64</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">net.transpose_conv.weight.data.copy_(W)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 损失函数</span></span>
<span class="line"><span style="color:#6A737D;"># 因为使用通道预测像素类别，所以需要指定通道维。用每个像素预测类别是否正确来计算准确率。</span></span>
<span class="line"><span style="color:#F97583;">def</span><span style="color:#B392F0;"> loss</span><span style="color:#E1E4E8;">(inputs, targets):</span></span>
<span class="line"><span style="color:#F97583;">  return</span><span style="color:#E1E4E8;"> F.cross_entropy(inputs, targets, </span><span style="color:#FFAB70;">reduction</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">&#39;none&#39;</span><span style="color:#E1E4E8;">).mean(</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">).mean(</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 训练</span></span>
<span class="line"><span style="color:#E1E4E8;">num_epochs, lr, wd, devices </span><span style="color:#F97583;">=</span><span style="color:#79B8FF;"> 5</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">0.001</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">1e-3</span><span style="color:#E1E4E8;">, d2l.try_all_gpus()</span></span>
<span class="line"><span style="color:#E1E4E8;">trainer </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torch.optim.SGD(net.parameters(), </span><span style="color:#FFAB70;">lr</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">lr, </span><span style="color:#FFAB70;">weight_decay</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">wd)</span></span>
<span class="line"><span style="color:#E1E4E8;">d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 预测</span></span>
<span class="line"><span style="color:#6A737D;"># 需要将输入图像在各个通道做标准化，并转成卷积神经网络所需要四维输入格式</span></span>
<span class="line"><span style="color:#F97583;">def</span><span style="color:#B392F0;"> predict</span><span style="color:#E1E4E8;">(img):</span></span>
<span class="line"><span style="color:#E1E4E8;">  X </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> test_iter.dataset.normalize_image(img).unsqueeze(</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">  pred </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> net(X.to(devices[</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">])).argmax(</span><span style="color:#FFAB70;">dim</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#F97583;">  return</span><span style="color:#E1E4E8;"> pred.reshape(pred.shape[</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">], pred.shape[</span><span style="color:#79B8FF;">2</span><span style="color:#E1E4E8;">])</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 为可视化预测类别给每个像素，将预测类别映射回它们在数据集中的标注颜色</span></span>
<span class="line"><span style="color:#F97583;">def</span><span style="color:#B392F0;"> label2image</span><span style="color:#E1E4E8;">(pred):</span></span>
<span class="line"><span style="color:#E1E4E8;">  colormap </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torch.tensor(d2l.</span><span style="color:#79B8FF;">VOC_COLORMAP</span><span style="color:#E1E4E8;">, </span><span style="color:#FFAB70;">device</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">devices[</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">])</span></span>
<span class="line"><span style="color:#E1E4E8;">  X </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> pred.long()</span></span>
<span class="line"><span style="color:#F97583;">  return</span><span style="color:#E1E4E8;"> colormap[X, :]</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 预测启动</span></span>
<span class="line"><span style="color:#E1E4E8;">voc_dir </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> d2l.download_extract(</span><span style="color:#9ECBFF;">&#39;voc2012&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;VOCdevkit/VOC2012&#39;</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">test_images, test_labels </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> d2l.read_voc_images(voc_dir, </span><span style="color:#79B8FF;">False</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">n, imgs </span><span style="color:#F97583;">=</span><span style="color:#79B8FF;"> 4</span><span style="color:#E1E4E8;">, []</span></span>
<span class="line"><span style="color:#F97583;">for</span><span style="color:#E1E4E8;"> i </span><span style="color:#F97583;">in</span><span style="color:#79B8FF;"> range</span><span style="color:#E1E4E8;">(n):</span></span>
<span class="line"><span style="color:#E1E4E8;">    crop_rect </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> (</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">320</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">480</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">    X </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torchvision.transforms.functional.crop(test_images[i], </span><span style="color:#F97583;">*</span><span style="color:#E1E4E8;">crop_rect)</span></span>
<span class="line"><span style="color:#E1E4E8;">    pred </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> label2image(predict(X))</span></span>
<span class="line"><span style="color:#E1E4E8;">    imgs </span><span style="color:#F97583;">+=</span><span style="color:#E1E4E8;"> [X.permute(</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">,</span><span style="color:#79B8FF;">2</span><span style="color:#E1E4E8;">,</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">), pred.cpu(),</span></span>
<span class="line"><span style="color:#E1E4E8;">             torchvision.transforms.functional.crop(</span></span>
<span class="line"><span style="color:#E1E4E8;">                 test_labels[i], </span><span style="color:#F97583;">*</span><span style="color:#E1E4E8;">crop_rect).permute(</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">,</span><span style="color:#79B8FF;">2</span><span style="color:#E1E4E8;">,</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">)]</span></span>
<span class="line"><span style="color:#E1E4E8;">d2l.show_images(imgs[::</span><span style="color:#79B8FF;">3</span><span style="color:#E1E4E8;">] </span><span style="color:#F97583;">+</span><span style="color:#E1E4E8;"> imgs[</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">::</span><span style="color:#79B8FF;">3</span><span style="color:#E1E4E8;">] </span><span style="color:#F97583;">+</span><span style="color:#E1E4E8;"> imgs[</span><span style="color:#79B8FF;">2</span><span style="color:#E1E4E8;">::</span><span style="color:#79B8FF;">3</span><span style="color:#E1E4E8;">], </span><span style="color:#79B8FF;">3</span><span style="color:#E1E4E8;">, n, </span><span style="color:#FFAB70;">scale</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">2</span><span style="color:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br><span class="line-number">43</span><br><span class="line-number">44</span><br><span class="line-number">45</span><br><span class="line-number">46</span><br><span class="line-number">47</span><br><span class="line-number">48</span><br><span class="line-number">49</span><br><span class="line-number">50</span><br><span class="line-number">51</span><br><span class="line-number">52</span><br><span class="line-number">53</span><br><span class="line-number">54</span><br><span class="line-number">55</span><br><span class="line-number">56</span><br><span class="line-number">57</span><br><span class="line-number">58</span><br></div></div><h2 id="转置卷积-反卷积-上采样" tabindex="-1">转置卷积(反卷积/上采样) <a class="header-anchor" href="#转置卷积-反卷积-上采样" aria-label="Permalink to &quot;转置卷积(反卷积/上采样)&quot;">​</a></h2><p>卷积神经网络<sup>CNN</sup>的卷积层和汇聚层，通常会减少<strong>下采样</strong>输入图像空间维度（高和宽）。</p><p><img src="`+i+'" alt="An Image"><strong>转置卷积</strong><sup>transposed convolution</sup>通过卷积核<strong>广播</strong>输入元素，增加<strong>上采样</strong>中间层特征图空间维度，实现<strong>输出大于输入</strong>，用于逆转下采样导致的空间尺寸减小。</p><p><strong>填充</strong>：转置卷积中，填充被应<strong>用于输出</strong>（常规卷积将填充应用于输入）。例如，当将高和宽两侧填充数指定为 1 时，转置卷积输出中将<strong>删除第一和最后的行与列</strong>。</p><p><img src="'+E+`" alt="An Image"><strong>步幅</strong>：被指定为中间结果（输出），而不是输入。</p><p><strong>多输入和输出通道</strong>：转置卷积与常规卷积以<strong>相同</strong>方式运作。</p><p><strong>矩阵变换</strong>：转置卷积层能够<strong>交换</strong>卷积层的<strong>正向传播</strong>函数和<strong>反向传播</strong>函数。</p><h2 id="风格迁移-style-transfer-🔥🔥🔥" tabindex="-1">风格迁移(style transfer)🔥🔥🔥 <a class="header-anchor" href="#风格迁移-style-transfer-🔥🔥🔥" aria-label="Permalink to &quot;风格迁移(style transfer)🔥🔥🔥&quot;">​</a></h2><p>把一张图的“内容”和另一张图的“风格”结合，生成一张“内容不变但风格变化”的图。</p><p><strong>典型例子：</strong> 你拍了一张猫，用梵高《星夜》风格迁移它，就能得到一张“星夜风格猫”。</p><p><strong>底层原理(简化)：</strong></p><ul><li>内容图像保留结构和形状信息（比如人脸、物体轮廓）。</li><li>风格图像提供纹理、颜色、笔触风格等。</li><li>通过神经网络（如卷积神经网络 CNN）提取两者的特征，并组合输出。</li></ul><p><strong>常见技术：</strong></p><ul><li>Gatys 等人提出的经典神经风格迁移（基于 VGG 网络）。</li><li>更快的实时风格迁移（Fast Style Transfer）用于移动端 App（如 Prisma）。</li></ul><p><strong>编码实现：</strong></p><div class="language-py line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">py</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> torch</span></span>
<span class="line"><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> torchvision</span></span>
<span class="line"><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> torch </span><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> nn</span></span>
<span class="line"><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> d2l </span><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> torch </span><span style="color:#F97583;">as</span><span style="color:#E1E4E8;"> d2l</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">content_img </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> d2l.Image.open(</span><span style="color:#9ECBFF;">&#39;../img/rainier.jpg&#39;</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">style_img </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> d2l.Image.open(</span><span style="color:#9ECBFF;">&#39;../img/autumn-oak.jpg&#39;</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 预处理和后处理</span></span>
<span class="line"><span style="color:#E1E4E8;">rgb_mean </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torch.tensor([</span><span style="color:#79B8FF;">0.485</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">0.456</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">0.406</span><span style="color:#E1E4E8;">])</span></span>
<span class="line"><span style="color:#E1E4E8;">rgb_std </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torch.tensor([</span><span style="color:#79B8FF;">0.229</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">0.224</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">0.225</span><span style="color:#E1E4E8;">])</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583;">def</span><span style="color:#B392F0;"> preprocess</span><span style="color:#E1E4E8;">(img, image_shape):</span></span>
<span class="line"><span style="color:#E1E4E8;">  transforms </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torchvision.transforms.Compose([</span></span>
<span class="line"><span style="color:#E1E4E8;">    torchvision.transforms.Resize(image_shape),</span></span>
<span class="line"><span style="color:#E1E4E8;">    torchvision.transforms.ToTensor(),</span></span>
<span class="line"><span style="color:#E1E4E8;">    torchvision.transforms.Normalize(</span><span style="color:#FFAB70;">mean</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">rgb_mean, </span><span style="color:#FFAB70;">std</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">rgb_std)])</span></span>
<span class="line"><span style="color:#F97583;">  return</span><span style="color:#E1E4E8;"> transforms(img).unsqueeze(</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583;">def</span><span style="color:#B392F0;"> postprocess</span><span style="color:#E1E4E8;">(img):</span></span>
<span class="line"><span style="color:#E1E4E8;">  img </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> img[</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">].to(rgb_std.device)</span></span>
<span class="line"><span style="color:#E1E4E8;">  img </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torch.clamp(img.permute(</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">2</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">) </span><span style="color:#F97583;">*</span><span style="color:#E1E4E8;"> rgb_std </span><span style="color:#F97583;">+</span><span style="color:#E1E4E8;"> rgb_mean, </span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#F97583;">  return</span><span style="color:#E1E4E8;"> torchvision.transforms.ToPILImage()(img.permute(</span><span style="color:#79B8FF;">2</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">))</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 抽取图像特征</span></span>
<span class="line"><span style="color:#6A737D;"># 使用基于ImageNet数据集预训练的VGG-19模型来抽取图像特征</span></span>
<span class="line"><span style="color:#E1E4E8;">pretrained_net </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torchvision.models.vgg19(</span><span style="color:#FFAB70;">pretrained</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">True</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">style_layers, content_layers </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> [</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">5</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">10</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">19</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">28</span><span style="color:#E1E4E8;">], [</span><span style="color:#79B8FF;">25</span><span style="color:#E1E4E8;">]</span></span>
<span class="line"><span style="color:#6A737D;"># 抽取特定层，新建网络模型</span></span>
<span class="line"><span style="color:#E1E4E8;">net </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> nn.Sequential(</span><span style="color:#F97583;">*</span><span style="color:#E1E4E8;">[pretrained_net.features[i] </span><span style="color:#F97583;">for</span><span style="color:#E1E4E8;"> i </span><span style="color:#F97583;">in</span><span style="color:#79B8FF;"> range</span><span style="color:#E1E4E8;">(</span><span style="color:#79B8FF;">max</span><span style="color:#E1E4E8;">(content_layers </span><span style="color:#F97583;">+</span><span style="color:#E1E4E8;"> style_layers) </span><span style="color:#F97583;">+</span><span style="color:#79B8FF;"> 1</span><span style="color:#E1E4E8;">)])</span></span>
<span class="line"><span style="color:#6A737D;"># 抽取、存储目标内容层和风格层</span></span>
<span class="line"><span style="color:#F97583;">def</span><span style="color:#B392F0;"> extract_features</span><span style="color:#E1E4E8;">(X, content_layers, style_layers):</span></span>
<span class="line"><span style="color:#E1E4E8;">  contents </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> []</span></span>
<span class="line"><span style="color:#E1E4E8;">  styles </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> []</span></span>
<span class="line"><span style="color:#F97583;">  for</span><span style="color:#E1E4E8;"> i </span><span style="color:#F97583;">in</span><span style="color:#79B8FF;"> range</span><span style="color:#E1E4E8;">(</span><span style="color:#79B8FF;">len</span><span style="color:#E1E4E8;">(net)):</span></span>
<span class="line"><span style="color:#E1E4E8;">    X </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> net[i](X)</span></span>
<span class="line"><span style="color:#F97583;">    if</span><span style="color:#E1E4E8;"> i </span><span style="color:#F97583;">in</span><span style="color:#E1E4E8;"> style_layers:</span></span>
<span class="line"><span style="color:#E1E4E8;">      styles.append(X)</span></span>
<span class="line"><span style="color:#F97583;">    if</span><span style="color:#E1E4E8;"> i </span><span style="color:#F97583;">in</span><span style="color:#E1E4E8;"> content_layers:</span></span>
<span class="line"><span style="color:#E1E4E8;">      contents.append(X)</span></span>
<span class="line"><span style="color:#F97583;">  return</span><span style="color:#E1E4E8;"> contents, styles</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 抽取内容特征</span></span>
<span class="line"><span style="color:#F97583;">def</span><span style="color:#B392F0;"> get_contents</span><span style="color:#E1E4E8;">(image_shape, device):</span></span>
<span class="line"><span style="color:#E1E4E8;">  content_X </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> preprocess(content_img, image_shape).to(device)</span></span>
<span class="line"><span style="color:#E1E4E8;">  contents_Y, _ </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> extract_features(content_X, content_layers, style_layers)</span></span>
<span class="line"><span style="color:#F97583;">  return</span><span style="color:#E1E4E8;"> content_X, contents_Y</span></span>
<span class="line"><span style="color:#6A737D;"># 抽取风格特征</span></span>
<span class="line"><span style="color:#F97583;">def</span><span style="color:#B392F0;"> get_styles</span><span style="color:#E1E4E8;">(image_shape, device):</span></span>
<span class="line"><span style="color:#E1E4E8;">  style_X </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> preprocess(style_img, image_shape).to(device)</span></span>
<span class="line"><span style="color:#E1E4E8;">  _, styles_Y </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> extract_features(style_X, content_layers, style_layers)</span></span>
<span class="line"><span style="color:#F97583;">  return</span><span style="color:#E1E4E8;"> style_X, styles_Y</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 定义损失函数(内容损失 + 风格损失 + 全变分损失)</span></span>
<span class="line"><span style="color:#6A737D;"># 内容损失：使合成图像与内容图像在内容特征上接近</span></span>
<span class="line"><span style="color:#6A737D;"># 通过平方误差函数衡量合成图像与内容图像在 内容特征 差异</span></span>
<span class="line"><span style="color:#F97583;">def</span><span style="color:#B392F0;"> content_loss</span><span style="color:#E1E4E8;">(Y_hat, Y):</span></span>
<span class="line"><span style="color:#6A737D;">  # 我们从动态计算梯度的树中分离目标：</span></span>
<span class="line"><span style="color:#6A737D;">  # 这是一个规定的值，而不是一个变量。</span></span>
<span class="line"><span style="color:#F97583;">  return</span><span style="color:#E1E4E8;"> torch.square(Y_hat </span><span style="color:#F97583;">-</span><span style="color:#E1E4E8;"> Y.detach()).mean()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 风格损失：使合成图像与风格图像在风格特征上接近</span></span>
<span class="line"><span style="color:#6A737D;"># 通过平方误差函数衡量合成图像与风格图像在 风格特征 差异</span></span>
<span class="line"><span style="color:#F97583;">def</span><span style="color:#B392F0;"> style_loss</span><span style="color:#E1E4E8;">(Y_hat, gram_Y):</span></span>
<span class="line"><span style="color:#F97583;">  return</span><span style="color:#E1E4E8;"> torch.square(gram(Y_hat) </span><span style="color:#F97583;">-</span><span style="color:#E1E4E8;"> gram_Y.detach()).mean()</span></span>
<span class="line"><span style="color:#6A737D;"># 格拉姆矩阵：表达风格特征之间互相关性，表达风格层输出的风格</span></span>
<span class="line"><span style="color:#F97583;">def</span><span style="color:#B392F0;"> gram</span><span style="color:#E1E4E8;">(X):</span></span>
<span class="line"><span style="color:#E1E4E8;">  num_channels, n </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> X.shape[</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">], X.numel() </span><span style="color:#F97583;">//</span><span style="color:#E1E4E8;"> X.shape[</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">]</span></span>
<span class="line"><span style="color:#E1E4E8;">  X </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> X.reshape((num_channels, n))</span></span>
<span class="line"><span style="color:#F97583;">  return</span><span style="color:#E1E4E8;"> torch.matmul(X, X.T) </span><span style="color:#F97583;">/</span><span style="color:#E1E4E8;"> (num_channels </span><span style="color:#F97583;">*</span><span style="color:#E1E4E8;"> n)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 全变分损失：减少合成图像中噪点</span></span>
<span class="line"><span style="color:#6A737D;"># 合成图像会有高频噪点，极暗或极亮。全变分去噪(total variation denoising)使邻近像素值相似。</span></span>
<span class="line"><span style="color:#F97583;">def</span><span style="color:#B392F0;"> tv_loss</span><span style="color:#E1E4E8;">(Y_hat):</span></span>
<span class="line"><span style="color:#F97583;">  return</span><span style="color:#79B8FF;"> 0.5</span><span style="color:#F97583;"> *</span><span style="color:#E1E4E8;"> (torch.abs(Y_hat[:, :, </span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">:, :] </span><span style="color:#F97583;">-</span><span style="color:#E1E4E8;"> Y_hat[:, :, :</span><span style="color:#F97583;">-</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">, :]).mean() </span><span style="color:#F97583;">+</span></span>
<span class="line"><span style="color:#E1E4E8;">               torch.abs(Y_hat[:, :, :, </span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">:] </span><span style="color:#F97583;">-</span><span style="color:#E1E4E8;"> Y_hat[:, :, :, :</span><span style="color:#F97583;">-</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">]).mean())</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 风格转移损失函数是内容损失、风格损失和总变化损失的加权和</span></span>
<span class="line"><span style="color:#E1E4E8;">content_weight, style_weight, tv_weight </span><span style="color:#F97583;">=</span><span style="color:#79B8FF;"> 1</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">1e3</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">10</span></span>
<span class="line"><span style="color:#F97583;">def</span><span style="color:#B392F0;"> compute_loss</span><span style="color:#E1E4E8;">(X, contents_Y_hat, styles_Y_hat, contents_Y, styles_Y_gram):</span></span>
<span class="line"><span style="color:#6A737D;">  # 分别计算内容损失、风格损失和全变分损失</span></span>
<span class="line"><span style="color:#E1E4E8;">  contents_l </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> [content_loss(Y_hat, Y) </span><span style="color:#F97583;">*</span><span style="color:#E1E4E8;"> content_weight </span><span style="color:#F97583;">for</span><span style="color:#E1E4E8;"> Y_hat, Y </span><span style="color:#F97583;">in</span><span style="color:#79B8FF;"> zip</span><span style="color:#E1E4E8;">(contents_Y_hat, contents_Y)]</span></span>
<span class="line"><span style="color:#E1E4E8;">  styles_l </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> [style_loss(Y_hat, Y) </span><span style="color:#F97583;">*</span><span style="color:#E1E4E8;"> style_weight </span><span style="color:#F97583;">for</span><span style="color:#E1E4E8;"> Y_hat, Y </span><span style="color:#F97583;">in</span><span style="color:#79B8FF;"> zip</span><span style="color:#E1E4E8;">(styles_Y_hat, styles_Y_gram)]</span></span>
<span class="line"><span style="color:#E1E4E8;">  tv_l </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> tv_loss(X) </span><span style="color:#F97583;">*</span><span style="color:#E1E4E8;"> tv_weight</span></span>
<span class="line"><span style="color:#6A737D;">  # 对所有损失求和</span></span>
<span class="line"><span style="color:#E1E4E8;">  l </span><span style="color:#F97583;">=</span><span style="color:#79B8FF;"> sum</span><span style="color:#E1E4E8;">(</span><span style="color:#79B8FF;">10</span><span style="color:#F97583;"> *</span><span style="color:#E1E4E8;"> styles_l </span><span style="color:#F97583;">+</span><span style="color:#E1E4E8;"> contents_l </span><span style="color:#F97583;">+</span><span style="color:#E1E4E8;"> [tv_l])</span></span>
<span class="line"><span style="color:#F97583;">  return</span><span style="color:#E1E4E8;"> contents_l, styles_l, tv_l, l</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 初始化合成图像：风格迁移中，合成的图像是训练期间唯一需要更新的变量。</span></span>
<span class="line"><span style="color:#6A737D;"># 定义一个模型，将合成图像视为模型参数，模型前向传播只需返回模型参数即可。</span></span>
<span class="line"><span style="color:#F97583;">class</span><span style="color:#B392F0;"> SynthesizedImage</span><span style="color:#E1E4E8;">(</span><span style="color:#B392F0;">nn</span><span style="color:#E1E4E8;">.</span><span style="color:#B392F0;">Module</span><span style="color:#E1E4E8;">):</span></span>
<span class="line"><span style="color:#F97583;">  def</span><span style="color:#79B8FF;"> __init__</span><span style="color:#E1E4E8;">(self, img_shape, </span><span style="color:#F97583;">**</span><span style="color:#E1E4E8;">kwargs):</span></span>
<span class="line"><span style="color:#79B8FF;">    super</span><span style="color:#E1E4E8;">(SynthesizedImage, </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">).</span><span style="color:#79B8FF;">__init__</span><span style="color:#E1E4E8;">(</span><span style="color:#F97583;">**</span><span style="color:#E1E4E8;">kwargs)</span></span>
<span class="line"><span style="color:#79B8FF;">    self</span><span style="color:#E1E4E8;">.weight </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> nn.Parameter(torch.rand(</span><span style="color:#F97583;">*</span><span style="color:#E1E4E8;">img_shape))</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583;">  def</span><span style="color:#B392F0;"> forward</span><span style="color:#E1E4E8;">(self):</span></span>
<span class="line"><span style="color:#F97583;">    return</span><span style="color:#79B8FF;"> self</span><span style="color:#E1E4E8;">.weight</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 创建模型实例，并初始化为内容图像 X</span></span>
<span class="line"><span style="color:#F97583;">def</span><span style="color:#B392F0;"> get_inits</span><span style="color:#E1E4E8;">(X, device, lr, styles_Y):</span></span>
<span class="line"><span style="color:#E1E4E8;">  gen_img </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> SynthesizedImage(X.shape).to(device)</span></span>
<span class="line"><span style="color:#E1E4E8;">  gen_img.weight.data.copy_(X.data)</span></span>
<span class="line"><span style="color:#E1E4E8;">  trainer </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torch.optim.Adam(gen_img.parameters(), </span><span style="color:#FFAB70;">lr</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">lr)</span></span>
<span class="line"><span style="color:#E1E4E8;">  styles_Y_gram </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> [gram(Y) </span><span style="color:#F97583;">for</span><span style="color:#E1E4E8;"> Y </span><span style="color:#F97583;">in</span><span style="color:#E1E4E8;"> styles_Y]</span></span>
<span class="line"><span style="color:#F97583;">  return</span><span style="color:#E1E4E8;"> gen_img(), styles_Y_gram, trainer</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 训练</span></span>
<span class="line"><span style="color:#F97583;">def</span><span style="color:#B392F0;"> train</span><span style="color:#E1E4E8;">(X, contents_Y, styles_Y, device, lr, num_epochs, lr_decay_epoch):</span></span>
<span class="line"><span style="color:#E1E4E8;">  X, styles_Y_gram, trainer </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> get_inits(X, device, lr, styles_Y)</span></span>
<span class="line"><span style="color:#E1E4E8;">  scheduler </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torch.optim.lr_scheduler.StepLR(trainer, lr_decay_epoch, </span><span style="color:#79B8FF;">0.8</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">  animator </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> d2l.Animator(</span><span style="color:#FFAB70;">xlabel</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">&#39;epoch&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#FFAB70;">ylabel</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">&#39;loss&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#FFAB70;">xlim</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">[</span><span style="color:#79B8FF;">10</span><span style="color:#E1E4E8;">, num_epochs],</span></span>
<span class="line"><span style="color:#FFAB70;">                          legend</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">[</span><span style="color:#9ECBFF;">&#39;content&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;style&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;TV&#39;</span><span style="color:#E1E4E8;">],</span></span>
<span class="line"><span style="color:#FFAB70;">                          ncols</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">2</span><span style="color:#E1E4E8;">, </span><span style="color:#FFAB70;">figsize</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">(</span><span style="color:#79B8FF;">7</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">2.5</span><span style="color:#E1E4E8;">))</span></span>
<span class="line"><span style="color:#F97583;">  for</span><span style="color:#E1E4E8;"> epoch </span><span style="color:#F97583;">in</span><span style="color:#79B8FF;"> range</span><span style="color:#E1E4E8;">(num_epochs):</span></span>
<span class="line"><span style="color:#E1E4E8;">    trainer.zero_grad()</span></span>
<span class="line"><span style="color:#E1E4E8;">    contents_Y_hat, styles_Y_hat </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> extract_features(X, content_layers, style_layers)</span></span>
<span class="line"><span style="color:#E1E4E8;">    contents_l, styles_l, tv_l, l </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> compute_loss(X, contents_Y_hat, styles_Y_hat, contents_Y, styles_Y_gram)</span></span>
<span class="line"><span style="color:#E1E4E8;">    l.backward()</span></span>
<span class="line"><span style="color:#E1E4E8;">    trainer.step()</span></span>
<span class="line"><span style="color:#E1E4E8;">    scheduler.step()</span></span>
<span class="line"><span style="color:#F97583;">    if</span><span style="color:#E1E4E8;"> (epoch </span><span style="color:#F97583;">+</span><span style="color:#79B8FF;"> 1</span><span style="color:#E1E4E8;">) </span><span style="color:#F97583;">%</span><span style="color:#79B8FF;"> 10</span><span style="color:#F97583;"> ==</span><span style="color:#79B8FF;"> 0</span><span style="color:#E1E4E8;">:</span></span>
<span class="line"><span style="color:#E1E4E8;">      animator.axes[</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">].imshow(postprocess(X))</span></span>
<span class="line"><span style="color:#E1E4E8;">      animator.add(epoch </span><span style="color:#F97583;">+</span><span style="color:#79B8FF;"> 1</span><span style="color:#E1E4E8;">, [</span><span style="color:#79B8FF;">float</span><span style="color:#E1E4E8;">(</span><span style="color:#79B8FF;">sum</span><span style="color:#E1E4E8;">(contents_l)), </span><span style="color:#79B8FF;">float</span><span style="color:#E1E4E8;">(</span><span style="color:#79B8FF;">sum</span><span style="color:#E1E4E8;">(styles_l)), </span><span style="color:#79B8FF;">float</span><span style="color:#E1E4E8;">(tv_l)])</span></span>
<span class="line"><span style="color:#F97583;">  return</span><span style="color:#E1E4E8;"> X</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># gogogo!!!</span></span>
<span class="line"><span style="color:#E1E4E8;">device, image_shape </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> d2l.try_gpu(), (</span><span style="color:#79B8FF;">300</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">450</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">net </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> net.to(device)</span></span>
<span class="line"><span style="color:#E1E4E8;">content_X, contents_Y </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> get_contents(image_shape, device)</span></span>
<span class="line"><span style="color:#E1E4E8;">_, styles_Y </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> get_styles(image_shape, device)</span></span>
<span class="line"><span style="color:#E1E4E8;">output </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> train(content_X, contents_Y, styles_Y, device, </span><span style="color:#79B8FF;">0.3</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">500</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">50</span><span style="color:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br><span class="line-number">43</span><br><span class="line-number">44</span><br><span class="line-number">45</span><br><span class="line-number">46</span><br><span class="line-number">47</span><br><span class="line-number">48</span><br><span class="line-number">49</span><br><span class="line-number">50</span><br><span class="line-number">51</span><br><span class="line-number">52</span><br><span class="line-number">53</span><br><span class="line-number">54</span><br><span class="line-number">55</span><br><span class="line-number">56</span><br><span class="line-number">57</span><br><span class="line-number">58</span><br><span class="line-number">59</span><br><span class="line-number">60</span><br><span class="line-number">61</span><br><span class="line-number">62</span><br><span class="line-number">63</span><br><span class="line-number">64</span><br><span class="line-number">65</span><br><span class="line-number">66</span><br><span class="line-number">67</span><br><span class="line-number">68</span><br><span class="line-number">69</span><br><span class="line-number">70</span><br><span class="line-number">71</span><br><span class="line-number">72</span><br><span class="line-number">73</span><br><span class="line-number">74</span><br><span class="line-number">75</span><br><span class="line-number">76</span><br><span class="line-number">77</span><br><span class="line-number">78</span><br><span class="line-number">79</span><br><span class="line-number">80</span><br><span class="line-number">81</span><br><span class="line-number">82</span><br><span class="line-number">83</span><br><span class="line-number">84</span><br><span class="line-number">85</span><br><span class="line-number">86</span><br><span class="line-number">87</span><br><span class="line-number">88</span><br><span class="line-number">89</span><br><span class="line-number">90</span><br><span class="line-number">91</span><br><span class="line-number">92</span><br><span class="line-number">93</span><br><span class="line-number">94</span><br><span class="line-number">95</span><br><span class="line-number">96</span><br><span class="line-number">97</span><br><span class="line-number">98</span><br><span class="line-number">99</span><br><span class="line-number">100</span><br><span class="line-number">101</span><br><span class="line-number">102</span><br><span class="line-number">103</span><br><span class="line-number">104</span><br><span class="line-number">105</span><br><span class="line-number">106</span><br><span class="line-number">107</span><br><span class="line-number">108</span><br><span class="line-number">109</span><br><span class="line-number">110</span><br><span class="line-number">111</span><br><span class="line-number">112</span><br><span class="line-number">113</span><br><span class="line-number">114</span><br><span class="line-number">115</span><br><span class="line-number">116</span><br><span class="line-number">117</span><br><span class="line-number">118</span><br><span class="line-number">119</span><br><span class="line-number">120</span><br><span class="line-number">121</span><br><span class="line-number">122</span><br><span class="line-number">123</span><br><span class="line-number">124</span><br><span class="line-number">125</span><br><span class="line-number">126</span><br><span class="line-number">127</span><br><span class="line-number">128</span><br><span class="line-number">129</span><br><span class="line-number">130</span><br><span class="line-number">131</span><br><span class="line-number">132</span><br></div></div><p><strong>上面编码中技术术语</strong>：</p>`,23)),s("ul",null,[n[49]||(n[49]=s("li",null,[s("a",{href:"/aiart/deep-learning/basic-concept.html#格拉姆矩阵-gram-matrix"},"格拉姆矩阵(Gram Matrix)")],-1)),s("li",null,[n[47]||(n[47]=s("strong",null,"全变分损失",-1)),n[48]||(n[48]=a("公式为：")),s("mjx-container",P,[(e(),p("svg",q,n[45]||(n[45]=[l('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="munder"><g data-mml-node="mo"><path data-c="2211" d="M60 948Q63 950 665 950H1267L1325 815Q1384 677 1388 669H1348L1341 683Q1320 724 1285 761Q1235 809 1174 838T1033 881T882 898T699 902H574H543H251L259 891Q722 258 724 252Q725 250 724 246Q721 243 460 -56L196 -356Q196 -357 407 -357Q459 -357 548 -357T676 -358Q812 -358 896 -353T1063 -332T1204 -283T1307 -196Q1328 -170 1348 -124H1388Q1388 -125 1381 -145T1356 -210T1325 -294L1267 -449L666 -450Q64 -450 61 -448Q55 -446 55 -439Q55 -437 57 -433L590 177Q590 178 557 222T452 366T322 544L56 909L55 924Q55 945 60 948Z" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" transform="translate(356.1,-1084.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(345,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(623,0)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z" style="stroke-width:3;"></path></g></g></g><g data-mml-node="mo" transform="translate(1610.7,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z" style="stroke-width:3;"></path></g><g data-mml-node="msub" transform="translate(1888.7,0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" transform="translate(605,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(345,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(623,0)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z" style="stroke-width:3;"></path></g></g></g><g data-mml-node="mo" transform="translate(3497.7,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z" style="stroke-width:3;"></path></g><g data-mml-node="msub" transform="translate(4498,0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" transform="translate(605,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(345,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(1123,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(1623,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1901,0)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z" style="stroke-width:3;"></path></g></g></g><g data-mml-node="mo" transform="translate(6788.5,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(7288.7,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(8288.9,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z" style="stroke-width:3;"></path></g><g data-mml-node="msub" transform="translate(8566.9,0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" transform="translate(605,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(345,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(623,0)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z" style="stroke-width:3;"></path></g></g></g><g data-mml-node="mo" transform="translate(10176,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z" style="stroke-width:3;"></path></g><g data-mml-node="msub" transform="translate(11176.2,0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" transform="translate(605,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(345,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(623,0)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(1035,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(1813,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" style="stroke-width:3;"></path></g></g></g><g data-mml-node="mo" transform="translate(13466.8,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z" style="stroke-width:3;"></path></g></g></g>',1)]))),n[46]||(n[46]=s("mjx-assistive-mml",{unselectable:"on",display:"block",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",overflow:"hidden",width:"100%"}},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},[s("munder",null,[s("mo",{"data-mjx-texclass":"OP"},"∑"),s("mrow",{"data-mjx-texclass":"ORD"},[s("mi",null,"i"),s("mo",null,","),s("mi",null,"j")])]),s("mo",{"data-mjx-texclass":"ORD",stretchy:"false"},"|"),s("msub",null,[s("mi",null,"x"),s("mrow",{"data-mjx-texclass":"ORD"},[s("mi",null,"i"),s("mo",null,","),s("mi",null,"j")])]),s("mo",null,"−"),s("msub",null,[s("mi",null,"x"),s("mrow",{"data-mjx-texclass":"ORD"},[s("mi",null,"i"),s("mo",null,"+"),s("mn",null,"1"),s("mo",null,","),s("mi",null,"j")])]),s("mo",{"data-mjx-texclass":"ORD",stretchy:"false"},"|"),s("mo",null,"+"),s("mo",{"data-mjx-texclass":"ORD",stretchy:"false"},"|"),s("msub",null,[s("mi",null,"x"),s("mrow",{"data-mjx-texclass":"ORD"},[s("mi",null,"i"),s("mo",null,","),s("mi",null,"j")])]),s("mo",null,"−"),s("msub",null,[s("mi",null,"x"),s("mrow",{"data-mjx-texclass":"ORD"},[s("mi",null,"i"),s("mo",null,","),s("mi",null,"j"),s("mo",null,"+"),s("mn",null,"1")])]),s("mo",{"data-mjx-texclass":"ORD",stretchy:"false"},"|")])],-1))])])])])}const J=o(y,[["render",z]]);export{G as __pageData,J as default};
