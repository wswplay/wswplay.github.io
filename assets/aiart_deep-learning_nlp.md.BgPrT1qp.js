import{_ as n,c as a,o as l,b as p}from"./chunks/framework._eNwL97Z.js";const u=JSON.parse('{"title":"自然语言处理、NLP","description":"","frontmatter":{"title":"自然语言处理、NLP","outline":"deep"},"headers":[{"level":2,"title":"预训练(pre-training)","slug":"预训练-pre-training","link":"#预训练-pre-training","children":[{"level":3,"title":"子词嵌入","slug":"子词嵌入","link":"#子词嵌入","children":[]},{"level":3,"title":"字节对编码(BPE)","slug":"字节对编码-bpe","link":"#字节对编码-bpe","children":[]},{"level":3,"title":"Transformers 的 BERT","slug":"transformers-的-bert","link":"#transformers-的-bert","children":[]}]}],"relativePath":"aiart/deep-learning/nlp.md","filePath":"aiart/deep-learning/nlp.md"}'),o={name:"aiart/deep-learning/nlp.md"};function e(r,s,t,c,E,i){return l(),a("div",null,s[0]||(s[0]=[p(`<h1 id="自然语言处理-nlp" tabindex="-1">自然语言处理(NLP) <a class="header-anchor" href="#自然语言处理-nlp" aria-label="Permalink to &quot;自然语言处理(NLP)&quot;">​</a></h1><p><strong>NLP</strong>：Natural Language Processing，是指研究使用自然语言的<strong>计算机和人类</strong>之间的<strong>交互</strong>。</p><h2 id="预训练-pre-training" tabindex="-1">预训练(pre-training) <a class="header-anchor" href="#预训练-pre-training" aria-label="Permalink to &quot;预训练(pre-training)&quot;">​</a></h2><p><strong>自监督学习</strong><sup>self-supervised learning</sup>已被广泛用于<strong>预训练</strong>文本表示，例如通过使用周围文本的其它部分来预测文本的隐藏部分。</p><h3 id="子词嵌入" tabindex="-1">子词嵌入 <a class="header-anchor" href="#子词嵌入" aria-label="Permalink to &quot;子词嵌入&quot;">​</a></h3><div class="language-py line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">py</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#6A737D;"># 代码示例（Hugging Face 库）</span></span>
<span class="line"><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> transformers </span><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> AutoTokenizer</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 加载BERT的WordPiece分词器</span></span>
<span class="line"><span style="color:#E1E4E8;">tokenizer </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> AutoTokenizer.from_pretrained(</span><span style="color:#9ECBFF;">&quot;bert-base-uncased&quot;</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">tokens </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> tokenizer.tokenize(</span><span style="color:#9ECBFF;">&quot;unhappiness&quot;</span><span style="color:#E1E4E8;">)  </span><span style="color:#6A737D;"># 输出：[&#39;un&#39;, &#39;##happy&#39;, &#39;##ness&#39;]</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 加载BPE分词器（GPT-2）</span></span>
<span class="line"><span style="color:#E1E4E8;">tokenizer </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> AutoTokenizer.from_pretrained(</span><span style="color:#9ECBFF;">&quot;gpt2&quot;</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">tokens </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> tokenizer.tokenize(</span><span style="color:#9ECBFF;">&quot;unhappiness&quot;</span><span style="color:#E1E4E8;">)  </span><span style="color:#6A737D;"># 输出：[&#39;un&#39;, &#39;happiness&#39;]</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><p>自然语言是用来表达人脑思维的复杂系统，<strong>词</strong>是意义的基本单元。</p><ul><li><strong>词向量</strong>：用于表示<strong>单词意义</strong>的向量，并且还可以被认为是单词<strong>特征向量或表示</strong>。</li><li><strong>词嵌入</strong>：将<strong>单词映射到实向量</strong>的技术。</li></ul><p><strong>子词嵌入</strong><sup>Subword Embedding</sup>通过<strong>将单词拆分为更小的单元</strong>（子词）来解决传统词嵌入局限性。</p><ul><li>未登录词（OOV）问题：传统词嵌入（如 Word2Vec）无法处理词汇表外的单词。</li><li>形态学相似性：子词能捕捉单词间的结构关系（如&quot;running&quot; → &quot;run&quot; + &quot;ing&quot;）。</li><li>多语言支持：共享子词可跨语言建模（如拉丁语系的共同词根）。</li></ul><p><strong>子词嵌入</strong> = <strong>子词分词</strong>(<code>BPE</code>等) + <strong>向量化</strong><sup>Embedding</sup></p><h3 id="字节对编码-bpe" tabindex="-1">字节对编码(BPE) <a class="header-anchor" href="#字节对编码-bpe" aria-label="Permalink to &quot;字节对编码(BPE)&quot;">​</a></h3><p><strong>BPE</strong>：Byte Pair Encoding，是一种<strong>子词分词算法</strong><sup>Subword Tokenization</sup>。</p><p><strong>核心思想</strong>：通过迭代<strong>合并高频符号对</strong>(字节对)构建词汇表，有效平衡词表大小与语义覆盖能力。</p><p><strong>基本步骤</strong>：</p><ol><li><strong>初始化词汇表</strong>：将所有基本字符（如字母、标点）加入词表。</li><li><strong>统计符号对频率</strong>：在训练语料中统计所有相邻符号对的共现频率。</li><li><strong>合并最高频对</strong>：将出现频率最高的符号对合并为一个新符号，加入词表。</li><li><strong>重复迭代</strong>：持续合并直到达到预设的词表大小或迭代次数。</li></ol><p><strong>示例演示</strong>：</p><p>假设语料为 <code>&quot;low low low lower newest widest&quot;</code>：</p><ul><li>初始词表：<code>{l, o, w, e, r, n, s, t, d, i}</code></li><li>第 1 步：最高频对 <code>&quot;lo&quot;</code>（出现 6 次）→ 合并为 <code>&quot;lo&quot;</code>，词表新增 <code>&quot;lo&quot;</code>。</li><li>第 2 步：最高频对 <code>&quot;low&quot;</code>（出现 4 次）→ 合并为 <code>&quot;low&quot;</code>，词表新增 <code>&quot;low&quot;</code>。</li><li>后续可能合并 <code>&quot;er&quot;</code>、<code>&quot;est&quot;</code>等。</li><li>最终词表可能包含子词如：<code>low, er, est, newer, wid</code>。</li></ul><p><strong>优点优势</strong>：</p><ul><li><strong>解决未登录词（OOV）</strong>：通过子词组合表示罕见词（如 <code>&quot;unhappiness&quot;</code> → <code>&quot;un&quot; + &quot;happy&quot; + &quot;ness&quot;</code>）。</li><li><strong>压缩词表大小</strong>：避免维护超大词表（如英语常用词约 20 万，BPE 词表可压缩到 1 万~3 万）。</li><li><strong>保留语义</strong>：相似词共享子词（如 <code>&quot;playing&quot;</code> 和 <code>&quot;played&quot;</code> 共享 <code>&quot;play&quot;</code>）。</li></ul><p><strong>代码示例</strong>：</p><div class="language-py line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">py</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#6A737D;"># Hugging Face 库调用</span></span>
<span class="line"><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> transformers </span><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> GPT2Tokenizer</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># GPT-2使用BPE分词</span></span>
<span class="line"><span style="color:#E1E4E8;">tokenizer </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> GPT2Tokenizer.from_pretrained(</span><span style="color:#9ECBFF;">&quot;gpt2&quot;</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">tokens </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> tokenizer.tokenize(</span><span style="color:#9ECBFF;">&quot;unhappiness&quot;</span><span style="color:#E1E4E8;">)  </span><span style="color:#6A737D;"># 输出：[&#39;un&#39;, &#39;happiness&#39;]</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p><strong>代码实现</strong>：</p><div class="language-py line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">py</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> collections</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">symbols </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> [</span><span style="color:#9ECBFF;">&#39;a&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;b&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;c&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;d&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;e&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;f&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;g&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;h&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;i&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;j&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;k&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;l&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;m&#39;</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#9ECBFF;">           &#39;n&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;o&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;p&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;q&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;r&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;s&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;t&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;u&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;v&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;w&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;x&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;y&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;z&#39;</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#9ECBFF;">           &#39;_&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;[UNK]&#39;</span><span style="color:#E1E4E8;">]</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 频率字典</span></span>
<span class="line"><span style="color:#E1E4E8;">raw_token_freqs </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> {</span><span style="color:#9ECBFF;">&#39;fast_&#39;</span><span style="color:#E1E4E8;">: </span><span style="color:#79B8FF;">4</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;faster_&#39;</span><span style="color:#E1E4E8;">: </span><span style="color:#79B8FF;">3</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;tall_&#39;</span><span style="color:#E1E4E8;">: </span><span style="color:#79B8FF;">5</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;taller_&#39;</span><span style="color:#E1E4E8;">: </span><span style="color:#79B8FF;">4</span><span style="color:#E1E4E8;">}</span></span>
<span class="line"><span style="color:#E1E4E8;">token_freqs </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> {}</span></span>
<span class="line"><span style="color:#F97583;">for</span><span style="color:#E1E4E8;"> token, freq </span><span style="color:#F97583;">in</span><span style="color:#E1E4E8;"> raw_token_freqs.items():</span></span>
<span class="line"><span style="color:#E1E4E8;">  token_freqs[</span><span style="color:#9ECBFF;">&#39; &#39;</span><span style="color:#E1E4E8;">.join(</span><span style="color:#79B8FF;">list</span><span style="color:#E1E4E8;">(token))] </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> raw_token_freqs[token]</span></span>
<span class="line"><span style="color:#E1E4E8;">token_freqs</span></span>
<span class="line"><span style="color:#6A737D;"># {&#39;f a s t _&#39;: 4, &#39;f a s t e r _&#39;: 3, &#39;t a l l _&#39;: 5, &#39;t a l l e r _&#39;: 4}</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 返回词内最频繁的连续符号对，其中词来自输入词典token_freqs的键</span></span>
<span class="line"><span style="color:#F97583;">def</span><span style="color:#B392F0;"> get_max_freq_pair</span><span style="color:#E1E4E8;">(token_freqs):</span></span>
<span class="line"><span style="color:#E1E4E8;">  pairs </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> collections.defaultdict(</span><span style="color:#79B8FF;">int</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#F97583;">  for</span><span style="color:#E1E4E8;"> token, freq </span><span style="color:#F97583;">in</span><span style="color:#E1E4E8;"> token_freqs.items():</span></span>
<span class="line"><span style="color:#E1E4E8;">    symbols </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> token.split()</span></span>
<span class="line"><span style="color:#F97583;">    for</span><span style="color:#E1E4E8;"> i </span><span style="color:#F97583;">in</span><span style="color:#79B8FF;"> range</span><span style="color:#E1E4E8;">(</span><span style="color:#79B8FF;">len</span><span style="color:#E1E4E8;">(symbols) </span><span style="color:#F97583;">-</span><span style="color:#79B8FF;"> 1</span><span style="color:#E1E4E8;">):</span></span>
<span class="line"><span style="color:#6A737D;">      # “pairs”的键是两个连续符号的元组</span></span>
<span class="line"><span style="color:#E1E4E8;">      pairs[symbols[i], symbols[i </span><span style="color:#F97583;">+</span><span style="color:#79B8FF;"> 1</span><span style="color:#E1E4E8;">]] </span><span style="color:#F97583;">+=</span><span style="color:#E1E4E8;"> freq</span></span>
<span class="line"><span style="color:#F97583;">  return</span><span style="color:#79B8FF;"> max</span><span style="color:#E1E4E8;">(pairs, </span><span style="color:#FFAB70;">key</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">pairs.get)  </span><span style="color:#6A737D;"># 具有最大值的“pairs”键</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 基于连续符号频率的贪心方法，合并最频繁的连续符号对以产生新符号</span></span>
<span class="line"><span style="color:#F97583;">def</span><span style="color:#B392F0;"> merge_symbols</span><span style="color:#E1E4E8;">(max_freq_pair, token_freqs, symbols):</span></span>
<span class="line"><span style="color:#E1E4E8;">  symbols.append(</span><span style="color:#9ECBFF;">&#39;&#39;</span><span style="color:#E1E4E8;">.join(max_freq_pair))</span></span>
<span class="line"><span style="color:#E1E4E8;">  new_token_freqs </span><span style="color:#F97583;">=</span><span style="color:#79B8FF;"> dict</span><span style="color:#E1E4E8;">()</span></span>
<span class="line"><span style="color:#F97583;">  for</span><span style="color:#E1E4E8;"> token, freq </span><span style="color:#F97583;">in</span><span style="color:#E1E4E8;"> token_freqs.items():</span></span>
<span class="line"><span style="color:#E1E4E8;">    new_token </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> token.replace(</span><span style="color:#9ECBFF;">&#39; &#39;</span><span style="color:#E1E4E8;">.join(max_freq_pair), </span><span style="color:#9ECBFF;">&#39;&#39;</span><span style="color:#E1E4E8;">.join(max_freq_pair))</span></span>
<span class="line"><span style="color:#E1E4E8;">    new_token_freqs[new_token] </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> token_freqs[token]</span></span>
<span class="line"><span style="color:#F97583;">  return</span><span style="color:#E1E4E8;"> new_token_freqs</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 迭代</span></span>
<span class="line"><span style="color:#E1E4E8;">num_merges </span><span style="color:#F97583;">=</span><span style="color:#79B8FF;"> 10</span></span>
<span class="line"><span style="color:#F97583;">for</span><span style="color:#E1E4E8;"> i </span><span style="color:#F97583;">in</span><span style="color:#79B8FF;"> range</span><span style="color:#E1E4E8;">(num_merges):</span></span>
<span class="line"><span style="color:#E1E4E8;">  max_freq_pair </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> get_max_freq_pair(token_freqs)</span></span>
<span class="line"><span style="color:#E1E4E8;">  token_freqs </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> merge_symbols(max_freq_pair, token_freqs, symbols)</span></span>
<span class="line"><span style="color:#79B8FF;">  print</span><span style="color:#E1E4E8;">(</span><span style="color:#F97583;">f</span><span style="color:#9ECBFF;">&#39;合并# </span><span style="color:#79B8FF;">{</span><span style="color:#E1E4E8;">i</span><span style="color:#F97583;">+</span><span style="color:#79B8FF;">1}</span><span style="color:#9ECBFF;">:&#39;</span><span style="color:#E1E4E8;">,max_freq_pair)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 合并# 1: (&#39;t&#39;, &#39;a&#39;)</span></span>
<span class="line"><span style="color:#6A737D;"># 合并# 2: (&#39;ta&#39;, &#39;l&#39;)</span></span>
<span class="line"><span style="color:#6A737D;"># 合并# 3: (&#39;tal&#39;, &#39;l&#39;)</span></span>
<span class="line"><span style="color:#6A737D;"># 合并# 4: (&#39;f&#39;, &#39;a&#39;)</span></span>
<span class="line"><span style="color:#6A737D;"># 合并# 5: (&#39;fa&#39;, &#39;s&#39;)</span></span>
<span class="line"><span style="color:#6A737D;"># 合并# 6: (&#39;fas&#39;, &#39;t&#39;)</span></span>
<span class="line"><span style="color:#6A737D;"># 合并# 7: (&#39;e&#39;, &#39;r&#39;)</span></span>
<span class="line"><span style="color:#6A737D;"># 合并# 8: (&#39;er&#39;, &#39;_&#39;)</span></span>
<span class="line"><span style="color:#6A737D;"># 合并# 9: (&#39;tall&#39;, &#39;_&#39;)</span></span>
<span class="line"><span style="color:#6A737D;"># 合并# 10: (&#39;fast&#39;, &#39;_&#39;)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#79B8FF;">print</span><span style="color:#E1E4E8;">(symbols)</span></span>
<span class="line"><span style="color:#6A737D;"># [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;, &#39;e&#39;, &#39;f&#39;, &#39;g&#39;, &#39;h&#39;, &#39;i&#39;, &#39;j&#39;, &#39;k&#39;, &#39;l&#39;, &#39;m&#39;, &#39;n&#39;, &#39;o&#39;, &#39;p&#39;, &#39;q&#39;, &#39;r&#39;, &#39;s&#39;, &#39;t&#39;, &#39;u&#39;, &#39;v&#39;, &#39;w&#39;, &#39;x&#39;, &#39;y&#39;, &#39;z&#39;, &#39;_&#39;, &#39;[UNK]&#39;, &#39;ta&#39;, &#39;tal&#39;, &#39;tall&#39;, &#39;fa&#39;, &#39;fas&#39;, &#39;fast&#39;, &#39;er&#39;, &#39;er_&#39;, &#39;tall_&#39;, &#39;fast_&#39;]</span></span>
<span class="line"></span>
<span class="line"><span style="color:#79B8FF;">print</span><span style="color:#E1E4E8;">(</span><span style="color:#79B8FF;">list</span><span style="color:#E1E4E8;">(token_freqs.keys()))</span></span>
<span class="line"><span style="color:#6A737D;"># [&#39;fast_&#39;, &#39;fast er_&#39;, &#39;tall_&#39;, &#39;tall er_&#39;]</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 用从一个数据集学习的子词来切分另一个数据集的单词</span></span>
<span class="line"><span style="color:#6A737D;"># 尝试将单词从输入参数symbols分成可能最长的子词</span></span>
<span class="line"><span style="color:#F97583;">def</span><span style="color:#B392F0;"> segment_BPE</span><span style="color:#E1E4E8;">(tokens, symbols):</span></span>
<span class="line"><span style="color:#E1E4E8;">  outputs </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> []</span></span>
<span class="line"><span style="color:#F97583;">  for</span><span style="color:#E1E4E8;"> token </span><span style="color:#F97583;">in</span><span style="color:#E1E4E8;"> tokens:</span></span>
<span class="line"><span style="color:#E1E4E8;">    start, end </span><span style="color:#F97583;">=</span><span style="color:#79B8FF;"> 0</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">len</span><span style="color:#E1E4E8;">(token)</span></span>
<span class="line"><span style="color:#E1E4E8;">    cur_output </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> []</span></span>
<span class="line"><span style="color:#6A737D;">    # 具有符号中可能最长子字的词元段</span></span>
<span class="line"><span style="color:#F97583;">    while</span><span style="color:#E1E4E8;"> start </span><span style="color:#F97583;">&lt;</span><span style="color:#79B8FF;"> len</span><span style="color:#E1E4E8;">(token) </span><span style="color:#F97583;">and</span><span style="color:#E1E4E8;"> start </span><span style="color:#F97583;">&lt;</span><span style="color:#E1E4E8;"> end:</span></span>
<span class="line"><span style="color:#F97583;">      if</span><span style="color:#E1E4E8;"> token[start: end] </span><span style="color:#F97583;">in</span><span style="color:#E1E4E8;"> symbols:</span></span>
<span class="line"><span style="color:#E1E4E8;">        cur_output.append(token[start: end])</span></span>
<span class="line"><span style="color:#E1E4E8;">        start </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> end</span></span>
<span class="line"><span style="color:#E1E4E8;">        end </span><span style="color:#F97583;">=</span><span style="color:#79B8FF;"> len</span><span style="color:#E1E4E8;">(token)</span></span>
<span class="line"><span style="color:#F97583;">      else</span><span style="color:#E1E4E8;">:</span></span>
<span class="line"><span style="color:#E1E4E8;">        end </span><span style="color:#F97583;">-=</span><span style="color:#79B8FF;"> 1</span></span>
<span class="line"><span style="color:#F97583;">    if</span><span style="color:#E1E4E8;"> start </span><span style="color:#F97583;">&lt;</span><span style="color:#79B8FF;"> len</span><span style="color:#E1E4E8;">(token):</span></span>
<span class="line"><span style="color:#E1E4E8;">      cur_output.append(</span><span style="color:#9ECBFF;">&#39;[UNK]&#39;</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">    outputs.append(</span><span style="color:#9ECBFF;">&#39; &#39;</span><span style="color:#E1E4E8;">.join(cur_output))</span></span>
<span class="line"><span style="color:#F97583;">  return</span><span style="color:#E1E4E8;"> outputs</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">tokens </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> [</span><span style="color:#9ECBFF;">&#39;tallest_&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;fatter_&#39;</span><span style="color:#E1E4E8;">]</span></span>
<span class="line"><span style="color:#79B8FF;">print</span><span style="color:#E1E4E8;">(segment_BPE(tokens, symbols))</span></span>
<span class="line"><span style="color:#6A737D;"># [&#39;tall e s t _&#39;, &#39;fa t t er_&#39;]</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br><span class="line-number">43</span><br><span class="line-number">44</span><br><span class="line-number">45</span><br><span class="line-number">46</span><br><span class="line-number">47</span><br><span class="line-number">48</span><br><span class="line-number">49</span><br><span class="line-number">50</span><br><span class="line-number">51</span><br><span class="line-number">52</span><br><span class="line-number">53</span><br><span class="line-number">54</span><br><span class="line-number">55</span><br><span class="line-number">56</span><br><span class="line-number">57</span><br><span class="line-number">58</span><br><span class="line-number">59</span><br><span class="line-number">60</span><br><span class="line-number">61</span><br><span class="line-number">62</span><br><span class="line-number">63</span><br><span class="line-number">64</span><br><span class="line-number">65</span><br><span class="line-number">66</span><br><span class="line-number">67</span><br><span class="line-number">68</span><br><span class="line-number">69</span><br><span class="line-number">70</span><br><span class="line-number">71</span><br><span class="line-number">72</span><br><span class="line-number">73</span><br><span class="line-number">74</span><br><span class="line-number">75</span><br><span class="line-number">76</span><br><span class="line-number">77</span><br><span class="line-number">78</span><br><span class="line-number">79</span><br><span class="line-number">80</span><br></div></div><p>字节对编码执行训练数据集的统计分析，以发现词内的公共符号。作为一种<strong>贪心方法</strong>，字节对编码迭代地<strong>合并最频繁的连续符号对</strong>。</p><h3 id="transformers-的-bert" tabindex="-1">Transformers 的 BERT <a class="header-anchor" href="#transformers-的-bert" aria-label="Permalink to &quot;Transformers 的 BERT&quot;">​</a></h3><p><strong>词嵌入模型</strong>预训练后，输出可认为是一个<strong>矩阵</strong>，每一行都是一个表示预定义词表中词的向量。</p><p>但，这些词嵌入模型都是与<strong>上下文无关</strong>的。</p><p><strong>ELMo</strong>：从上下文无关到<strong>上下文敏感</strong>，ELMo<sup>Embeddings from Language Models</sup>为输入序列中的每个单词分配一个表示的函数。具体来说，ELMo 将来自预训练的<strong>双向长短期记忆</strong>网络的所有中间层表示<strong>组合为输出</strong>表示。然后，ELMo 的表示将作为附加特征添加到下游任务的现有监督模型中，例如通过将 ELMo 的表示和现有模型中词元的原始表示连结起来。</p><p>尽管，ELMo 显著改进了各种自然语言处理任务的解决方案，但每个解决方案仍然<strong>依赖于一个特定于任务架构</strong>。</p><p><strong>GPT</strong>：生成式预训练<sup>Generative Pre Training</sup>模型为上下文的敏感表示设计了通用<strong>任务无关</strong>模型。GPT 建立在 <code>Transformer</code> 解码器的基础上，预训练了一个用于表示文本序列的语言模型。当将 <code>GPT</code> 应用于下游任务时，语言模型的输出将被送到一个附加的线性输出层，以预测任务的标签。与 <code>ELMo</code> 冻结预训练模型的参数不同，<code>GPT</code> 在下游任务的监督学习过程中对预训练 <code>Transformer</code> 解码器中的所有参数进行<strong>微调</strong>。</p><p>然而，由于语言模型的<strong>自回归</strong>特性，GPT <strong>只能向前看</strong>（从左到右）。</p><p><strong>BERT</strong>：Google 2018 年提出基于 <code>Transformer</code> 架构预训练语言模型 BERT<sup>Bidirectional Encoder Representations from Transformers</sup>(双向编码模型)。BERT 推动了<strong>预训练模型</strong>的浪潮，后续模型如 <code>GPT-3、T5</code> 等均受其启发。当前趋势转向更大规模（如 <code>PaLM</code>）、多模态（如 <code>CLIP</code>）和高效训练（如 <code>LoRA</code>）。</p><p><strong>1. 核心思想</strong></p><ul><li>双向上下文建模：<br> 与传统单向语言模型（如 GPT）不同，BERT 通过 <strong>Masked Language Model (MLM)</strong> 同时利用左右两侧的上下文信息，更全面地理解词语含义。</li><li>预训练+微调：<br> 先在大规模语料上无监督预训练，再针对下游任务（如分类、问答）进行少量数据微调。</li></ul><p><strong>2. 关键技术创新</strong></p><ul><li>Transformer 编码器：<br> 完全基于 Transformer 的编码器堆叠（多层 Self-Attention + Feed-Forward），无需解码器。</li><li>两种预训练任务： <ul><li>MLM（掩码语言模型）：随机遮盖 15% 的单词，预测被遮盖的词。</li><li>NSP（下一句预测）：判断两个句子是否连续，增强句子间关系理解。</li></ul></li><li>输入表示：<br> 使用 <code>[CLS]</code>（分类标记）、<code>[SEP]</code>（分隔标记）和词/段/位置嵌入的三层编码。</li></ul><p><strong>3. 优缺点</strong></p><ul><li>优点： <ul><li>上下文敏感，解决多义词问题（如 &quot;bank&quot; 在金融或河岸的差异）。</li><li>通用性强，微调即可适配多种任务。</li></ul></li><li>缺点： <ul><li>计算资源消耗大（尤其是 Large 版本）。</li><li>对超长文本处理有限（最大长度通常为 512 token）。</li></ul></li></ul><p><strong>4. 代码示例（Hugging Face 库</strong></p><div class="language-python line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> transformers </span><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> BertTokenizer, BertModel</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 加载预训练模型和分词器</span></span>
<span class="line"><span style="color:#E1E4E8;">tokenizer </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> BertTokenizer.from_pretrained(</span><span style="color:#9ECBFF;">&#39;bert-base-uncased&#39;</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">model </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> BertModel.from_pretrained(</span><span style="color:#9ECBFF;">&#39;bert-base-uncased&#39;</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 输入处理</span></span>
<span class="line"><span style="color:#E1E4E8;">inputs </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> tokenizer(</span><span style="color:#9ECBFF;">&quot;Hello, BERT!&quot;</span><span style="color:#E1E4E8;">, </span><span style="color:#FFAB70;">return_tensors</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">&quot;pt&quot;</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">outputs </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> model(</span><span style="color:#F97583;">**</span><span style="color:#E1E4E8;">inputs)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 获取词嵌入或句子表示</span></span>
<span class="line"><span style="color:#E1E4E8;">last_hidden_states </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> outputs.last_hidden_state  </span><span style="color:#6A737D;"># 词级别嵌入</span></span>
<span class="line"><span style="color:#E1E4E8;">pooler_output </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> outputs.pooler_output          </span><span style="color:#6A737D;"># [CLS] 的句子表示</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br></div></div>`,42)]))}const b=n(o,[["render",e]]);export{u as __pageData,b as default};
