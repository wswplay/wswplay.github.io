import{_ as o,c as l,o as p,b as e,a as s,m as a}from"./chunks/framework.DUr976bL.js";const t="/assets/lstm.Bv_KUP4K.svg",r="/assets/gru.DWWBWCgF.svg",c="/assets/encoder-decoder.CTwnUkfh.svg",E="/assets/seq2seq.CvvUDatq.svg",i="/assets/seq2seq_train_output.C0kSpJDB.svg",v=JSON.parse('{"title":"现代循环神经网络","description":"","frontmatter":{"title":"现代循环神经网络","outline":"deep"},"headers":[{"level":2,"title":"门控机制","slug":"门控机制","link":"#门控机制","children":[{"level":3,"title":"长短期记忆网络(LSTM)","slug":"长短期记忆网络-lstm","link":"#长短期记忆网络-lstm","children":[]},{"level":3,"title":"门控循环单元(GRU)","slug":"门控循环单元-gru","link":"#门控循环单元-gru","children":[]},{"level":3,"title":"对比与现代改进方向","slug":"对比与现代改进方向","link":"#对比与现代改进方向","children":[]}]},{"level":2,"title":"编码器-解码器架构","slug":"编码器-解码器架构","link":"#编码器-解码器架构","children":[{"level":3,"title":"编码器","slug":"编码器","link":"#编码器","children":[]},{"level":3,"title":"解码器","slug":"解码器","link":"#解码器","children":[]},{"level":3,"title":"合并","slug":"合并","link":"#合并","children":[]}]},{"level":2,"title":"序列到序列学习(seq2seq)","slug":"序列到序列学习-seq2seq","link":"#序列到序列学习-seq2seq","children":[{"level":3,"title":"用 RNN 实现编码器","slug":"用-rnn-实现编码器","link":"#用-rnn-实现编码器","children":[]},{"level":3,"title":"实现解码器","slug":"实现解码器","link":"#实现解码器","children":[]},{"level":3,"title":"实现损失函数","slug":"实现损失函数","link":"#实现损失函数","children":[]},{"level":3,"title":"训练(train)","slug":"训练-train","link":"#训练-train","children":[]},{"level":3,"title":"预测(predict)","slug":"预测-predict","link":"#预测-predict","children":[]},{"level":3,"title":"对预测的评估","slug":"对预测的评估","link":"#对预测的评估","children":[]},{"level":3,"title":"All in","slug":"all-in","link":"#all-in","children":[]}]}],"relativePath":"aiart/deep-learning/rnn-modern.md","filePath":"aiart/deep-learning/rnn-modern.md"}'),y={name:"aiart/deep-learning/rnn-modern.md"},d={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},T={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.025ex"},xmlns:"http://www.w3.org/2000/svg",width:"0.781ex",height:"1.52ex",role:"img",focusable:"false",viewBox:"0 -661 345 672","aria-hidden":"true"},m={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},F={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.025ex"},xmlns:"http://www.w3.org/2000/svg",width:"0.781ex",height:"1.52ex",role:"img",focusable:"false",viewBox:"0 -661 345 672","aria-hidden":"true"},Q={tabindex:"0",class:"MathJax",jax:"SVG",display:"true",style:{direction:"ltr",display:"block","text-align":"center",margin:"1em 0",position:"relative"}},b={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-2.819ex"},xmlns:"http://www.w3.org/2000/svg",width:"37.511ex",height:"6.757ex",role:"img",focusable:"false",viewBox:"0 -1740.7 16580.1 2986.6","aria-hidden":"true"};function u(h,n,_,g,B,H){return p(),l("div",null,[n[15]||(n[15]=e('<h1 id="现代循环神经网络" tabindex="-1">现代循环神经网络 <a class="header-anchor" href="#现代循环神经网络" aria-label="Permalink to &quot;现代循环神经网络&quot;">​</a></h1><h2 id="门控机制" tabindex="-1">门控机制 <a class="header-anchor" href="#门控机制" aria-label="Permalink to &quot;门控机制&quot;">​</a></h2><h3 id="长短期记忆网络-lstm" tabindex="-1">长短期记忆网络(LSTM) <a class="header-anchor" href="#长短期记忆网络-lstm" aria-label="Permalink to &quot;长短期记忆网络(LSTM)&quot;">​</a></h3><p>LSTM：long short-term memory。</p><p>可以认为 LSTM 的核心创新就是引入了<code>Cell State</code>（细胞状态）这个&quot;全局记忆通道&quot;，而 GRU 可以看作是对 LSTM 的简化版本。</p><p><img src="'+t+'" alt="An Image"> 核心组件：</p><ul><li><strong>遗忘门(Forget Gate, f)</strong>：决定丢弃哪些信息。</li><li><strong>输入门(Input Gate, i)</strong>：决定更新哪些信息。</li><li><strong>输出门(Output Gate, o)</strong>：决定输出哪些信息。</li><li><strong>细胞状态(Cell State, C)</strong>：长期记忆通道。</li></ul><p>长短期记忆网络的隐藏层输出包括“隐状态”和“记忆元”。只有隐状态会传递到输出层，而记忆元完全属于内部信息。</p><h3 id="门控循环单元-gru" tabindex="-1">门控循环单元(GRU) <a class="header-anchor" href="#门控循环单元-gru" aria-label="Permalink to &quot;门控循环单元(GRU)&quot;">​</a></h3><p>GRU：Gated Recurrent Unit。</p><p>普通循环神经网络之相比，门控循环单元与支持<strong>隐状态门控</strong>，是简化版的 LSTM。</p><p>这意味着模型有专门的机制来确定应该<strong>何时更新隐状态，以及应该何时重置隐状态。这些机制是可学习的</strong>。例如，如果第一个词元非常重要，模型将学会在第一次观测之后不更新隐状态。同样，模型也可以学会跳过不相关的临时观测。</p><p>它通过引入可学习的&quot;门&quot;来控制信息流动，决定哪些信息应该被保留、哪些应该被遗忘。</p><p><img src="'+r+'" alt="An Image"> 核心组件：</p><ul><li><strong>重置门(Reset Gate, r)</strong>：控制前一时刻隐藏状态有多少信息需要被&quot;遗忘&quot;。</li><li><strong>更新门(Update Gate, z)</strong>：控制新状态中有多少来自前一状态，有多少来自当前计算的新候选状态。</li></ul><p>重置门打开时，门控循环单元包含基本循环神经网络；更新门打开时，门控循环单元可以跳过子序列。</p><h3 id="对比与现代改进方向" tabindex="-1">对比与现代改进方向 <a class="header-anchor" href="#对比与现代改进方向" aria-label="Permalink to &quot;对比与现代改进方向&quot;">​</a></h3><table tabindex="0"><thead><tr><th>机制</th><th>GRU</th><th>LSTM</th></tr></thead><tbody><tr><td>门数量</td><td>2 个(更新门、重置门)</td><td>3 个(输入门、遗忘门、输出门)</td></tr><tr><td>状态变量</td><td>只有隐藏状态 h</td><td>隐藏状态 h + 细胞状态 C</td></tr><tr><td>参数数量</td><td>较少(约少 1/3)</td><td>较多</td></tr><tr><td>计算复杂度</td><td>较低</td><td>较高</td></tr><tr><td>信息流</td><td>直接通过隐藏状态传递</td><td>通过细胞状态和隐藏状态双通道传递</td></tr><tr><td>性能表现</td><td>简单任务表现好，资源消耗低</td><td>复杂任务表现更稳定</td></tr></tbody></table><p>现代最新架构（如 <code>Transformer</code>）实际上吸收了这两种思想的优点：</p><ul><li>LSTM 思想：通过残差连接实现&quot;记忆高速公路&quot;</li><li>GRU 思想：简化门控机制（如 Transformer 中的 FFN 层）</li></ul><h2 id="编码器-解码器架构" tabindex="-1">编码器-解码器架构 <a class="header-anchor" href="#编码器-解码器架构" aria-label="Permalink to &quot;编码器-解码器架构&quot;">​</a></h2><p><img src="'+c+`" alt="An Image"> 编码器-解码器（<code>encoder-decoder</code>）架构两个主要组件：</p><ol><li><strong>编码器</strong>（encoder）：接受长度可变序列作为输入，并将其转换为具有固定形状编码状态。</li><li><strong>解码器</strong>（decoder）：将固定形状编码状态映射到长度可变序列。</li></ol><p>“编码器－解码器”架构可以将长度可变的序列作为输入和输出，因此适用于<strong>机器翻译</strong>等序列转换问题。</p><h3 id="编码器" tabindex="-1">编码器 <a class="header-anchor" href="#编码器" aria-label="Permalink to &quot;编码器&quot;">​</a></h3><div class="language-py line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">py</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> torch </span><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> nn</span></span>
<span class="line"><span style="color:#F97583;">class</span><span style="color:#B392F0;"> Encoder</span><span style="color:#E1E4E8;">(</span><span style="color:#B392F0;">nn</span><span style="color:#E1E4E8;">.</span><span style="color:#B392F0;">Module</span><span style="color:#E1E4E8;">):</span></span>
<span class="line"><span style="color:#F97583;">  def</span><span style="color:#79B8FF;"> __init__</span><span style="color:#E1E4E8;">(self, </span><span style="color:#F97583;">**</span><span style="color:#E1E4E8;">kwargs):</span></span>
<span class="line"><span style="color:#79B8FF;">    super</span><span style="color:#E1E4E8;">(Encoder, </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">).</span><span style="color:#79B8FF;">__init__</span><span style="color:#E1E4E8;">(</span><span style="color:#F97583;">**</span><span style="color:#E1E4E8;">kwargs)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583;">  def</span><span style="color:#B392F0;"> forward</span><span style="color:#E1E4E8;">(self, X, </span><span style="color:#F97583;">*</span><span style="color:#E1E4E8;">args):</span></span>
<span class="line"><span style="color:#F97583;">    raise</span><span style="color:#79B8FF;"> NotImplementedError</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><h3 id="解码器" tabindex="-1">解码器 <a class="header-anchor" href="#解码器" aria-label="Permalink to &quot;解码器&quot;">​</a></h3><div class="language-py line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">py</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#F97583;">class</span><span style="color:#B392F0;"> Decoder</span><span style="color:#E1E4E8;">(</span><span style="color:#B392F0;">nn</span><span style="color:#E1E4E8;">.</span><span style="color:#B392F0;">Module</span><span style="color:#E1E4E8;">):</span></span>
<span class="line"><span style="color:#F97583;">  def</span><span style="color:#79B8FF;"> __init__</span><span style="color:#E1E4E8;">(self, </span><span style="color:#F97583;">**</span><span style="color:#E1E4E8;">kwargs):</span></span>
<span class="line"><span style="color:#79B8FF;">    super</span><span style="color:#E1E4E8;">(Decoder, </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">).</span><span style="color:#79B8FF;">__init__</span><span style="color:#E1E4E8;">(</span><span style="color:#F97583;">**</span><span style="color:#E1E4E8;">kwargs)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583;">  def</span><span style="color:#B392F0;"> init_state</span><span style="color:#E1E4E8;">(self, enc_outputs, </span><span style="color:#F97583;">*</span><span style="color:#E1E4E8;">args):</span></span>
<span class="line"><span style="color:#F97583;">    raise</span><span style="color:#79B8FF;"> NotImplementedError</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583;">  def</span><span style="color:#B392F0;"> forward</span><span style="color:#E1E4E8;">(self, X, state):</span></span>
<span class="line"><span style="color:#F97583;">    raise</span><span style="color:#79B8FF;"> NotImplementedError</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><p>init_state 函数，用于将编码器的输出(<code>enc_outputs</code>)转换为编码后的状态。注意，此步骤可能需要额外的输入，例如：输入序列的有效长度。为了逐个地生成长度可变的词元序列，解码器在每个时间步都会将输入(例如：在前一时间步生成的词元)和编码后的状态映射成当前时间步的输出词元。</p><h3 id="合并" tabindex="-1">合并 <a class="header-anchor" href="#合并" aria-label="Permalink to &quot;合并&quot;">​</a></h3><div class="language-py line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">py</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#F97583;">class</span><span style="color:#B392F0;"> EncoderDecoder</span><span style="color:#E1E4E8;">(</span><span style="color:#B392F0;">nn</span><span style="color:#E1E4E8;">.</span><span style="color:#B392F0;">Module</span><span style="color:#E1E4E8;">):</span></span>
<span class="line"><span style="color:#F97583;">  def</span><span style="color:#79B8FF;"> __init__</span><span style="color:#E1E4E8;">(self, encoder, decoder, </span><span style="color:#F97583;">**</span><span style="color:#E1E4E8;">kwargs):</span></span>
<span class="line"><span style="color:#79B8FF;">    super</span><span style="color:#E1E4E8;">(EncoderDecoder, </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">).</span><span style="color:#79B8FF;">__init__</span><span style="color:#E1E4E8;">(</span><span style="color:#F97583;">**</span><span style="color:#E1E4E8;">kwargs)</span></span>
<span class="line"><span style="color:#79B8FF;">    self</span><span style="color:#E1E4E8;">.encoder </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> encoder</span></span>
<span class="line"><span style="color:#79B8FF;">    self</span><span style="color:#E1E4E8;">.decoder </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> decoder</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583;">  def</span><span style="color:#B392F0;"> forward</span><span style="color:#E1E4E8;">(self, enc_X, dec_X, </span><span style="color:#F97583;">*</span><span style="color:#E1E4E8;">args):</span></span>
<span class="line"><span style="color:#E1E4E8;">    enc_outputs </span><span style="color:#F97583;">=</span><span style="color:#79B8FF;"> self</span><span style="color:#E1E4E8;">.encoder(enc_X, </span><span style="color:#F97583;">*</span><span style="color:#E1E4E8;">args)</span></span>
<span class="line"><span style="color:#E1E4E8;">    dec_state </span><span style="color:#F97583;">=</span><span style="color:#79B8FF;"> self</span><span style="color:#E1E4E8;">.decoder.init_state(enc_outputs, </span><span style="color:#F97583;">*</span><span style="color:#E1E4E8;">args)</span></span>
<span class="line"><span style="color:#F97583;">    return</span><span style="color:#79B8FF;"> self</span><span style="color:#E1E4E8;">.decoder(dec_X, dec_state)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><p>“编码器-解码器”架构包含了一个编码器和一个解码器，并且还拥有可选的额外的参数。在前向传播中，编码器的输出用于生成编码状态，这个状态又被解码器作为其输入的一部分。</p><h2 id="序列到序列学习-seq2seq" tabindex="-1">序列到序列学习(seq2seq) <a class="header-anchor" href="#序列到序列学习-seq2seq" aria-label="Permalink to &quot;序列到序列学习(seq2seq)&quot;">​</a></h2><p>seq2seq：sequence to sequence。</p><p><img src="`+E+`" alt="An Image"> 上图是机器翻译中使用<strong>两个循环神经网络</strong>的编码器和解码器，进行序列到序列学习。特定的<code>“&lt;eos&gt;”</code>表示序列结束词元。一旦输出序列生成此词元，模型就会停止预测。</p><p>在循环神经网络解码器的初始化时间步，有两个特定的设计决定：首先，特定的<code>“&lt;bos&gt;”</code>表示序列开始词元，它是解码器的输入序列的第一个词元。其次，使用循环神经网络编码器最终的隐状态来初始化解码器的隐状态。</p><h3 id="用-rnn-实现编码器" tabindex="-1">用 RNN 实现编码器 <a class="header-anchor" href="#用-rnn-实现编码器" aria-label="Permalink to &quot;用 RNN 实现编码器&quot;">​</a></h3><div class="language-py line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">py</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> collections</span></span>
<span class="line"><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> math</span></span>
<span class="line"><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> torch</span></span>
<span class="line"><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> torch </span><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> nn</span></span>
<span class="line"><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> d2l </span><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> torch </span><span style="color:#F97583;">as</span><span style="color:#E1E4E8;"> d2l</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 用于序列到序列学习的循环神经网络编码器</span></span>
<span class="line"><span style="color:#F97583;">class</span><span style="color:#B392F0;"> Seq2SeqEncoder</span><span style="color:#E1E4E8;">(</span><span style="color:#B392F0;">d2l</span><span style="color:#E1E4E8;">.</span><span style="color:#B392F0;">Encoder</span><span style="color:#E1E4E8;">):</span></span>
<span class="line"><span style="color:#F97583;">  def</span><span style="color:#79B8FF;"> __init__</span><span style="color:#E1E4E8;">(self, vocab_size, embed_size, num_hiddens, num_layers, dropout</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">, </span><span style="color:#F97583;">**</span><span style="color:#E1E4E8;">kwargs):</span></span>
<span class="line"><span style="color:#79B8FF;">    super</span><span style="color:#E1E4E8;">(Seq2SeqEncoder, </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">).</span><span style="color:#79B8FF;">__init__</span><span style="color:#E1E4E8;">(</span><span style="color:#F97583;">**</span><span style="color:#E1E4E8;">kwargs)</span></span>
<span class="line"><span style="color:#6A737D;">    # 嵌入层</span></span>
<span class="line"><span style="color:#79B8FF;">    self</span><span style="color:#E1E4E8;">.embedding </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> nn.Embedding(vocab_size, embed_size)</span></span>
<span class="line"><span style="color:#79B8FF;">    self</span><span style="color:#E1E4E8;">.rnn </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> nn.GRU(embed_size, num_hiddens, num_layers, </span><span style="color:#FFAB70;">dropout</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">dropout)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583;">  def</span><span style="color:#B392F0;"> forward</span><span style="color:#E1E4E8;">(self, X, </span><span style="color:#F97583;">*</span><span style="color:#E1E4E8;">args):</span></span>
<span class="line"><span style="color:#6A737D;">    # 输出&#39;X&#39;的形状：(batch_size,num_steps,embed_size)</span></span>
<span class="line"><span style="color:#E1E4E8;">    X </span><span style="color:#F97583;">=</span><span style="color:#79B8FF;"> self</span><span style="color:#E1E4E8;">.embedding(X)</span></span>
<span class="line"><span style="color:#6A737D;">    # 在循环神经网络模型中，第一个轴对应于时间步</span></span>
<span class="line"><span style="color:#E1E4E8;">    X </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> X.permute(</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">2</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#6A737D;">    # 如果未提及状态，则默认为0</span></span>
<span class="line"><span style="color:#E1E4E8;">    output, state </span><span style="color:#F97583;">=</span><span style="color:#79B8FF;"> self</span><span style="color:#E1E4E8;">.rnn(X)</span></span>
<span class="line"><span style="color:#6A737D;">    # output的形状:(num_steps,batch_size,num_hiddens)</span></span>
<span class="line"><span style="color:#6A737D;">    # state的形状:(num_layers,batch_size,num_hiddens)</span></span>
<span class="line"><span style="color:#F97583;">    return</span><span style="color:#E1E4E8;"> output, state</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br></div></div><p><strong>嵌入层</strong>(<code>embedding layer</code>，<code>nn.Embedding</code>)，用以获得输入序列中每个词元的特征向量。</p>`,39)),s("p",null,[n[4]||(n[4]=a("嵌入层的权重是一个矩阵，其行数是输入词表大小(",-1)),n[5]||(n[5]=s("code",null,"vocab_size",-1)),n[6]||(n[6]=a(")，其列数是特征向量维度(",-1)),n[7]||(n[7]=s("code",null,"embed_size",-1)),n[8]||(n[8]=a(")。对于任意输入词元索引 ",-1)),s("mjx-container",d,[(p(),l("svg",T,n[0]||(n[0]=[s("g",{stroke:"currentColor",fill:"currentColor","stroke-width":"0",transform:"scale(1,-1)"},[s("g",{"data-mml-node":"math"},[s("g",{"data-mml-node":"mi"},[s("path",{"data-c":"1D456",d:"M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z",style:{"stroke-width":"3"}})])])],-1)]))),n[1]||(n[1]=s("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("mi",null,"i")])],-1))]),n[9]||(n[9]=a("，嵌入层获取权重矩阵第 ",-1)),s("mjx-container",m,[(p(),l("svg",F,n[2]||(n[2]=[s("g",{stroke:"currentColor",fill:"currentColor","stroke-width":"0",transform:"scale(1,-1)"},[s("g",{"data-mml-node":"math"},[s("g",{"data-mml-node":"mi"},[s("path",{"data-c":"1D456",d:"M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z",style:{"stroke-width":"3"}})])])],-1)]))),n[3]||(n[3]=s("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("mi",null,"i")])],-1))]),n[10]||(n[10]=a(" 行(",-1)),n[11]||(n[11]=s("code",null,"从0开始",-1)),n[12]||(n[12]=a(")以返回其特征向量。",-1))]),n[16]||(n[16]=e(`<div class="language-py line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">py</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#E1E4E8;">encoder </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> Seq2SeqEncoder(</span><span style="color:#FFAB70;">vocab_size</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">10</span><span style="color:#E1E4E8;">, </span><span style="color:#FFAB70;">embed_size</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">8</span><span style="color:#E1E4E8;">, </span><span style="color:#FFAB70;">num_hiddens</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">16</span><span style="color:#E1E4E8;">, </span><span style="color:#FFAB70;">num_layers</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">2</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">encoder.eval()</span></span>
<span class="line"><span style="color:#E1E4E8;">X </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torch.zeros((</span><span style="color:#79B8FF;">4</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">7</span><span style="color:#E1E4E8;">), </span><span style="color:#FFAB70;">dtype</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">torch.long)</span></span>
<span class="line"><span style="color:#E1E4E8;">output, state </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> encoder(X)</span></span>
<span class="line"><span style="color:#E1E4E8;">output.shape, state.shape</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># (torch.Size([7, 4, 16]), torch.Size([2, 4, 16]))</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><div class="tip custom-block"><p class="custom-block-title">参数制约</p><p><code>embed_size &lt;&lt; vocab_size</code>，如大小接近，或导致参数量爆炸、过拟合。比率：<code>1/10 到 1/100</code>。<br><code>num_hiddens &gt;= embed_size</code>，确保隐藏层有足够容量融合时序信息和输入特征。比率：<code>num_hiddens = 2~4 × embed_size</code>。</p></div><p>最后一层的隐状态的输出是一个张量（output 由编码器的循环层返回），其形状为（时间步数，批量大小，隐藏单元数）。最后一个时间步的多层隐状态的形状是（隐藏层的数量，批量大小，隐藏单元的数量）。如果使用长短期记忆网络，<code>state</code>中还将包含记忆单元信息。</p><h3 id="实现解码器" tabindex="-1">实现解码器 <a class="header-anchor" href="#实现解码器" aria-label="Permalink to &quot;实现解码器&quot;">​</a></h3><div class="language-py line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">py</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#6A737D;"># 用于序列到序列学习的循环神经网络解码器</span></span>
<span class="line"><span style="color:#F97583;">class</span><span style="color:#B392F0;"> Seq2SeqDecoder</span><span style="color:#E1E4E8;">(</span><span style="color:#B392F0;">d2l</span><span style="color:#E1E4E8;">.</span><span style="color:#B392F0;">Decoder</span><span style="color:#E1E4E8;">):</span></span>
<span class="line"><span style="color:#F97583;">  def</span><span style="color:#79B8FF;"> __init__</span><span style="color:#E1E4E8;">(self, vocab_size, embed_size, num_hiddens, num_layers, dropout</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">, </span><span style="color:#F97583;">**</span><span style="color:#E1E4E8;">kwargs):</span></span>
<span class="line"><span style="color:#79B8FF;">    super</span><span style="color:#E1E4E8;">(Seq2SeqDecoder, </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">).</span><span style="color:#79B8FF;">__init__</span><span style="color:#E1E4E8;">(</span><span style="color:#F97583;">**</span><span style="color:#E1E4E8;">kwargs)</span></span>
<span class="line"><span style="color:#79B8FF;">    self</span><span style="color:#E1E4E8;">.embedding </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> nn.Embedding(vocab_size, embed_size)</span></span>
<span class="line"><span style="color:#79B8FF;">    self</span><span style="color:#E1E4E8;">.rnn </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> nn.GRU(embed_size </span><span style="color:#F97583;">+</span><span style="color:#E1E4E8;"> num_hiddens, num_hiddens, num_layers, </span><span style="color:#FFAB70;">dropout</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">dropout)</span></span>
<span class="line"><span style="color:#79B8FF;">    self</span><span style="color:#E1E4E8;">.dense </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> nn.Linear(num_hiddens, vocab_size)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583;">  def</span><span style="color:#B392F0;"> init_state</span><span style="color:#E1E4E8;">(self, enc_outputs, </span><span style="color:#F97583;">*</span><span style="color:#E1E4E8;">args):</span></span>
<span class="line"><span style="color:#F97583;">    return</span><span style="color:#E1E4E8;"> enc_outputs[</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">]</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583;">  def</span><span style="color:#B392F0;"> forward</span><span style="color:#E1E4E8;">(self, X, state):</span></span>
<span class="line"><span style="color:#6A737D;">    # 输出&#39;X&#39;的形状：(batch_size,num_steps,embed_size)</span></span>
<span class="line"><span style="color:#E1E4E8;">    X </span><span style="color:#F97583;">=</span><span style="color:#79B8FF;"> self</span><span style="color:#E1E4E8;">.embedding(X).permute(</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">2</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#6A737D;">    # 广播context，使其具有与X相同的num_steps</span></span>
<span class="line"><span style="color:#E1E4E8;">    context </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> state[</span><span style="color:#F97583;">-</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">].repeat(X.shape[</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">], </span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">    X_and_context </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torch.cat((X, context), </span><span style="color:#79B8FF;">2</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">    output, state </span><span style="color:#F97583;">=</span><span style="color:#79B8FF;"> self</span><span style="color:#E1E4E8;">.rnn(X_and_context, state)</span></span>
<span class="line"><span style="color:#E1E4E8;">    output </span><span style="color:#F97583;">=</span><span style="color:#79B8FF;"> self</span><span style="color:#E1E4E8;">.dense(output).permute(</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">2</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#6A737D;">    # output的形状:(batch_size,num_steps,vocab_size)</span></span>
<span class="line"><span style="color:#6A737D;">    # state的形状:(num_layers,batch_size,num_hiddens)</span></span>
<span class="line"><span style="color:#F97583;">    return</span><span style="color:#E1E4E8;"> output, state</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br></div></div><p>实现解码器时，我们直接使用<strong>编码器最后一个时间步</strong>的隐状态来<strong>初始化解码器</strong>的隐状态，解码器的最后一层使用全连接层来变换隐状态。</p><div class="language-py line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">py</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#E1E4E8;">decoder </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> Seq2SeqDecoder(</span><span style="color:#FFAB70;">vocab_size</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">10</span><span style="color:#E1E4E8;">, </span><span style="color:#FFAB70;">embed_size</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">8</span><span style="color:#E1E4E8;">, </span><span style="color:#FFAB70;">num_hiddens</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">16</span><span style="color:#E1E4E8;">, </span><span style="color:#FFAB70;">num_layers</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">2</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">decoder.eval()</span></span>
<span class="line"><span style="color:#E1E4E8;">state </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> decoder.init_state(encoder(X))</span></span>
<span class="line"><span style="color:#E1E4E8;">output, state </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> decoder(X, state)</span></span>
<span class="line"><span style="color:#E1E4E8;">output.shape, state.shape</span></span>
<span class="line"><span style="color:#6A737D;"># (torch.Size([4, 7, 10]), torch.Size([2, 4, 16]))</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><h3 id="实现损失函数" tabindex="-1">实现损失函数 <a class="header-anchor" href="#实现损失函数" aria-label="Permalink to &quot;实现损失函数&quot;">​</a></h3><p>应该将<strong>填充词元</strong>的预测排除在损失函数的计算之外。通过零值化屏蔽不相关的项，以便后面任何不相关预测的计算都是与零的乘积，结果都等于零。</p><div class="language-py line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">py</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#6A737D;"># 在序列中屏蔽不相关</span></span>
<span class="line"><span style="color:#F97583;">def</span><span style="color:#B392F0;"> sequence_mask</span><span style="color:#E1E4E8;">(X, valid_len, value</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">):</span></span>
<span class="line"><span style="color:#E1E4E8;">  maxlen </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> X.size(</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">  mask </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torch.arange((maxlen), </span><span style="color:#FFAB70;">dtype</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">torch.float32, </span><span style="color:#FFAB70;">device</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">X.device)[</span><span style="color:#79B8FF;">None</span><span style="color:#E1E4E8;">, :] </span><span style="color:#F97583;">&lt;</span><span style="color:#E1E4E8;"> valid_len[:, </span><span style="color:#79B8FF;">None</span><span style="color:#E1E4E8;">]</span></span>
<span class="line"><span style="color:#E1E4E8;">  X[</span><span style="color:#F97583;">~</span><span style="color:#E1E4E8;">mask] </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> value</span></span>
<span class="line"><span style="color:#F97583;">  return</span><span style="color:#E1E4E8;"> X</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 带遮蔽的softmax交叉熵损失</span></span>
<span class="line"><span style="color:#F97583;">class</span><span style="color:#B392F0;"> MaskedSoftmaxCELoss</span><span style="color:#E1E4E8;">(</span><span style="color:#B392F0;">nn</span><span style="color:#E1E4E8;">.</span><span style="color:#B392F0;">CrossEntropyLoss</span><span style="color:#E1E4E8;">):</span></span>
<span class="line"><span style="color:#6A737D;">  # pred的形状：(batch_size,num_steps,vocab_size)</span></span>
<span class="line"><span style="color:#6A737D;">  # label的形状：(batch_size,num_steps)</span></span>
<span class="line"><span style="color:#6A737D;">  # valid_len的形状：(batch_size,)</span></span>
<span class="line"><span style="color:#F97583;">  def</span><span style="color:#B392F0;"> forward</span><span style="color:#E1E4E8;">(self, pred, label, valid_len):</span></span>
<span class="line"><span style="color:#E1E4E8;">    weights </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torch.ones_like(label)</span></span>
<span class="line"><span style="color:#E1E4E8;">    weights </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> sequence_mask(weights, valid_len)</span></span>
<span class="line"><span style="color:#79B8FF;">    self</span><span style="color:#E1E4E8;">.reduction</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">&#39;none&#39;</span></span>
<span class="line"><span style="color:#E1E4E8;">    unweighted_loss </span><span style="color:#F97583;">=</span><span style="color:#79B8FF;"> super</span><span style="color:#E1E4E8;">(MaskedSoftmaxCELoss, </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">).forward(pred.permute(</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">2</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">), label)</span></span>
<span class="line"><span style="color:#E1E4E8;">    weighted_loss </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> (unweighted_loss </span><span style="color:#F97583;">*</span><span style="color:#E1E4E8;"> weights).mean(</span><span style="color:#FFAB70;">dim</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#F97583;">    return</span><span style="color:#E1E4E8;"> weighted_loss</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br></div></div><p>通过扩展 <code>softmax</code> 交叉熵损失函数来遮蔽不相关的预测。 最初，所有预测词元的掩码都设置为 1。一旦给定了有效长度，与填充词元对应的掩码将被设置为 0。 最后，将所有词元的损失乘以掩码，以过滤掉损失中填充词元产生的不相关预测。</p><h3 id="训练-train" tabindex="-1">训练(train) <a class="header-anchor" href="#训练-train" aria-label="Permalink to &quot;训练(train)&quot;">​</a></h3><div class="language-py line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">py</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#6A737D;"># 训练序列到序列</span></span>
<span class="line"><span style="color:#F97583;">def</span><span style="color:#B392F0;"> train_seq2seq</span><span style="color:#E1E4E8;">(net, data_iter, lr, num_epochs, tgt_vocab, device):</span></span>
<span class="line"><span style="color:#F97583;">  def</span><span style="color:#B392F0;"> xavier_init_weights</span><span style="color:#E1E4E8;">(m):</span></span>
<span class="line"><span style="color:#F97583;">    if</span><span style="color:#79B8FF;"> type</span><span style="color:#E1E4E8;">(m) </span><span style="color:#F97583;">==</span><span style="color:#E1E4E8;"> nn.Linear:</span></span>
<span class="line"><span style="color:#E1E4E8;">      nn.init.xavier_uniform_(m.weight)</span></span>
<span class="line"><span style="color:#F97583;">    if</span><span style="color:#79B8FF;"> type</span><span style="color:#E1E4E8;">(m) </span><span style="color:#F97583;">==</span><span style="color:#E1E4E8;"> nn.</span><span style="color:#79B8FF;">GRU</span><span style="color:#E1E4E8;">:</span></span>
<span class="line"><span style="color:#F97583;">      for</span><span style="color:#E1E4E8;"> param </span><span style="color:#F97583;">in</span><span style="color:#E1E4E8;"> m._flat_weights_names:</span></span>
<span class="line"><span style="color:#F97583;">        if</span><span style="color:#9ECBFF;"> &quot;weight&quot;</span><span style="color:#F97583;"> in</span><span style="color:#E1E4E8;"> param:</span></span>
<span class="line"><span style="color:#E1E4E8;">          nn.init.xavier_uniform_(m._parameters[param])</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">  net.apply(xavier_init_weights)</span></span>
<span class="line"><span style="color:#E1E4E8;">  net.to(device)</span></span>
<span class="line"><span style="color:#E1E4E8;">  optimizer </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torch.optim.Adam(net.parameters(), </span><span style="color:#FFAB70;">lr</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">lr)</span></span>
<span class="line"><span style="color:#E1E4E8;">  loss </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> MaskedSoftmaxCELoss()</span></span>
<span class="line"><span style="color:#E1E4E8;">  net.train()</span></span>
<span class="line"><span style="color:#E1E4E8;">  animator </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> d2l.Animator(</span><span style="color:#FFAB70;">xlabel</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">&#39;epoch&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#FFAB70;">ylabel</span><span style="color:#F97583;">=</span><span style="color:#9ECBFF;">&#39;loss&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#FFAB70;">xlim</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">[</span><span style="color:#79B8FF;">10</span><span style="color:#E1E4E8;">, num_epochs])</span></span>
<span class="line"><span style="color:#F97583;">  for</span><span style="color:#E1E4E8;"> epoch </span><span style="color:#F97583;">in</span><span style="color:#79B8FF;"> range</span><span style="color:#E1E4E8;">(num_epochs):</span></span>
<span class="line"><span style="color:#E1E4E8;">    timer </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> d2l.Timer()</span></span>
<span class="line"><span style="color:#E1E4E8;">    metric </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> d2l.Accumulator(</span><span style="color:#79B8FF;">2</span><span style="color:#E1E4E8;">)  </span><span style="color:#6A737D;"># 训练损失总和，词元数量</span></span>
<span class="line"><span style="color:#F97583;">    for</span><span style="color:#E1E4E8;"> batch </span><span style="color:#F97583;">in</span><span style="color:#E1E4E8;"> data_iter:</span></span>
<span class="line"><span style="color:#E1E4E8;">      optimizer.zero_grad()</span></span>
<span class="line"><span style="color:#E1E4E8;">      X, X_valid_len, Y, Y_valid_len </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> [x.to(device) </span><span style="color:#F97583;">for</span><span style="color:#E1E4E8;"> x </span><span style="color:#F97583;">in</span><span style="color:#E1E4E8;"> batch]</span></span>
<span class="line"><span style="color:#E1E4E8;">      bos </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torch.tensor([tgt_vocab[</span><span style="color:#9ECBFF;">&#39;&lt;bos&gt;&#39;</span><span style="color:#E1E4E8;">]] </span><span style="color:#F97583;">*</span><span style="color:#E1E4E8;"> Y.shape[</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">], </span><span style="color:#FFAB70;">device</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">device).reshape(</span><span style="color:#F97583;">-</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">      dec_input </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torch.cat([bos, Y[:, :</span><span style="color:#F97583;">-</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">]], </span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">)  </span><span style="color:#6A737D;"># 强制教学</span></span>
<span class="line"><span style="color:#E1E4E8;">      Y_hat, _ </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> net(X, dec_input, X_valid_len)</span></span>
<span class="line"><span style="color:#E1E4E8;">      l </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> loss(Y_hat, Y, Y_valid_len)</span></span>
<span class="line"><span style="color:#E1E4E8;">      l.sum().backward()      </span><span style="color:#6A737D;"># 损失函数的标量进行“反向传播”</span></span>
<span class="line"><span style="color:#E1E4E8;">      d2l.grad_clipping(net, </span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">      num_tokens </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> Y_valid_len.sum()</span></span>
<span class="line"><span style="color:#E1E4E8;">      optimizer.step()</span></span>
<span class="line"><span style="color:#F97583;">      with</span><span style="color:#E1E4E8;"> torch.no_grad():</span></span>
<span class="line"><span style="color:#E1E4E8;">        metric.add(l.sum(), num_tokens)</span></span>
<span class="line"><span style="color:#F97583;">    if</span><span style="color:#E1E4E8;"> (epoch </span><span style="color:#F97583;">+</span><span style="color:#79B8FF;"> 1</span><span style="color:#E1E4E8;">) </span><span style="color:#F97583;">%</span><span style="color:#79B8FF;"> 10</span><span style="color:#F97583;"> ==</span><span style="color:#79B8FF;"> 0</span><span style="color:#E1E4E8;">:</span></span>
<span class="line"><span style="color:#E1E4E8;">      animator.add(epoch </span><span style="color:#F97583;">+</span><span style="color:#79B8FF;"> 1</span><span style="color:#E1E4E8;">, (metric[</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">] </span><span style="color:#F97583;">/</span><span style="color:#E1E4E8;"> metric[</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">],))</span></span>
<span class="line"><span style="color:#79B8FF;">  print</span><span style="color:#E1E4E8;">(</span><span style="color:#F97583;">f</span><span style="color:#9ECBFF;">&#39;loss </span><span style="color:#79B8FF;">{</span><span style="color:#E1E4E8;">metric[</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">] </span><span style="color:#F97583;">/</span><span style="color:#E1E4E8;"> metric[</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">]</span><span style="color:#F97583;">:.3f</span><span style="color:#79B8FF;">}</span><span style="color:#9ECBFF;">, </span><span style="color:#79B8FF;">{</span><span style="color:#E1E4E8;">metric[</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">] </span><span style="color:#F97583;">/</span><span style="color:#E1E4E8;"> timer.stop()</span><span style="color:#F97583;">:.1f</span><span style="color:#79B8FF;">}</span><span style="color:#9ECBFF;"> &#39;</span></span>
<span class="line"><span style="color:#F97583;">      f</span><span style="color:#9ECBFF;">&#39;tokens/sec on </span><span style="color:#79B8FF;">{str</span><span style="color:#E1E4E8;">(device)</span><span style="color:#79B8FF;">}</span><span style="color:#9ECBFF;">&#39;</span><span style="color:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br></div></div><p>循环训练过程中，特定序列开始词元（<code>“&lt;bos&gt;”</code>）和 原始输出序列（不包括序列结束词元<code>“&lt;eos&gt;”</code>） 拼接在一起作为解码器的输入。这被称为<strong>强制教学（teacher forcing）</strong>，因为原始的输出序列（词元的标签）被送入解码器。或者，将来自上一个时间步的预测得到的词元作为解码器的当前输入。</p><p><strong>强制教学</strong>方法将原始输出序列（<strong>而非预测结果</strong>）输入解码器。</p><div class="language-py line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">py</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#E1E4E8;">embed_size, num_hiddens, num_layers, dropout </span><span style="color:#F97583;">=</span><span style="color:#79B8FF;"> 32</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">32</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">2</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">0.1</span></span>
<span class="line"><span style="color:#E1E4E8;">batch_size, num_steps </span><span style="color:#F97583;">=</span><span style="color:#79B8FF;"> 64</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">10</span></span>
<span class="line"><span style="color:#E1E4E8;">lr, num_epochs, device </span><span style="color:#F97583;">=</span><span style="color:#79B8FF;"> 0.005</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">300</span><span style="color:#E1E4E8;">, d2l.try_gpu()</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">train_iter, src_vocab, tgt_vocab </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> d2l.load_data_nmt(batch_size, num_steps)</span></span>
<span class="line"><span style="color:#E1E4E8;">encoder </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> Seq2SeqEncoder(</span><span style="color:#79B8FF;">len</span><span style="color:#E1E4E8;">(src_vocab), embed_size, num_hiddens, num_layers, dropout)</span></span>
<span class="line"><span style="color:#E1E4E8;">decoder </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> Seq2SeqDecoder(</span><span style="color:#79B8FF;">len</span><span style="color:#E1E4E8;">(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)</span></span>
<span class="line"><span style="color:#E1E4E8;">net </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> d2l.EncoderDecoder(encoder, decoder)</span></span>
<span class="line"><span style="color:#E1E4E8;">train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)</span></span>
<span class="line"><span style="color:#6A737D;"># loss 0.019, 12745.1 tokens/sec on cuda:0</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><p><img src="`+i+`" alt="An Image"></p><h3 id="预测-predict" tabindex="-1">预测(predict) <a class="header-anchor" href="#预测-predict" aria-label="Permalink to &quot;预测(predict)&quot;">​</a></h3><div class="language-py line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">py</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#6A737D;"># 序列到序列模型的预测</span></span>
<span class="line"><span style="color:#F97583;">def</span><span style="color:#B392F0;"> predict_seq2seq</span><span style="color:#E1E4E8;">(net, src_sentence, src_vocab, tgt_vocab, num_steps,</span></span>
<span class="line"><span style="color:#E1E4E8;">                    device, save_attention_weights</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">False</span><span style="color:#E1E4E8;">):</span></span>
<span class="line"><span style="color:#6A737D;">  # 在预测时将net设置为评估模式</span></span>
<span class="line"><span style="color:#E1E4E8;">  net.eval()</span></span>
<span class="line"><span style="color:#E1E4E8;">  src_tokens </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> src_vocab[src_sentence.lower().split(</span><span style="color:#9ECBFF;">&#39; &#39;</span><span style="color:#E1E4E8;">)] </span><span style="color:#F97583;">+</span><span style="color:#E1E4E8;"> [</span></span>
<span class="line"><span style="color:#E1E4E8;">      src_vocab[</span><span style="color:#9ECBFF;">&#39;&lt;eos&gt;&#39;</span><span style="color:#E1E4E8;">]]</span></span>
<span class="line"><span style="color:#E1E4E8;">  enc_valid_len </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torch.tensor([</span><span style="color:#79B8FF;">len</span><span style="color:#E1E4E8;">(src_tokens)], </span><span style="color:#FFAB70;">device</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">device)</span></span>
<span class="line"><span style="color:#E1E4E8;">  src_tokens </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> d2l.truncate_pad(src_tokens, num_steps, src_vocab[</span><span style="color:#9ECBFF;">&#39;&lt;pad&gt;&#39;</span><span style="color:#E1E4E8;">])</span></span>
<span class="line"><span style="color:#6A737D;">  # 添加批量轴</span></span>
<span class="line"><span style="color:#E1E4E8;">  enc_X </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torch.unsqueeze(</span></span>
<span class="line"><span style="color:#E1E4E8;">      torch.tensor(src_tokens, </span><span style="color:#FFAB70;">dtype</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">torch.long, </span><span style="color:#FFAB70;">device</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">device), </span><span style="color:#FFAB70;">dim</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">  enc_outputs </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> net.encoder(enc_X, enc_valid_len)</span></span>
<span class="line"><span style="color:#E1E4E8;">  dec_state </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> net.decoder.init_state(enc_outputs, enc_valid_len)</span></span>
<span class="line"><span style="color:#6A737D;">  # 添加批量轴</span></span>
<span class="line"><span style="color:#E1E4E8;">  dec_X </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torch.unsqueeze(torch.tensor([tgt_vocab[</span><span style="color:#9ECBFF;">&#39;&lt;bos&gt;&#39;</span><span style="color:#E1E4E8;">]], </span><span style="color:#FFAB70;">dtype</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">torch.long, </span><span style="color:#FFAB70;">device</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">device), </span><span style="color:#FFAB70;">dim</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">  output_seq, attention_weight_seq </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> [], []</span></span>
<span class="line"><span style="color:#F97583;">  for</span><span style="color:#E1E4E8;"> _ </span><span style="color:#F97583;">in</span><span style="color:#79B8FF;"> range</span><span style="color:#E1E4E8;">(num_steps):</span></span>
<span class="line"><span style="color:#E1E4E8;">    Y, dec_state </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> net.decoder(dec_X, dec_state)</span></span>
<span class="line"><span style="color:#6A737D;">    # 我们使用具有预测最高可能性的词元，作为解码器在下一时间步的输入</span></span>
<span class="line"><span style="color:#E1E4E8;">    dec_X </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> Y.argmax(</span><span style="color:#FFAB70;">dim</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">2</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">    pred </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> dec_X.squeeze(</span><span style="color:#FFAB70;">dim</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">).type(torch.int32).item()</span></span>
<span class="line"><span style="color:#6A737D;">    # 保存注意力权重（稍后讨论）</span></span>
<span class="line"><span style="color:#F97583;">    if</span><span style="color:#E1E4E8;"> save_attention_weights:</span></span>
<span class="line"><span style="color:#E1E4E8;">      attention_weight_seq.append(net.decoder.attention_weights)</span></span>
<span class="line"><span style="color:#6A737D;">    # 一旦序列结束词元被预测，输出序列的生成就完成了</span></span>
<span class="line"><span style="color:#F97583;">    if</span><span style="color:#E1E4E8;"> pred </span><span style="color:#F97583;">==</span><span style="color:#E1E4E8;"> tgt_vocab[</span><span style="color:#9ECBFF;">&#39;&lt;eos&gt;&#39;</span><span style="color:#E1E4E8;">]:</span></span>
<span class="line"><span style="color:#F97583;">      break</span></span>
<span class="line"><span style="color:#E1E4E8;">    output_seq.append(pred)</span></span>
<span class="line"><span style="color:#F97583;">  return</span><span style="color:#9ECBFF;"> &#39; &#39;</span><span style="color:#E1E4E8;">.join(tgt_vocab.to_tokens(output_seq)), attention_weight_seq</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br></div></div><p>为了采用一个接着一个词元的方式预测输出序列， 每个解码器当前时间步的输入都将来自于前一时间步的预测词元。 与训练类似，序列开始词元（<code>“&lt;bos&gt;”</code>） 在初始时间步被输入到解码器中。输出序列的预测遇到序列结束词元（<code>“&lt;eos&gt;”</code>）时，预测就结束了。</p><h3 id="对预测的评估" tabindex="-1">对预测的评估 <a class="header-anchor" href="#对预测的评估" aria-label="Permalink to &quot;对预测的评估&quot;">​</a></h3><p>BLEU（<code>bilingual evaluation understudy</code>） 最先是用于评估机器翻译的结果。</p>`,22)),s("mjx-container",Q,[(p(),l("svg",b,n[13]||(n[13]=[e('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" style="stroke-width:3;"></path><path data-c="78" d="M201 0Q189 3 102 3Q26 3 17 0H11V46H25Q48 47 67 52T96 61T121 78T139 96T160 122T180 150L226 210L168 288Q159 301 149 315T133 336T122 351T113 363T107 370T100 376T94 379T88 381T80 383Q74 383 44 385H16V431H23Q59 429 126 429Q219 429 229 431H237V385Q201 381 201 369Q201 367 211 353T239 315T268 274L272 270L297 304Q329 345 329 358Q329 364 327 369T322 376T317 380T310 384L307 385H302V431H309Q324 428 408 428Q487 428 493 431H499V385H492Q443 385 411 368Q394 360 377 341T312 257L296 236L358 151Q424 61 429 57T446 50Q464 46 499 46H516V0H510H502Q494 1 482 1T457 2T432 2T414 3Q403 3 377 3T327 1L304 0H295V46H298Q309 46 320 51T331 63Q331 65 291 120L250 175Q249 174 219 133T185 88Q181 83 181 74Q181 63 188 55T206 46Q208 46 208 23V0H201Z" transform="translate(444,0)" style="stroke-width:3;"></path><path data-c="70" d="M36 -148H50Q89 -148 97 -134V-126Q97 -119 97 -107T97 -77T98 -38T98 6T98 55T98 106Q98 140 98 177T98 243T98 296T97 335T97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 61 434T98 436Q115 437 135 438T165 441T176 442H179V416L180 390L188 397Q247 441 326 441Q407 441 464 377T522 216Q522 115 457 52T310 -11Q242 -11 190 33L182 40V-45V-101Q182 -128 184 -134T195 -145Q216 -148 244 -148H260V-194H252L228 -193Q205 -192 178 -192T140 -191Q37 -191 28 -194H20V-148H36ZM424 218Q424 292 390 347T305 402Q234 402 182 337V98Q222 26 294 26Q345 26 384 80T424 218Z" transform="translate(972,0)" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(1528,0)"><path data-c="2061" d="" style="stroke-width:3;"></path></g><g data-mml-node="mrow" transform="translate(1694.7,0)"><g data-mml-node="mo" transform="translate(0 -0.5)"><path data-c="28" d="M701 -940Q701 -943 695 -949H664Q662 -947 636 -922T591 -879T537 -818T475 -737T412 -636T350 -511T295 -362T250 -186T221 17T209 251Q209 962 573 1361Q596 1386 616 1405T649 1437T664 1450H695Q701 1444 701 1441Q701 1436 681 1415T629 1356T557 1261T476 1118T400 927T340 675T308 359Q306 321 306 250Q306 -139 400 -430T690 -924Q701 -936 701 -940Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(736,0)"><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" style="stroke-width:3;"></path><path data-c="69" d="M69 609Q69 637 87 653T131 669Q154 667 171 652T188 609Q188 579 171 564T129 549Q104 549 87 564T69 609ZM247 0Q232 3 143 3Q132 3 106 3T56 1L34 0H26V46H42Q70 46 91 49Q100 53 102 60T104 102V205V293Q104 345 102 359T88 378Q74 385 41 385H30V408Q30 431 32 431L42 432Q52 433 70 434T106 436Q123 437 142 438T171 441T182 442H185V62Q190 52 197 50T232 46H255V0H247Z" transform="translate(833,0)" style="stroke-width:3;"></path><path data-c="6E" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(1111,0)" style="stroke-width:3;"></path></g><g data-mml-node="mrow" transform="translate(2569.7,0)"><g data-mml-node="mo" transform="translate(0 -0.5)"><path data-c="28" d="M701 -940Q701 -943 695 -949H664Q662 -947 636 -922T591 -879T537 -818T475 -737T412 -636T350 -511T295 -362T250 -186T221 17T209 251Q209 962 573 1361Q596 1386 616 1405T649 1437T664 1450H695Q701 1444 701 1441Q701 1436 681 1415T629 1356T557 1261T476 1118T400 927T340 675T308 359Q306 321 306 250Q306 -139 400 -430T690 -924Q701 -936 701 -940Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(736,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(1236,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(1680.7,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(2402.9,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mfrac" transform="translate(3403.1,0)"><g data-mml-node="mtext" transform="translate(220,676)"><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z" style="stroke-width:3;"></path><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" transform="translate(278,0)" style="stroke-width:3;"></path><path data-c="6E" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(722,0)" style="stroke-width:3;"></path><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z" transform="translate(1278,0)" style="stroke-width:3;"></path><path data-c="61" d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z" transform="translate(1556,0)" style="stroke-width:3;"></path><path data-c="62" d="M307 -11Q234 -11 168 55L158 37Q156 34 153 28T147 17T143 10L138 1L118 0H98V298Q98 599 97 603Q94 622 83 628T38 637H20V660Q20 683 22 683L32 684Q42 685 61 686T98 688Q115 689 135 690T165 693T176 694H179V543Q179 391 180 391L183 394Q186 397 192 401T207 411T228 421T254 431T286 439T323 442Q401 442 461 379T522 216Q522 115 458 52T307 -11ZM182 98Q182 97 187 90T196 79T206 67T218 55T233 44T250 35T271 29T295 26Q330 26 363 46T412 113Q424 148 424 212Q424 287 412 323Q385 405 300 405Q270 405 239 390T188 347L182 339V98Z" transform="translate(2056,0)" style="stroke-width:3;"></path><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" transform="translate(2612,0)" style="stroke-width:3;"></path><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z" transform="translate(3056,0)" style="stroke-width:3;"></path></g><g data-mml-node="mtext" transform="translate(274,-686)"><path data-c="6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z" style="stroke-width:3;"></path><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" transform="translate(278,0)" style="stroke-width:3;"></path><path data-c="6E" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(722,0)" style="stroke-width:3;"></path><path data-c="70" d="M36 -148H50Q89 -148 97 -134V-126Q97 -119 97 -107T97 -77T98 -38T98 6T98 55T98 106Q98 140 98 177T98 243T98 296T97 335T97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 61 434T98 436Q115 437 135 438T165 441T176 442H179V416L180 390L188 397Q247 441 326 441Q407 441 464 377T522 216Q522 115 457 52T310 -11Q242 -11 190 33L182 40V-45V-101Q182 -128 184 -134T195 -145Q216 -148 244 -148H260V-194H252L228 -193Q205 -192 178 -192T140 -191Q37 -191 28 -194H20V-148H36ZM424 218Q424 292 390 347T305 402Q234 402 182 337V98Q222 26 294 26Q345 26 384 80T424 218Z" transform="translate(1278,0)" style="stroke-width:3;"></path><path data-c="72" d="M36 46H50Q89 46 97 60V68Q97 77 97 91T98 122T98 161T98 203Q98 234 98 269T98 328L97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 60 434T96 436Q112 437 131 438T160 441T171 442H174V373Q213 441 271 441H277Q322 441 343 419T364 373Q364 352 351 337T313 322Q288 322 276 338T263 372Q263 381 265 388T270 400T273 405Q271 407 250 401Q234 393 226 386Q179 341 179 207V154Q179 141 179 127T179 101T180 81T180 66V61Q181 59 183 57T188 54T193 51T200 49T207 48T216 47T225 47T235 46T245 46H276V0H267Q249 3 140 3Q37 3 28 0H20V46H36Z" transform="translate(1834,0)" style="stroke-width:3;"></path><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z" transform="translate(2226,0)" style="stroke-width:3;"></path><path data-c="64" d="M376 495Q376 511 376 535T377 568Q377 613 367 624T316 637H298V660Q298 683 300 683L310 684Q320 685 339 686T376 688Q393 689 413 690T443 693T454 694H457V390Q457 84 458 81Q461 61 472 55T517 46H535V0Q533 0 459 -5T380 -11H373V44L365 37Q307 -11 235 -11Q158 -11 96 50T34 215Q34 315 97 378T244 442Q319 442 376 393V495ZM373 342Q328 405 260 405Q211 405 173 369Q146 341 139 305T131 211Q131 155 138 120T173 59Q203 26 251 26Q322 26 373 103V342Z" transform="translate(2670,0)" style="stroke-width:3;"></path></g><rect width="3534" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(7177.1,0) translate(0 -0.5)"><path data-c="29" d="M34 1438Q34 1446 37 1448T50 1450H56H71Q73 1448 99 1423T144 1380T198 1319T260 1238T323 1137T385 1013T440 864T485 688T514 485T526 251Q526 134 519 53Q472 -519 162 -860Q139 -885 119 -904T86 -936T71 -949H56Q43 -949 39 -947T34 -937Q88 -883 140 -813Q428 -430 428 251Q428 453 402 628T338 922T245 1146T145 1309T46 1425Q44 1427 42 1429T39 1433T36 1436L34 1438Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(10482.8,0) translate(0 -0.5)"><path data-c="29" d="M34 1438Q34 1446 37 1448T50 1450H56H71Q73 1448 99 1423T144 1380T198 1319T260 1238T323 1137T385 1013T440 864T485 688T514 485T526 251Q526 134 519 53Q472 -519 162 -860Q139 -885 119 -904T86 -936T71 -949H56Q43 -949 39 -947T34 -937Q88 -883 140 -813Q428 -430 428 251Q428 453 402 628T338 922T245 1146T145 1309T46 1425Q44 1427 42 1429T39 1433T36 1436L34 1438Z" style="stroke-width:3;"></path></g></g><g data-mml-node="munderover" transform="translate(13080.1,0)"><g data-mml-node="mo" transform="translate(25,0)"><path data-c="220F" d="M220 812Q220 813 218 819T214 829T208 840T199 853T185 866T166 878T140 887T107 893T66 896H56V950H1221V896H1211Q1080 896 1058 812V-311Q1076 -396 1211 -396H1221V-450H725V-396H735Q864 -396 888 -314Q889 -312 889 -311V896H388V292L389 -311Q405 -396 542 -396H552V-450H56V-396H66Q195 -396 219 -314Q220 -312 220 -311V812Z" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" transform="translate(0,-1087.9) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(600,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(1378,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" style="stroke-width:3;"></path></g></g><g data-mml-node="TeXAtom" transform="translate(479.8,1150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z" style="stroke-width:3;"></path></g></g></g><g data-mml-node="msubsup" transform="translate(14574.7,0)"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" transform="translate(536,530.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" style="stroke-width:3;"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(500,0)"><g data-mml-node="mo"><path data-c="2F" d="M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z" style="stroke-width:3;"></path></g></g><g data-mml-node="msup" transform="translate(1000,0)"><g data-mml-node="mn"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(533,363) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g></g></g><g data-mml-node="mi" transform="translate(536,-138.9) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g></g></g></g>',1)]))),n[14]||(n[14]=s("mjx-assistive-mml",{unselectable:"on",display:"block",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",overflow:"hidden",width:"100%"}},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},[s("mi",null,"exp"),s("mo",{"data-mjx-texclass":"NONE"},"⁡"),s("mrow",{"data-mjx-texclass":"INNER"},[s("mo",{"data-mjx-texclass":"OPEN"},"("),s("mo",{"data-mjx-texclass":"OP",movablelimits:"true"},"min"),s("mrow",{"data-mjx-texclass":"INNER"},[s("mo",{"data-mjx-texclass":"OPEN"},"("),s("mn",null,"0"),s("mo",null,","),s("mn",null,"1"),s("mo",null,"−"),s("mfrac",null,[s("mtext",null,"lenlabel"),s("mtext",null,"lenpred")]),s("mo",{"data-mjx-texclass":"CLOSE"},")")]),s("mo",{"data-mjx-texclass":"CLOSE"},")")]),s("munderover",null,[s("mo",{"data-mjx-texclass":"OP"},"∏"),s("mrow",{"data-mjx-texclass":"ORD"},[s("mi",null,"n"),s("mo",null,"="),s("mn",null,"1")]),s("mrow",{"data-mjx-texclass":"ORD"},[s("mi",null,"k")])]),s("msubsup",null,[s("mi",null,"p"),s("mi",null,"n"),s("mrow",{"data-mjx-texclass":"ORD"},[s("mn",null,"1"),s("mrow",{"data-mjx-texclass":"ORD"},[s("mo",null,"/")]),s("msup",null,[s("mn",null,"2"),s("mi",null,"n")])])])])],-1))]),n[17]||(n[17]=e(`<div class="language-py line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">py</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#6A737D;"># 计算BLEU</span></span>
<span class="line"><span style="color:#F97583;">def</span><span style="color:#B392F0;"> bleu</span><span style="color:#E1E4E8;">(pred_seq, label_seq, k):</span></span>
<span class="line"><span style="color:#E1E4E8;">  pred_tokens, label_tokens </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> pred_seq.split(</span><span style="color:#9ECBFF;">&#39; &#39;</span><span style="color:#E1E4E8;">), label_seq.split(</span><span style="color:#9ECBFF;">&#39; &#39;</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">  len_pred, len_label </span><span style="color:#F97583;">=</span><span style="color:#79B8FF;"> len</span><span style="color:#E1E4E8;">(pred_tokens), </span><span style="color:#79B8FF;">len</span><span style="color:#E1E4E8;">(label_tokens)</span></span>
<span class="line"><span style="color:#E1E4E8;">  score </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> math.exp(</span><span style="color:#79B8FF;">min</span><span style="color:#E1E4E8;">(</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">1</span><span style="color:#F97583;"> -</span><span style="color:#E1E4E8;"> len_label </span><span style="color:#F97583;">/</span><span style="color:#E1E4E8;"> len_pred))</span></span>
<span class="line"><span style="color:#F97583;">  for</span><span style="color:#E1E4E8;"> n </span><span style="color:#F97583;">in</span><span style="color:#79B8FF;"> range</span><span style="color:#E1E4E8;">(</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">, k </span><span style="color:#F97583;">+</span><span style="color:#79B8FF;"> 1</span><span style="color:#E1E4E8;">):</span></span>
<span class="line"><span style="color:#E1E4E8;">    num_matches, label_subs </span><span style="color:#F97583;">=</span><span style="color:#79B8FF;"> 0</span><span style="color:#E1E4E8;">, collections.defaultdict(</span><span style="color:#79B8FF;">int</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#F97583;">    for</span><span style="color:#E1E4E8;"> i </span><span style="color:#F97583;">in</span><span style="color:#79B8FF;"> range</span><span style="color:#E1E4E8;">(len_label </span><span style="color:#F97583;">-</span><span style="color:#E1E4E8;"> n </span><span style="color:#F97583;">+</span><span style="color:#79B8FF;"> 1</span><span style="color:#E1E4E8;">):</span></span>
<span class="line"><span style="color:#E1E4E8;">      label_subs[</span><span style="color:#9ECBFF;">&#39; &#39;</span><span style="color:#E1E4E8;">.join(label_tokens[i: i </span><span style="color:#F97583;">+</span><span style="color:#E1E4E8;"> n])] </span><span style="color:#F97583;">+=</span><span style="color:#79B8FF;"> 1</span></span>
<span class="line"><span style="color:#F97583;">    for</span><span style="color:#E1E4E8;"> i </span><span style="color:#F97583;">in</span><span style="color:#79B8FF;"> range</span><span style="color:#E1E4E8;">(len_pred </span><span style="color:#F97583;">-</span><span style="color:#E1E4E8;"> n </span><span style="color:#F97583;">+</span><span style="color:#79B8FF;"> 1</span><span style="color:#E1E4E8;">):</span></span>
<span class="line"><span style="color:#F97583;">      if</span><span style="color:#E1E4E8;"> label_subs[</span><span style="color:#9ECBFF;">&#39; &#39;</span><span style="color:#E1E4E8;">.join(pred_tokens[i: i </span><span style="color:#F97583;">+</span><span style="color:#E1E4E8;"> n])] </span><span style="color:#F97583;">&gt;</span><span style="color:#79B8FF;"> 0</span><span style="color:#E1E4E8;">:</span></span>
<span class="line"><span style="color:#E1E4E8;">        num_matches </span><span style="color:#F97583;">+=</span><span style="color:#79B8FF;"> 1</span></span>
<span class="line"><span style="color:#E1E4E8;">        label_subs[</span><span style="color:#9ECBFF;">&#39; &#39;</span><span style="color:#E1E4E8;">.join(pred_tokens[i: i </span><span style="color:#F97583;">+</span><span style="color:#E1E4E8;"> n])] </span><span style="color:#F97583;">-=</span><span style="color:#79B8FF;"> 1</span></span>
<span class="line"><span style="color:#E1E4E8;">    score </span><span style="color:#F97583;">*=</span><span style="color:#E1E4E8;"> math.pow(num_matches </span><span style="color:#F97583;">/</span><span style="color:#E1E4E8;"> (len_pred </span><span style="color:#F97583;">-</span><span style="color:#E1E4E8;"> n </span><span style="color:#F97583;">+</span><span style="color:#79B8FF;"> 1</span><span style="color:#E1E4E8;">), math.pow(</span><span style="color:#79B8FF;">0.5</span><span style="color:#E1E4E8;">, n))</span></span>
<span class="line"><span style="color:#F97583;">  return</span><span style="color:#E1E4E8;"> score</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br></div></div><h3 id="all-in" tabindex="-1">All in <a class="header-anchor" href="#all-in" aria-label="Permalink to &quot;All in&quot;">​</a></h3><div class="language-py line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">py</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#E1E4E8;">engs </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> [</span><span style="color:#9ECBFF;">&#39;go .&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&quot;i lost .&quot;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;he</span><span style="color:#79B8FF;">\\&#39;</span><span style="color:#9ECBFF;">s calm .&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;i</span><span style="color:#79B8FF;">\\&#39;</span><span style="color:#9ECBFF;">m home .&#39;</span><span style="color:#E1E4E8;">]</span></span>
<span class="line"><span style="color:#E1E4E8;">fras </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> [</span><span style="color:#9ECBFF;">&#39;va !&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;j</span><span style="color:#79B8FF;">\\&#39;</span><span style="color:#9ECBFF;">ai perdu .&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;il est calme .&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39;je suis chez moi .&#39;</span><span style="color:#E1E4E8;">]</span></span>
<span class="line"><span style="color:#F97583;">for</span><span style="color:#E1E4E8;"> eng, fra </span><span style="color:#F97583;">in</span><span style="color:#79B8FF;"> zip</span><span style="color:#E1E4E8;">(engs, fras):</span></span>
<span class="line"><span style="color:#E1E4E8;">  translation, attention_weight_seq </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> predict_seq2seq(</span></span>
<span class="line"><span style="color:#E1E4E8;">      net, eng, src_vocab, tgt_vocab, num_steps, device)</span></span>
<span class="line"><span style="color:#79B8FF;">  print</span><span style="color:#E1E4E8;">(</span><span style="color:#F97583;">f</span><span style="color:#9ECBFF;">&#39;</span><span style="color:#79B8FF;">{</span><span style="color:#E1E4E8;">eng</span><span style="color:#79B8FF;">}</span><span style="color:#9ECBFF;"> =&gt; </span><span style="color:#79B8FF;">{</span><span style="color:#E1E4E8;">translation</span><span style="color:#79B8FF;">}</span><span style="color:#9ECBFF;">, bleu </span><span style="color:#79B8FF;">{</span><span style="color:#E1E4E8;">bleu(translation, fra, </span><span style="color:#FFAB70;">k</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">2</span><span style="color:#E1E4E8;">)</span><span style="color:#F97583;">:.3f</span><span style="color:#79B8FF;">}</span><span style="color:#9ECBFF;">&#39;</span><span style="color:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div>`,3))])}const V=o(y,[["render",u]]);export{v as __pageData,V as default};
