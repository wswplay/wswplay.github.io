import{_ as s,c as a,o as e,b as r}from"./chunks/framework._eNwL97Z.js";const E=JSON.parse('{"title":"自然语言处理、NLP","description":"","frontmatter":{"title":"自然语言处理、NLP","outline":"deep"},"headers":[{"level":2,"title":"预训练(pre-training)","slug":"预训练-pre-training","link":"#预训练-pre-training","children":[{"level":3,"title":"子词嵌入","slug":"子词嵌入","link":"#子词嵌入","children":[]},{"level":3,"title":"字节对编码(BPE)","slug":"字节对编码-bpe","link":"#字节对编码-bpe","children":[]}]}],"relativePath":"aiart/deep-learning/nlp.md","filePath":"aiart/deep-learning/nlp.md"}'),o={name:"aiart/deep-learning/nlp.md"};function l(t,n,p,i,c,u){return e(),a("div",null,n[0]||(n[0]=[r(`<h1 id="自然语言处理-nlp" tabindex="-1">自然语言处理(NLP) <a class="header-anchor" href="#自然语言处理-nlp" aria-label="Permalink to &quot;自然语言处理(NLP)&quot;">​</a></h1><p><strong>NLP</strong>：Natural Language Processing，是指研究使用自然语言的<strong>计算机和人类</strong>之间的<strong>交互</strong>。</p><h2 id="预训练-pre-training" tabindex="-1">预训练(pre-training) <a class="header-anchor" href="#预训练-pre-training" aria-label="Permalink to &quot;预训练(pre-training)&quot;">​</a></h2><p><strong>自监督学习</strong><sup>self-supervised learning</sup>已被广泛用于<strong>预训练</strong>文本表示，例如通过使用周围文本的其它部分来预测文本的隐藏部分。</p><h3 id="子词嵌入" tabindex="-1">子词嵌入 <a class="header-anchor" href="#子词嵌入" aria-label="Permalink to &quot;子词嵌入&quot;">​</a></h3><p>自然语言是用来表达人脑思维的复杂系统，<strong>词</strong>是意义的基本单元。</p><ul><li><strong>词向量</strong>：用于表示<strong>单词意义</strong>的向量，并且还可以被认为是单词<strong>特征向量或表示</strong>。</li><li><strong>词嵌入</strong>：将<strong>单词映射到实向量</strong>的技术。</li></ul><p><strong>子词嵌入</strong><sup>Subword Embedding</sup>是一种在自然语言处理（NLP）中用于表示单词的技术，它通过<strong>将单词拆分为更小的单元</strong>（子词）来解决传统词嵌入的局限性（如未登录词问题）。</p><ul><li><strong>未登录词（OOV）问题</strong>：传统词嵌入（如 Word2Vec）无法处理词汇表外的单词。</li><li><strong>形态学相似性</strong>：子词能捕捉单词间的结构关系（如&quot;running&quot; → &quot;run&quot; + &quot;ing&quot;）。</li><li><strong>多语言支持</strong>：共享子词可跨语言建模（如拉丁语系的共同词根）。</li></ul><p><strong>代码示例（Hugging Face 库）</strong></p><div class="language-python line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> transformers </span><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> AutoTokenizer</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 加载BERT的WordPiece分词器</span></span>
<span class="line"><span style="color:#E1E4E8;">tokenizer </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> AutoTokenizer.from_pretrained(</span><span style="color:#9ECBFF;">&quot;bert-base-uncased&quot;</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">tokens </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> tokenizer.tokenize(</span><span style="color:#9ECBFF;">&quot;unhappiness&quot;</span><span style="color:#E1E4E8;">)  </span><span style="color:#6A737D;"># 输出：[&#39;un&#39;, &#39;##happy&#39;, &#39;##ness&#39;]</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 加载BPE分词器（GPT-2）</span></span>
<span class="line"><span style="color:#E1E4E8;">tokenizer </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> AutoTokenizer.from_pretrained(</span><span style="color:#9ECBFF;">&quot;gpt2&quot;</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">tokens </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> tokenizer.tokenize(</span><span style="color:#9ECBFF;">&quot;unhappiness&quot;</span><span style="color:#E1E4E8;">)  </span><span style="color:#6A737D;"># 输出：[&#39;un&#39;, &#39;happiness&#39;]</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><h3 id="字节对编码-bpe" tabindex="-1">字节对编码(BPE) <a class="header-anchor" href="#字节对编码-bpe" aria-label="Permalink to &quot;字节对编码(BPE)&quot;">​</a></h3><p><strong>BPE</strong>：Byte Pair Encoding。</p>`,13)]))}const d=s(o,[["render",l]]);export{E as __pageData,d as default};
