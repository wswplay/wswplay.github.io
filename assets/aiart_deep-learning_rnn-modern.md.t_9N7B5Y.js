import{_ as e,c as r,o as a,b as l}from"./chunks/framework._eNwL97Z.js";const d="/assets/lstm.Bv_KUP4K.svg",n="/assets/gru.DWWBWCgF.svg",m=JSON.parse('{"title":"现代循环神经网络","description":"","frontmatter":{"title":"现代循环神经网络","outline":"deep"},"headers":[{"level":2,"title":"门控机制","slug":"门控机制","link":"#门控机制","children":[{"level":3,"title":"长短期记忆网络(LSTM)","slug":"长短期记忆网络-lstm","link":"#长短期记忆网络-lstm","children":[]},{"level":3,"title":"门控循环单元(GRU)","slug":"门控循环单元-gru","link":"#门控循环单元-gru","children":[]},{"level":3,"title":"对比与现代改进方向","slug":"对比与现代改进方向","link":"#对比与现代改进方向","children":[]}]}],"relativePath":"aiart/deep-learning/rnn-modern.md","filePath":"aiart/deep-learning/rnn-modern.md"}'),o={name:"aiart/deep-learning/rnn-modern.md"};function i(s,t,u,h,p,g){return a(),r("div",null,t[0]||(t[0]=[l('<h1 id="现代循环神经网络" tabindex="-1">现代循环神经网络 <a class="header-anchor" href="#现代循环神经网络" aria-label="Permalink to &quot;现代循环神经网络&quot;">​</a></h1><h2 id="门控机制" tabindex="-1">门控机制 <a class="header-anchor" href="#门控机制" aria-label="Permalink to &quot;门控机制&quot;">​</a></h2><h3 id="长短期记忆网络-lstm" tabindex="-1">长短期记忆网络(LSTM) <a class="header-anchor" href="#长短期记忆网络-lstm" aria-label="Permalink to &quot;长短期记忆网络(LSTM)&quot;">​</a></h3><p>LSTM：long short-term memory。</p><p>可以认为 LSTM 的核心创新就是引入了<code>Cell State</code>（细胞状态）这个&quot;全局记忆通道&quot;，而 GRU 可以看作是对 LSTM 的简化版本。</p><p><img src="'+d+'" alt="An Image"> 核心组件：</p><ul><li><strong>遗忘门(Forget Gate, f)</strong>：决定丢弃哪些信息。</li><li><strong>输入门(Input Gate, i)</strong>：决定更新哪些信息。</li><li><strong>输出门(Output Gate, o)</strong>：决定输出哪些信息。</li><li><strong>细胞状态(Cell State, C)</strong>：长期记忆通道。</li></ul><p>长短期记忆网络的隐藏层输出包括“隐状态”和“记忆元”。只有隐状态会传递到输出层，而记忆元完全属于内部信息。</p><h3 id="门控循环单元-gru" tabindex="-1">门控循环单元(GRU) <a class="header-anchor" href="#门控循环单元-gru" aria-label="Permalink to &quot;门控循环单元(GRU)&quot;">​</a></h3><p>GRU：Gated Recurrent Unit。</p><p>普通循环神经网络之相比，门控循环单元与支持<strong>隐状态门控</strong>，是简化版的 LSTM。</p><p>这意味着模型有专门的机制来确定应该<strong>何时更新隐状态，以及应该何时重置隐状态。这些机制是可学习的</strong>。例如，如果第一个词元非常重要，模型将学会在第一次观测之后不更新隐状态。同样，模型也可以学会跳过不相关的临时观测。</p><p>它通过引入可学习的&quot;门&quot;来控制信息流动，决定哪些信息应该被保留、哪些应该被遗忘。</p><p><img src="'+n+'" alt="An Image"> 核心组件：</p><ul><li><strong>重置门(Reset Gate, r)</strong>：控制前一时刻隐藏状态有多少信息需要被&quot;遗忘&quot;。</li><li><strong>更新门(Update Gate, z)</strong>：控制新状态中有多少来自前一状态，有多少来自当前计算的新候选状态。</li></ul><p>重置门打开时，门控循环单元包含基本循环神经网络；更新门打开时，门控循环单元可以跳过子序列。</p><h3 id="对比与现代改进方向" tabindex="-1">对比与现代改进方向 <a class="header-anchor" href="#对比与现代改进方向" aria-label="Permalink to &quot;对比与现代改进方向&quot;">​</a></h3><table tabindex="0"><thead><tr><th>机制</th><th>GRU</th><th>LSTM</th></tr></thead><tbody><tr><td>门数量</td><td>2 个(更新门、重置门)</td><td>3 个(输入门、遗忘门、输出门)</td></tr><tr><td>状态变量</td><td>只有隐藏状态 h</td><td>隐藏状态 h + 细胞状态 C</td></tr><tr><td>参数数量</td><td>较少(约少 1/3)</td><td>较多</td></tr><tr><td>计算复杂度</td><td>较低</td><td>较高</td></tr><tr><td>信息流</td><td>直接通过隐藏状态传递</td><td>通过细胞状态和隐藏状态双通道传递</td></tr><tr><td>性能表现</td><td>简单任务表现好，资源消耗低</td><td>复杂任务表现更稳定</td></tr></tbody></table><p>现代最新架构（如 <code>Transformer</code>）实际上吸收了这两种思想的优点：</p><ul><li>LSTM 思想：通过残差连接实现&quot;记忆高速公路&quot;</li><li>GRU 思想：简化门控机制（如 Transformer 中的 FFN 层）</li></ul>',20)]))}const _=e(o,[["render",i]]);export{m as __pageData,_ as default};
