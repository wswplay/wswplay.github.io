import{_ as n,c as o,o as e,b as a}from"./chunks/framework._eNwL97Z.js";const g=JSON.parse('{"title":"自然语言处理、NLP","description":"","frontmatter":{"title":"自然语言处理、NLP","outline":"deep"},"headers":[{"level":2,"title":"预训练(pre-training)","slug":"预训练-pre-training","link":"#预训练-pre-training","children":[{"level":3,"title":"子词嵌入","slug":"子词嵌入","link":"#子词嵌入","children":[]},{"level":3,"title":"字节对编码(BPE)","slug":"字节对编码-bpe","link":"#字节对编码-bpe","children":[]}]}],"relativePath":"aiart/deep-learning/nlp.md","filePath":"aiart/deep-learning/nlp.md"}'),l={name:"aiart/deep-learning/nlp.md"};function t(r,s,p,i,c,u){return e(),o("div",null,s[0]||(s[0]=[a(`<h1 id="自然语言处理-nlp" tabindex="-1">自然语言处理(NLP) <a class="header-anchor" href="#自然语言处理-nlp" aria-label="Permalink to &quot;自然语言处理(NLP)&quot;">​</a></h1><p><strong>NLP</strong>：Natural Language Processing，是指研究使用自然语言的<strong>计算机和人类</strong>之间的<strong>交互</strong>。</p><h2 id="预训练-pre-training" tabindex="-1">预训练(pre-training) <a class="header-anchor" href="#预训练-pre-training" aria-label="Permalink to &quot;预训练(pre-training)&quot;">​</a></h2><p><strong>自监督学习</strong><sup>self-supervised learning</sup>已被广泛用于<strong>预训练</strong>文本表示，例如通过使用周围文本的其它部分来预测文本的隐藏部分。</p><h3 id="子词嵌入" tabindex="-1">子词嵌入 <a class="header-anchor" href="#子词嵌入" aria-label="Permalink to &quot;子词嵌入&quot;">​</a></h3><div class="language-py line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">py</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#6A737D;"># 代码示例（Hugging Face 库）</span></span>
<span class="line"><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> transformers </span><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> AutoTokenizer</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 加载BERT的WordPiece分词器</span></span>
<span class="line"><span style="color:#E1E4E8;">tokenizer </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> AutoTokenizer.from_pretrained(</span><span style="color:#9ECBFF;">&quot;bert-base-uncased&quot;</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">tokens </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> tokenizer.tokenize(</span><span style="color:#9ECBFF;">&quot;unhappiness&quot;</span><span style="color:#E1E4E8;">)  </span><span style="color:#6A737D;"># 输出：[&#39;un&#39;, &#39;##happy&#39;, &#39;##ness&#39;]</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 加载BPE分词器（GPT-2）</span></span>
<span class="line"><span style="color:#E1E4E8;">tokenizer </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> AutoTokenizer.from_pretrained(</span><span style="color:#9ECBFF;">&quot;gpt2&quot;</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">tokens </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> tokenizer.tokenize(</span><span style="color:#9ECBFF;">&quot;unhappiness&quot;</span><span style="color:#E1E4E8;">)  </span><span style="color:#6A737D;"># 输出：[&#39;un&#39;, &#39;happiness&#39;]</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><p>自然语言是用来表达人脑思维的复杂系统，<strong>词</strong>是意义的基本单元。</p><ul><li><strong>词向量</strong>：用于表示<strong>单词意义</strong>的向量，并且还可以被认为是单词<strong>特征向量或表示</strong>。</li><li><strong>词嵌入</strong>：将<strong>单词映射到实向量</strong>的技术。</li></ul><p><strong>子词嵌入</strong><sup>Subword Embedding</sup>通过<strong>将单词拆分为更小的单元</strong>（子词）来解决传统词嵌入局限性。</p><ul><li>未登录词（OOV）问题：传统词嵌入（如 Word2Vec）无法处理词汇表外的单词。</li><li>形态学相似性：子词能捕捉单词间的结构关系（如&quot;running&quot; → &quot;run&quot; + &quot;ing&quot;）。</li><li>多语言支持：共享子词可跨语言建模（如拉丁语系的共同词根）。</li></ul><p><strong>子词嵌入</strong> = <strong>子词分词</strong>(<code>BPE</code>等) + <strong>向量化</strong><sup>Embedding</sup></p><h3 id="字节对编码-bpe" tabindex="-1">字节对编码(BPE) <a class="header-anchor" href="#字节对编码-bpe" aria-label="Permalink to &quot;字节对编码(BPE)&quot;">​</a></h3><p><strong>BPE</strong>：Byte Pair Encoding，是一种<strong>子词分词算法</strong><sup>Subword Tokenization</sup>。</p><p><strong>核心思想</strong>：通过迭代<strong>合并高频符号对</strong>(字节对)构建词汇表，有效平衡词表大小与语义覆盖能力。</p><p><strong>基本步骤</strong>：</p><ol><li><strong>初始化词汇表</strong>：将所有基本字符（如字母、标点）加入词表。</li><li><strong>统计符号对频率</strong>：在训练语料中统计所有相邻符号对的共现频率。</li><li><strong>合并最高频对</strong>：将出现频率最高的符号对合并为一个新符号，加入词表。</li><li><strong>重复迭代</strong>：持续合并直到达到预设的词表大小或迭代次数。</li></ol><p><strong>示例演示</strong>：</p><p>假设语料为 <code>&quot;low low low lower newest widest&quot;</code>：</p><ul><li>初始词表：<code>{l, o, w, e, r, n, s, t, d, i}</code></li><li>第 1 步：最高频对 <code>&quot;lo&quot;</code>（出现 6 次）→ 合并为 <code>&quot;lo&quot;</code>，词表新增 <code>&quot;lo&quot;</code>。</li><li>第 2 步：最高频对 <code>&quot;low&quot;</code>（出现 4 次）→ 合并为 <code>&quot;low&quot;</code>，词表新增 <code>&quot;low&quot;</code>。</li><li>后续可能合并 <code>&quot;er&quot;</code>、<code>&quot;est&quot;</code>等。</li><li>最终词表可能包含子词如：<code>low, er, est, newer, wid</code>。</li></ul><p><strong>优点优势</strong>：</p><ul><li><strong>解决未登录词（OOV）</strong>：通过子词组合表示罕见词（如 <code>&quot;unhappiness&quot;</code> → <code>&quot;un&quot; + &quot;happy&quot; + &quot;ness&quot;</code>）。</li><li><strong>压缩词表大小</strong>：避免维护超大词表（如英语常用词约 20 万，BPE 词表可压缩到 1 万~3 万）。</li><li><strong>保留语义</strong>：相似词共享子词（如 <code>&quot;playing&quot;</code> 和 <code>&quot;played&quot;</code> 共享 <code>&quot;play&quot;</code>）。</li></ul><p><strong>代码示例</strong>：</p><div class="language-py line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">py</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> transformers </span><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> GPT2Tokenizer</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># GPT-2使用BPE分词</span></span>
<span class="line"><span style="color:#E1E4E8;">tokenizer </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> GPT2Tokenizer.from_pretrained(</span><span style="color:#9ECBFF;">&quot;gpt2&quot;</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">tokens </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> tokenizer.tokenize(</span><span style="color:#9ECBFF;">&quot;unhappiness&quot;</span><span style="color:#E1E4E8;">)  </span><span style="color:#6A737D;"># 输出：[&#39;un&#39;, &#39;happiness&#39;]</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div>`,23)]))}const E=n(l,[["render",t]]);export{g as __pageData,E as default};
