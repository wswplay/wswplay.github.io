import{_ as o,c as l,o as e,b as p,a as n,m as a}from"./chunks/framework._eNwL97Z.js";const t="/assets/lstm.Bv_KUP4K.svg",r="/assets/gru.DWWBWCgF.svg",c="/assets/encoder-decoder.CTwnUkfh.svg",i="/assets/seq2seq.CvvUDatq.svg",v=JSON.parse('{"title":"现代循环神经网络","description":"","frontmatter":{"title":"现代循环神经网络","outline":"deep"},"headers":[{"level":2,"title":"门控机制","slug":"门控机制","link":"#门控机制","children":[{"level":3,"title":"长短期记忆网络(LSTM)","slug":"长短期记忆网络-lstm","link":"#长短期记忆网络-lstm","children":[]},{"level":3,"title":"门控循环单元(GRU)","slug":"门控循环单元-gru","link":"#门控循环单元-gru","children":[]},{"level":3,"title":"对比与现代改进方向","slug":"对比与现代改进方向","link":"#对比与现代改进方向","children":[]}]},{"level":2,"title":"编码器-解码器架构","slug":"编码器-解码器架构","link":"#编码器-解码器架构","children":[{"level":3,"title":"编码器","slug":"编码器","link":"#编码器","children":[]},{"level":3,"title":"解码器","slug":"解码器","link":"#解码器","children":[]},{"level":3,"title":"合并","slug":"合并","link":"#合并","children":[]}]},{"level":2,"title":"序列到序列学习(seq2seq)","slug":"序列到序列学习-seq2seq","link":"#序列到序列学习-seq2seq","children":[{"level":3,"title":"用 RNN 实现编码器","slug":"用-rnn-实现编码器","link":"#用-rnn-实现编码器","children":[]}]}],"relativePath":"aiart/deep-learning/rnn-modern.md","filePath":"aiart/deep-learning/rnn-modern.md"}'),d={name:"aiart/deep-learning/rnn-modern.md"},E={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},y={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.025ex"},xmlns:"http://www.w3.org/2000/svg",width:"0.781ex",height:"1.52ex",role:"img",focusable:"false",viewBox:"0 -661 345 672","aria-hidden":"true"},u={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},b={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.025ex"},xmlns:"http://www.w3.org/2000/svg",width:"0.781ex",height:"1.52ex",role:"img",focusable:"false",viewBox:"0 -661 345 672","aria-hidden":"true"};function m(F,s,h,g,_,T){return e(),l("div",null,[s[13]||(s[13]=p('<h1 id="现代循环神经网络" tabindex="-1">现代循环神经网络 <a class="header-anchor" href="#现代循环神经网络" aria-label="Permalink to &quot;现代循环神经网络&quot;">​</a></h1><h2 id="门控机制" tabindex="-1">门控机制 <a class="header-anchor" href="#门控机制" aria-label="Permalink to &quot;门控机制&quot;">​</a></h2><h3 id="长短期记忆网络-lstm" tabindex="-1">长短期记忆网络(LSTM) <a class="header-anchor" href="#长短期记忆网络-lstm" aria-label="Permalink to &quot;长短期记忆网络(LSTM)&quot;">​</a></h3><p>LSTM：long short-term memory。</p><p>可以认为 LSTM 的核心创新就是引入了<code>Cell State</code>（细胞状态）这个&quot;全局记忆通道&quot;，而 GRU 可以看作是对 LSTM 的简化版本。</p><p><img src="'+t+'" alt="An Image"> 核心组件：</p><ul><li><strong>遗忘门(Forget Gate, f)</strong>：决定丢弃哪些信息。</li><li><strong>输入门(Input Gate, i)</strong>：决定更新哪些信息。</li><li><strong>输出门(Output Gate, o)</strong>：决定输出哪些信息。</li><li><strong>细胞状态(Cell State, C)</strong>：长期记忆通道。</li></ul><p>长短期记忆网络的隐藏层输出包括“隐状态”和“记忆元”。只有隐状态会传递到输出层，而记忆元完全属于内部信息。</p><h3 id="门控循环单元-gru" tabindex="-1">门控循环单元(GRU) <a class="header-anchor" href="#门控循环单元-gru" aria-label="Permalink to &quot;门控循环单元(GRU)&quot;">​</a></h3><p>GRU：Gated Recurrent Unit。</p><p>普通循环神经网络之相比，门控循环单元与支持<strong>隐状态门控</strong>，是简化版的 LSTM。</p><p>这意味着模型有专门的机制来确定应该<strong>何时更新隐状态，以及应该何时重置隐状态。这些机制是可学习的</strong>。例如，如果第一个词元非常重要，模型将学会在第一次观测之后不更新隐状态。同样，模型也可以学会跳过不相关的临时观测。</p><p>它通过引入可学习的&quot;门&quot;来控制信息流动，决定哪些信息应该被保留、哪些应该被遗忘。</p><p><img src="'+r+'" alt="An Image"> 核心组件：</p><ul><li><strong>重置门(Reset Gate, r)</strong>：控制前一时刻隐藏状态有多少信息需要被&quot;遗忘&quot;。</li><li><strong>更新门(Update Gate, z)</strong>：控制新状态中有多少来自前一状态，有多少来自当前计算的新候选状态。</li></ul><p>重置门打开时，门控循环单元包含基本循环神经网络；更新门打开时，门控循环单元可以跳过子序列。</p><h3 id="对比与现代改进方向" tabindex="-1">对比与现代改进方向 <a class="header-anchor" href="#对比与现代改进方向" aria-label="Permalink to &quot;对比与现代改进方向&quot;">​</a></h3><table tabindex="0"><thead><tr><th>机制</th><th>GRU</th><th>LSTM</th></tr></thead><tbody><tr><td>门数量</td><td>2 个(更新门、重置门)</td><td>3 个(输入门、遗忘门、输出门)</td></tr><tr><td>状态变量</td><td>只有隐藏状态 h</td><td>隐藏状态 h + 细胞状态 C</td></tr><tr><td>参数数量</td><td>较少(约少 1/3)</td><td>较多</td></tr><tr><td>计算复杂度</td><td>较低</td><td>较高</td></tr><tr><td>信息流</td><td>直接通过隐藏状态传递</td><td>通过细胞状态和隐藏状态双通道传递</td></tr><tr><td>性能表现</td><td>简单任务表现好，资源消耗低</td><td>复杂任务表现更稳定</td></tr></tbody></table><p>现代最新架构（如 <code>Transformer</code>）实际上吸收了这两种思想的优点：</p><ul><li>LSTM 思想：通过残差连接实现&quot;记忆高速公路&quot;</li><li>GRU 思想：简化门控机制（如 Transformer 中的 FFN 层）</li></ul><h2 id="编码器-解码器架构" tabindex="-1">编码器-解码器架构 <a class="header-anchor" href="#编码器-解码器架构" aria-label="Permalink to &quot;编码器-解码器架构&quot;">​</a></h2><p><img src="'+c+`" alt="An Image"> 编码器-解码器（<code>encoder-decoder</code>）架构两个主要组件：</p><ol><li><strong>编码器</strong>（encoder）：接受长度可变序列作为输入，并将其转换为具有固定形状编码状态。</li><li><strong>解码器</strong>（decoder）：将固定形状编码状态映射到长度可变序列。</li></ol><p>“编码器－解码器”架构可以将长度可变的序列作为输入和输出，因此适用于<strong>机器翻译</strong>等序列转换问题。</p><h3 id="编码器" tabindex="-1">编码器 <a class="header-anchor" href="#编码器" aria-label="Permalink to &quot;编码器&quot;">​</a></h3><div class="language-py line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">py</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> torch </span><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> nn</span></span>
<span class="line"><span style="color:#F97583;">class</span><span style="color:#B392F0;"> Encoder</span><span style="color:#E1E4E8;">(</span><span style="color:#B392F0;">nn</span><span style="color:#E1E4E8;">.</span><span style="color:#B392F0;">Module</span><span style="color:#E1E4E8;">):</span></span>
<span class="line"><span style="color:#F97583;">  def</span><span style="color:#79B8FF;"> __init__</span><span style="color:#E1E4E8;">(self, </span><span style="color:#F97583;">**</span><span style="color:#E1E4E8;">kwargs):</span></span>
<span class="line"><span style="color:#79B8FF;">    super</span><span style="color:#E1E4E8;">(Encoder, </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">).</span><span style="color:#79B8FF;">__init__</span><span style="color:#E1E4E8;">(</span><span style="color:#F97583;">**</span><span style="color:#E1E4E8;">kwargs)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583;">  def</span><span style="color:#B392F0;"> forward</span><span style="color:#E1E4E8;">(self, X, </span><span style="color:#F97583;">*</span><span style="color:#E1E4E8;">args):</span></span>
<span class="line"><span style="color:#F97583;">    raise</span><span style="color:#79B8FF;"> NotImplementedError</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><h3 id="解码器" tabindex="-1">解码器 <a class="header-anchor" href="#解码器" aria-label="Permalink to &quot;解码器&quot;">​</a></h3><div class="language-py line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">py</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#F97583;">class</span><span style="color:#B392F0;"> Decoder</span><span style="color:#E1E4E8;">(</span><span style="color:#B392F0;">nn</span><span style="color:#E1E4E8;">.</span><span style="color:#B392F0;">Module</span><span style="color:#E1E4E8;">):</span></span>
<span class="line"><span style="color:#F97583;">  def</span><span style="color:#79B8FF;"> __init__</span><span style="color:#E1E4E8;">(self, </span><span style="color:#F97583;">**</span><span style="color:#E1E4E8;">kwargs):</span></span>
<span class="line"><span style="color:#79B8FF;">    super</span><span style="color:#E1E4E8;">(Decoder, </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">).</span><span style="color:#79B8FF;">__init__</span><span style="color:#E1E4E8;">(</span><span style="color:#F97583;">**</span><span style="color:#E1E4E8;">kwargs)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583;">  def</span><span style="color:#B392F0;"> init_state</span><span style="color:#E1E4E8;">(self, enc_outputs, </span><span style="color:#F97583;">*</span><span style="color:#E1E4E8;">args):</span></span>
<span class="line"><span style="color:#F97583;">    raise</span><span style="color:#79B8FF;"> NotImplementedError</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583;">  def</span><span style="color:#B392F0;"> forward</span><span style="color:#E1E4E8;">(self, X, state):</span></span>
<span class="line"><span style="color:#F97583;">    raise</span><span style="color:#79B8FF;"> NotImplementedError</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><p>init_state 函数，用于将编码器的输出(<code>enc_outputs</code>)转换为编码后的状态。注意，此步骤可能需要额外的输入，例如：输入序列的有效长度。为了逐个地生成长度可变的词元序列，解码器在每个时间步都会将输入(例如：在前一时间步生成的词元)和编码后的状态映射成当前时间步的输出词元。</p><h3 id="合并" tabindex="-1">合并 <a class="header-anchor" href="#合并" aria-label="Permalink to &quot;合并&quot;">​</a></h3><div class="language-py line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">py</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#F97583;">class</span><span style="color:#B392F0;"> EncoderDecoder</span><span style="color:#E1E4E8;">(</span><span style="color:#B392F0;">nn</span><span style="color:#E1E4E8;">.</span><span style="color:#B392F0;">Module</span><span style="color:#E1E4E8;">):</span></span>
<span class="line"><span style="color:#F97583;">  def</span><span style="color:#79B8FF;"> __init__</span><span style="color:#E1E4E8;">(self, encoder, decoder, </span><span style="color:#F97583;">**</span><span style="color:#E1E4E8;">kwargs):</span></span>
<span class="line"><span style="color:#79B8FF;">    super</span><span style="color:#E1E4E8;">(EncoderDecoder, </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">).</span><span style="color:#79B8FF;">__init__</span><span style="color:#E1E4E8;">(</span><span style="color:#F97583;">**</span><span style="color:#E1E4E8;">kwargs)</span></span>
<span class="line"><span style="color:#79B8FF;">    self</span><span style="color:#E1E4E8;">.encoder </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> encoder</span></span>
<span class="line"><span style="color:#79B8FF;">    self</span><span style="color:#E1E4E8;">.decoder </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> decoder</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583;">  def</span><span style="color:#B392F0;"> forward</span><span style="color:#E1E4E8;">(self, enc_X, dec_X, </span><span style="color:#F97583;">*</span><span style="color:#E1E4E8;">args):</span></span>
<span class="line"><span style="color:#E1E4E8;">    enc_outputs </span><span style="color:#F97583;">=</span><span style="color:#79B8FF;"> self</span><span style="color:#E1E4E8;">.encoder(enc_X, </span><span style="color:#F97583;">*</span><span style="color:#E1E4E8;">args)</span></span>
<span class="line"><span style="color:#E1E4E8;">    dec_state </span><span style="color:#F97583;">=</span><span style="color:#79B8FF;"> self</span><span style="color:#E1E4E8;">.decoder.init_state(enc_outputs, </span><span style="color:#F97583;">*</span><span style="color:#E1E4E8;">args)</span></span>
<span class="line"><span style="color:#F97583;">    return</span><span style="color:#79B8FF;"> self</span><span style="color:#E1E4E8;">.decoder(dec_X, dec_state)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><p>“编码器-解码器”架构包含了一个编码器和一个解码器，并且还拥有可选的额外的参数。在前向传播中，编码器的输出用于生成编码状态，这个状态又被解码器作为其输入的一部分。</p><h2 id="序列到序列学习-seq2seq" tabindex="-1">序列到序列学习(seq2seq) <a class="header-anchor" href="#序列到序列学习-seq2seq" aria-label="Permalink to &quot;序列到序列学习(seq2seq)&quot;">​</a></h2><p>seq2seq：sequence to sequence。</p><p><img src="`+i+`" alt="An Image"> 上图是机器翻译中使用两个循环神经网络进行序列到序列学习。特定的<code>“&lt;eos&gt;”</code>表示序列结束词元。一旦输出序列生成此词元，模型就会停止预测。</p><p>在循环神经网络解码器的初始化时间步，有两个特定的设计决定：首先，特定的<code>“&lt;bos&gt;”</code>表示序列开始词元，它是解码器的输入序列的第一个词元。其次，使用循环神经网络编码器最终的隐状态来初始化解码器的隐状态。</p><h3 id="用-rnn-实现编码器" tabindex="-1">用 RNN 实现编码器 <a class="header-anchor" href="#用-rnn-实现编码器" aria-label="Permalink to &quot;用 RNN 实现编码器&quot;">​</a></h3><div class="language-py line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">py</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> collections</span></span>
<span class="line"><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> math</span></span>
<span class="line"><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> torch</span></span>
<span class="line"><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> torch </span><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> nn</span></span>
<span class="line"><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> d2l </span><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> torch </span><span style="color:#F97583;">as</span><span style="color:#E1E4E8;"> d2l</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 用于序列到序列学习的循环神经网络编码器</span></span>
<span class="line"><span style="color:#F97583;">class</span><span style="color:#B392F0;"> Seq2SeqEncoder</span><span style="color:#E1E4E8;">(</span><span style="color:#B392F0;">d2l</span><span style="color:#E1E4E8;">.</span><span style="color:#B392F0;">Encoder</span><span style="color:#E1E4E8;">):</span></span>
<span class="line"><span style="color:#F97583;">  def</span><span style="color:#79B8FF;"> __init__</span><span style="color:#E1E4E8;">(self, vocab_size, embed_size, num_hiddens, num_layers, dropout</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">, </span><span style="color:#F97583;">**</span><span style="color:#E1E4E8;">kwargs):</span></span>
<span class="line"><span style="color:#79B8FF;">    super</span><span style="color:#E1E4E8;">(Seq2SeqEncoder, </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">).</span><span style="color:#79B8FF;">__init__</span><span style="color:#E1E4E8;">(</span><span style="color:#F97583;">**</span><span style="color:#E1E4E8;">kwargs)</span></span>
<span class="line"><span style="color:#6A737D;">    # 嵌入层</span></span>
<span class="line"><span style="color:#79B8FF;">    self</span><span style="color:#E1E4E8;">.embedding </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> nn.Embedding(vocab_size, embed_size)</span></span>
<span class="line"><span style="color:#79B8FF;">    self</span><span style="color:#E1E4E8;">.rnn </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> nn.GRU(embed_size, num_hiddens, num_layers, </span><span style="color:#FFAB70;">dropout</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">dropout)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583;">  def</span><span style="color:#B392F0;"> forward</span><span style="color:#E1E4E8;">(self, X, </span><span style="color:#F97583;">*</span><span style="color:#E1E4E8;">args):</span></span>
<span class="line"><span style="color:#6A737D;">    # 输出&#39;X&#39;的形状：(batch_size,num_steps,embed_size)</span></span>
<span class="line"><span style="color:#E1E4E8;">    X </span><span style="color:#F97583;">=</span><span style="color:#79B8FF;"> self</span><span style="color:#E1E4E8;">.embedding(X)</span></span>
<span class="line"><span style="color:#6A737D;">    # 在循环神经网络模型中，第一个轴对应于时间步</span></span>
<span class="line"><span style="color:#E1E4E8;">    X </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> X.permute(</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">2</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#6A737D;">    # 如果未提及状态，则默认为0</span></span>
<span class="line"><span style="color:#E1E4E8;">    output, state </span><span style="color:#F97583;">=</span><span style="color:#79B8FF;"> self</span><span style="color:#E1E4E8;">.rnn(X)</span></span>
<span class="line"><span style="color:#6A737D;">    # output的形状:(num_steps,batch_size,num_hiddens)</span></span>
<span class="line"><span style="color:#6A737D;">    # state的形状:(num_layers,batch_size,num_hiddens)</span></span>
<span class="line"><span style="color:#F97583;">    return</span><span style="color:#E1E4E8;"> output, state</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br></div></div><p><strong>嵌入层</strong>(<code>embedding layer</code>，<code>nn.Embedding</code>)，用以获得输入序列中每个词元的特征向量。</p>`,39)),n("p",null,[s[4]||(s[4]=a("嵌入层的权重是一个矩阵，其行数是输入词表大小(")),s[5]||(s[5]=n("code",null,"vocab_size",-1)),s[6]||(s[6]=a(")，其列数是特征向量维度(")),s[7]||(s[7]=n("code",null,"embed_size",-1)),s[8]||(s[8]=a(")。对于任意输入词元索引 ")),n("mjx-container",E,[(e(),l("svg",y,s[0]||(s[0]=[n("g",{stroke:"currentColor",fill:"currentColor","stroke-width":"0",transform:"scale(1,-1)"},[n("g",{"data-mml-node":"math"},[n("g",{"data-mml-node":"mi"},[n("path",{"data-c":"1D456",d:"M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z",style:{"stroke-width":"3"}})])])],-1)]))),s[1]||(s[1]=n("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[n("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[n("mi",null,"i")])],-1))]),s[9]||(s[9]=a("，嵌入层获取权重矩阵第 ")),n("mjx-container",u,[(e(),l("svg",b,s[2]||(s[2]=[n("g",{stroke:"currentColor",fill:"currentColor","stroke-width":"0",transform:"scale(1,-1)"},[n("g",{"data-mml-node":"math"},[n("g",{"data-mml-node":"mi"},[n("path",{"data-c":"1D456",d:"M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z",style:{"stroke-width":"3"}})])])],-1)]))),s[3]||(s[3]=n("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[n("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[n("mi",null,"i")])],-1))]),s[10]||(s[10]=a(" 行(")),s[11]||(s[11]=n("code",null,"从0开始",-1)),s[12]||(s[12]=a(")以返回其特征向量。"))]),s[14]||(s[14]=p(`<div class="language-py line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">py</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#E1E4E8;">encoder </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> Seq2SeqEncoder(</span><span style="color:#FFAB70;">vocab_size</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">10</span><span style="color:#E1E4E8;">, </span><span style="color:#FFAB70;">embed_size</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">8</span><span style="color:#E1E4E8;">, </span><span style="color:#FFAB70;">num_hiddens</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">16</span><span style="color:#E1E4E8;">, </span><span style="color:#FFAB70;">num_layers</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">2</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">encoder.eval()</span></span>
<span class="line"><span style="color:#E1E4E8;">X </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torch.zeros((</span><span style="color:#79B8FF;">4</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">7</span><span style="color:#E1E4E8;">), </span><span style="color:#FFAB70;">dtype</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">torch.long)</span></span>
<span class="line"><span style="color:#E1E4E8;">output, state </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> encoder(X)</span></span>
<span class="line"><span style="color:#E1E4E8;">output.shape, state.shape</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># (torch.Size([7, 4, 16]), torch.Size([2, 4, 16]))</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><div class="tip custom-block"><p class="custom-block-title">TIP</p><p><code>embed_size &lt;&lt; vocab_size</code>，如大小接近，或导致参数量爆炸、过拟合。比率：<code>1/10 到 1/100</code>。<br><code>num_hiddens &gt;= embed_size</code>，确保隐藏层有足够容量融合时序信息和输入特征。比率：<code>num_hiddens = 2~4 × embed_size</code>。</p></div><p>最后一层的隐状态的输出是一个张量（output 由编码器的循环层返回），其形状为（时间步数，批量大小，隐藏单元数）。最后一个时间步的多层隐状态的形状是（隐藏层的数量，批量大小，隐藏单元的数量）。如果使用长短期记忆网络，<code>state</code>中还将包含记忆单元信息。</p>`,3))])}const B=o(d,[["render",m]]);export{v as __pageData,B as default};
