import{_ as t,c as l,o as e,b as p,a as n,m as a}from"./chunks/framework._eNwL97Z.js";const w=JSON.parse('{"title":"注意力机制","description":"","frontmatter":{"title":"注意力机制","outline":"deep"},"headers":[{"level":2,"title":"查询、键和值","slug":"查询、键和值","link":"#查询、键和值","children":[]},{"level":2,"title":"注意力机制","slug":"注意力机制","link":"#注意力机制","children":[]},{"level":2,"title":"多头注意力(multihead attention)","slug":"多头注意力-multihead-attention","link":"#多头注意力-multihead-attention","children":[{"level":3,"title":"代码实现","slug":"代码实现","link":"#代码实现","children":[]}]},{"level":2,"title":"自注意力和位置编码","slug":"自注意力和位置编码","link":"#自注意力和位置编码","children":[{"level":3,"title":"自注意力","slug":"自注意力","link":"#自注意力","children":[]}]}],"relativePath":"aiart/deep-learning/attention-mechanisms.md","filePath":"aiart/deep-learning/attention-mechanisms.md"}'),o={name:"aiart/deep-learning/attention-mechanisms.md"},r={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},i={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.025ex"},xmlns:"http://www.w3.org/2000/svg",width:"1.303ex",height:"1.595ex",role:"img",focusable:"false",viewBox:"0 -694 576 705","aria-hidden":"true"},c={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},E={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.025ex"},xmlns:"http://www.w3.org/2000/svg",width:"1.303ex",height:"1.595ex",role:"img",focusable:"false",viewBox:"0 -694 576 705","aria-hidden":"true"},d={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},y={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.025ex"},xmlns:"http://www.w3.org/2000/svg",width:"1.303ex",height:"1.595ex",role:"img",focusable:"false",viewBox:"0 -694 576 705","aria-hidden":"true"},u={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},m={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.025ex"},xmlns:"http://www.w3.org/2000/svg",width:"1.303ex",height:"1.595ex",role:"img",focusable:"false",viewBox:"0 -694 576 705","aria-hidden":"true"};function b(h,s,F,g,x,Q){return e(),l("div",null,[s[26]||(s[26]=p('<h1 id="注意力机制-attention-mechanisms" tabindex="-1">注意力机制(attention mechanisms) <a class="header-anchor" href="#注意力机制-attention-mechanisms" aria-label="Permalink to &quot;注意力机制(attention mechanisms)&quot;">​</a></h1><h2 id="查询、键和值" tabindex="-1">查询、键和值 <a class="header-anchor" href="#查询、键和值" aria-label="Permalink to &quot;查询、键和值&quot;">​</a></h2><p>在注意力机制的背景下，<strong>自主性提示</strong>被称为<strong>查询（query）</strong>。</p><p>给定任何查询，注意力机制通过注意力汇聚（<code>attention pooling</code>） 将选择引导至感官输入（<code>sensory inputs</code>，例如中间特征表示）。这些感官输入被称为<strong>值（value）</strong>。</p><p>每个<strong>值</strong>都与一个<strong>键</strong>（<code>key</code>）配对，这可以想象为感官输入的<strong>非自主提示</strong>。</p><h2 id="注意力机制" tabindex="-1">注意力机制 <a class="header-anchor" href="#注意力机制" aria-label="Permalink to &quot;注意力机制&quot;">​</a></h2><p>通过<strong>注意力汇聚</strong>将<strong>查询</strong>（自主性提示）和<strong>键</strong>（非自主性提示）结合在一起，实现对<strong>值</strong>（感官输入）的<strong>选择</strong>倾向（智能选择）。</p><p>“查询-键”对<strong>越近</strong>，注意力汇聚<strong>注意力权重</strong>就越高。</p><p>“是否包含自主性提示”将<strong>注意力机制</strong>与<strong>全连接层或汇聚层</strong>区别开来。</p><table tabindex="0"><thead><tr><th>特性</th><th>注意力机制</th><th>全连接层</th><th>汇聚层</th></tr></thead><tbody><tr><td><strong>参数化</strong></td><td>是（动态权重）</td><td>是（静态权重）</td><td>否（固定规则）</td></tr><tr><td><strong>自主性提示</strong></td><td>✅ 动态适应输入</td><td>❌ 静态处理</td><td>❌ 静态处理</td></tr><tr><td><strong>输入依赖性</strong></td><td>高度依赖</td><td>不依赖</td><td>不依赖</td></tr><tr><td><strong>典型应用</strong></td><td>Transformer, NLP</td><td>传统分类模型</td><td>CNN 的空间降维</td></tr></tbody></table><h2 id="多头注意力-multihead-attention" tabindex="-1">多头注意力(multihead attention) <a class="header-anchor" href="#多头注意力-multihead-attention" aria-label="Permalink to &quot;多头注意力(multihead attention)&quot;">​</a></h2><p>在实践中，当给定相同的查询、键和值的集合时，我们希望模型可以基于相同的注意力机制学习到不同的行为，然后将不同的行为作为知识组合起来，捕获序列内各种范围的依赖关系（例如，短距离依赖和长距离依赖关系）。</p><p>因此，允许注意力机制<strong>组合查询</strong>、键和值的不同<strong>子空间</strong>表示（representation subspaces）可能是有益的。</p>',13)),n("p",null,[s[6]||(s[6]=a("与其只使用单独一个注意力汇聚， 我们可以用独立学习得到的 ")),n("mjx-container",r,[(e(),l("svg",i,s[0]||(s[0]=[n("g",{stroke:"currentColor",fill:"currentColor","stroke-width":"0",transform:"scale(1,-1)"},[n("g",{"data-mml-node":"math"},[n("g",{"data-mml-node":"mi"},[n("path",{"data-c":"210E",d:"M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z",style:{"stroke-width":"3"}})])])],-1)]))),s[1]||(s[1]=n("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[n("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[n("mi",null,"h")])],-1))]),s[7]||(s[7]=a(" 组不同的")),s[8]||(s[8]=n("strong",null,"线性投影",-1)),s[9]||(s[9]=a("（linear projections）来变换")),s[10]||(s[10]=n("strong",null,"查询、键和值",-1)),s[11]||(s[11]=a("。然后，这 ")),n("mjx-container",c,[(e(),l("svg",E,s[2]||(s[2]=[n("g",{stroke:"currentColor",fill:"currentColor","stroke-width":"0",transform:"scale(1,-1)"},[n("g",{"data-mml-node":"math"},[n("g",{"data-mml-node":"mi"},[n("path",{"data-c":"210E",d:"M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z",style:{"stroke-width":"3"}})])])],-1)]))),s[3]||(s[3]=n("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[n("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[n("mi",null,"h")])],-1))]),s[12]||(s[12]=a(" 组变换后的查询、键和值将并行地送到注意力汇聚中。最后，将这 ")),n("mjx-container",d,[(e(),l("svg",y,s[4]||(s[4]=[n("g",{stroke:"currentColor",fill:"currentColor","stroke-width":"0",transform:"scale(1,-1)"},[n("g",{"data-mml-node":"math"},[n("g",{"data-mml-node":"mi"},[n("path",{"data-c":"210E",d:"M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z",style:{"stroke-width":"3"}})])])],-1)]))),s[5]||(s[5]=n("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[n("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[n("mi",null,"h")])],-1))]),s[13]||(s[13]=a(" 个注意力汇聚的输出")),s[14]||(s[14]=n("strong",null,"拼接",-1)),s[15]||(s[15]=a("在一起，并且通过另一个可以学习的线性投影进行变换，以产生最终输出。这种设计被称为")),s[16]||(s[16]=n("strong",null,"多头注意力（multihead attention）",-1)),s[17]||(s[17]=a("。"))]),n("p",null,[s[20]||(s[20]=a("对于 ")),n("mjx-container",u,[(e(),l("svg",m,s[18]||(s[18]=[n("g",{stroke:"currentColor",fill:"currentColor","stroke-width":"0",transform:"scale(1,-1)"},[n("g",{"data-mml-node":"math"},[n("g",{"data-mml-node":"mi"},[n("path",{"data-c":"210E",d:"M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z",style:{"stroke-width":"3"}})])])],-1)]))),s[19]||(s[19]=n("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[n("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[n("mi",null,"h")])],-1))]),s[21]||(s[21]=a(" 个注意力汇聚输出，")),s[22]||(s[22]=n("strong",null,"每一个注意力汇聚",-1)),s[23]||(s[23]=a("都被称作")),s[24]||(s[24]=n("strong",null,"一个头（head）",-1)),s[25]||(s[25]=a("。"))]),s[27]||(s[27]=p(`<h3 id="代码实现" tabindex="-1">代码实现 <a class="header-anchor" href="#代码实现" aria-label="Permalink to &quot;代码实现&quot;">​</a></h3><p>在实现过程中通常选择缩放点积注意力作为每一个注意力头。</p><div class="language-py line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">py</span><pre class="shiki github-dark vp-code" tabindex="0"><code><span class="line"><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> math</span></span>
<span class="line"><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> torch</span></span>
<span class="line"><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> torch </span><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> nn</span></span>
<span class="line"><span style="color:#F97583;">from</span><span style="color:#E1E4E8;"> d2l </span><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> torch </span><span style="color:#F97583;">as</span><span style="color:#E1E4E8;"> d2l</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583;">class</span><span style="color:#B392F0;"> MultiHeadAttention</span><span style="color:#E1E4E8;">(</span><span style="color:#B392F0;">nn</span><span style="color:#E1E4E8;">.</span><span style="color:#B392F0;">Module</span><span style="color:#E1E4E8;">):</span></span>
<span class="line"><span style="color:#F97583;">  def</span><span style="color:#79B8FF;"> __init__</span><span style="color:#E1E4E8;">(self, key_size, query_size, value_size, num_hiddens,</span></span>
<span class="line"><span style="color:#E1E4E8;">                num_heads, dropout, bias</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">False</span><span style="color:#E1E4E8;">, </span><span style="color:#F97583;">**</span><span style="color:#E1E4E8;">kwargs):</span></span>
<span class="line"><span style="color:#79B8FF;">    super</span><span style="color:#E1E4E8;">(MultiHeadAttention, </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">).</span><span style="color:#79B8FF;">__init__</span><span style="color:#E1E4E8;">(</span><span style="color:#F97583;">**</span><span style="color:#E1E4E8;">kwargs)</span></span>
<span class="line"><span style="color:#79B8FF;">    self</span><span style="color:#E1E4E8;">.num_heads </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> num_heads</span></span>
<span class="line"><span style="color:#79B8FF;">    self</span><span style="color:#E1E4E8;">.attention </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> d2l.DotProductAttention(dropout)</span></span>
<span class="line"><span style="color:#79B8FF;">    self</span><span style="color:#E1E4E8;">.W_q </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> nn.Linear(query_size, num_hiddens, </span><span style="color:#FFAB70;">bias</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">bias)</span></span>
<span class="line"><span style="color:#79B8FF;">    self</span><span style="color:#E1E4E8;">.W_k </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> nn.Linear(key_size, num_hiddens, </span><span style="color:#FFAB70;">bias</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">bias)</span></span>
<span class="line"><span style="color:#79B8FF;">    self</span><span style="color:#E1E4E8;">.W_v </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> nn.Linear(value_size, num_hiddens, </span><span style="color:#FFAB70;">bias</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">bias)</span></span>
<span class="line"><span style="color:#79B8FF;">    self</span><span style="color:#E1E4E8;">.W_o </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> nn.Linear(num_hiddens, num_hiddens, </span><span style="color:#FFAB70;">bias</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">bias)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583;">  def</span><span style="color:#B392F0;"> forward</span><span style="color:#E1E4E8;">(self, queries, keys, values, valid_lens):</span></span>
<span class="line"><span style="color:#6A737D;">    # queries，keys，values的形状: (batch_size，查询或者“键－值”对的个数，num_hiddens)</span></span>
<span class="line"><span style="color:#6A737D;">    # valid_lens的形状: (batch_size，)或(batch_size，查询的个数)</span></span>
<span class="line"><span style="color:#6A737D;">    # 经过变换后，输出的queries，keys，values　的形状:</span></span>
<span class="line"><span style="color:#6A737D;">    # (batch_size*num_heads，查询或者“键－值”对的个数，num_hiddens/num_heads)</span></span>
<span class="line"><span style="color:#E1E4E8;">    queries </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> transpose_qkv(</span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.W_q(queries), </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.num_heads)</span></span>
<span class="line"><span style="color:#E1E4E8;">    keys </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> transpose_qkv(</span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.W_k(keys), </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.num_heads)</span></span>
<span class="line"><span style="color:#E1E4E8;">    values </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> transpose_qkv(</span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.W_v(values), </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.num_heads)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583;">    if</span><span style="color:#E1E4E8;"> valid_lens </span><span style="color:#F97583;">is</span><span style="color:#F97583;"> not</span><span style="color:#79B8FF;"> None</span><span style="color:#E1E4E8;">:</span></span>
<span class="line"><span style="color:#6A737D;">      # 在轴0，将第一项（标量或者矢量）复制num_heads次，</span></span>
<span class="line"><span style="color:#6A737D;">      # 然后如此复制第二项，然后诸如此类。</span></span>
<span class="line"><span style="color:#E1E4E8;">      valid_lens </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torch.repeat_interleave(valid_lens, </span><span style="color:#FFAB70;">repeats</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.num_heads, </span><span style="color:#FFAB70;">dim</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;">    # output的形状:(batch_size*num_heads，查询的个数，num_hiddens/num_heads)</span></span>
<span class="line"><span style="color:#E1E4E8;">    output </span><span style="color:#F97583;">=</span><span style="color:#79B8FF;"> self</span><span style="color:#E1E4E8;">.attention(queries, keys, values, valid_lens)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;">    # output_concat的形状:(batch_size，查询的个数，num_hiddens)</span></span>
<span class="line"><span style="color:#E1E4E8;">    output_concat </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> transpose_output(output, </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.num_heads)</span></span>
<span class="line"><span style="color:#F97583;">    return</span><span style="color:#79B8FF;"> self</span><span style="color:#E1E4E8;">.W_o(output_concat)</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 为了多注意力头的并行计算而变换形状</span></span>
<span class="line"><span style="color:#F97583;">def</span><span style="color:#B392F0;"> transpose_qkv</span><span style="color:#E1E4E8;">(X, num_heads):</span></span>
<span class="line"><span style="color:#6A737D;">  # 输入X的形状:(batch_size，查询或者“键－值”对的个数，num_hiddens)</span></span>
<span class="line"><span style="color:#6A737D;">  # 输出X的形状:(batch_size，查询或者“键－值”对的个数，num_heads，num_hiddens/num_heads)</span></span>
<span class="line"><span style="color:#E1E4E8;">  X </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> X.reshape(X.shape[</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">], X.shape[</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">], num_heads, </span><span style="color:#F97583;">-</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;">  # 输出X的形状:(batch_size，num_heads，查询或者“键－值”对的个数，num_hiddens/num_heads)</span></span>
<span class="line"><span style="color:#E1E4E8;">  X </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> X.permute(</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">2</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">3</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;">  # 最终输出的形状:(batch_size*num_heads，查询或者“键－值”对的个数，num_hiddens/num_heads)</span></span>
<span class="line"><span style="color:#F97583;">  return</span><span style="color:#E1E4E8;"> X.reshape(</span><span style="color:#F97583;">-</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">, X.shape[</span><span style="color:#79B8FF;">2</span><span style="color:#E1E4E8;">], X.shape[</span><span style="color:#79B8FF;">3</span><span style="color:#E1E4E8;">])</span></span>
<span class="line"></span>
<span class="line"><span style="color:#6A737D;"># 逆转transpose_qkv函数的操作</span></span>
<span class="line"><span style="color:#F97583;">def</span><span style="color:#B392F0;"> transpose_output</span><span style="color:#E1E4E8;">(X, num_heads):</span></span>
<span class="line"><span style="color:#E1E4E8;">  X </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> X.reshape(</span><span style="color:#F97583;">-</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">, num_heads, X.shape[</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">], X.shape[</span><span style="color:#79B8FF;">2</span><span style="color:#E1E4E8;">])</span></span>
<span class="line"><span style="color:#E1E4E8;">  X </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> X.permute(</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">2</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">, </span><span style="color:#79B8FF;">3</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#F97583;">  return</span><span style="color:#E1E4E8;"> X.reshape(X.shape[</span><span style="color:#79B8FF;">0</span><span style="color:#E1E4E8;">], X.shape[</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">], </span><span style="color:#F97583;">-</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br><span class="line-number">40</span><br><span class="line-number">41</span><br><span class="line-number">42</span><br><span class="line-number">43</span><br><span class="line-number">44</span><br><span class="line-number">45</span><br><span class="line-number">46</span><br><span class="line-number">47</span><br><span class="line-number">48</span><br><span class="line-number">49</span><br><span class="line-number">50</span><br><span class="line-number">51</span><br><span class="line-number">52</span><br><span class="line-number">53</span><br><span class="line-number">54</span><br><span class="line-number">55</span><br></div></div><h2 id="自注意力和位置编码" tabindex="-1">自注意力和位置编码 <a class="header-anchor" href="#自注意力和位置编码" aria-label="Permalink to &quot;自注意力和位置编码&quot;">​</a></h2><h3 id="自注意力" tabindex="-1">自注意力 <a class="header-anchor" href="#自注意力" aria-label="Permalink to &quot;自注意力&quot;">​</a></h3><p>将词元序列输入<strong>注意力池化</strong>中，以便同一组词元同时充当查询、键和值。</p><p>即每个查询都会关注所有键－值对并生成一个注意力输出。由于<strong>查询、键和值来自同一组输入</strong>，被称为<strong>自注意力</strong>（<code>self-attention</code>），或<strong>内部注意力</strong>（<code>intra-attention</code>）。</p>`,7))])}const f=t(o,[["render",b]]);export{w as __pageData,f as default};
