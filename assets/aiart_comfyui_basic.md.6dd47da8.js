import{_ as e,c as a,o as i,a as t}from"./app.ff132c15.js";const g=JSON.parse('{"title":"ComfyUI","description":"","frontmatter":{"title":"ComfyUI"},"headers":[{"level":2,"title":"VAE(变分自编码器)","slug":"vae-变分自编码器","link":"#vae-变分自编码器","children":[{"level":3,"title":"总结","slug":"总结","link":"#总结","children":[]}]},{"level":2,"title":"CLIP-(对比式语言-图像预训练)","slug":"clip-对比式语言-图像预训练","link":"#clip-对比式语言-图像预训练","children":[{"level":3,"title":"总结","slug":"总结-1","link":"#总结-1","children":[]}]}],"relativePath":"aiart/comfyui/basic.md"}'),r={name:"aiart/comfyui/basic.md"},n=t('<h1 id="your-creativity-we-visualed" tabindex="-1">Your creativity, We Visualed！ <a class="header-anchor" href="#your-creativity-we-visualed" aria-hidden="true">#</a></h1><h2 id="vae-变分自编码器" tabindex="-1">VAE(变分自编码器) <a class="header-anchor" href="#vae-变分自编码器" aria-hidden="true">#</a></h2><p>VAE(<code>Variational Autoencoder</code>) 是一种生成模型，包含两个主要部分：编码器（Encoder）和解码器（Decoder）。</p><h3 id="总结" tabindex="-1">总结 <a class="header-anchor" href="#总结" aria-hidden="true">#</a></h3><ul><li><strong>编码器</strong>：将数据映射到潜在空间。</li><li><strong>解码器</strong>：从潜在空间生成数据。</li><li><strong>目标</strong>：学习数据的低维表示并生成新数据。</li></ul><h2 id="clip-对比式语言-图像预训练" tabindex="-1">CLIP-(对比式语言-图像预训练) <a class="header-anchor" href="#clip-对比式语言-图像预训练" aria-hidden="true">#</a></h2><p>CLIP（<code>Contrastive Language–Image Pretraining</code>）是 OpenAI 提出的一种多模态模型，用于将图像和文本映射到同一个语义空间。与 VAE 不同，CLIP 没有传统意义上的“编码器”和“解码器”，而是由两个核心组件组成：<strong>图像编码器</strong>和<strong>文本编码器</strong>。</p><h3 id="总结-1" tabindex="-1">总结 <a class="header-anchor" href="#总结-1" aria-hidden="true">#</a></h3><p>CLIP 的“编码器”包括图像编码器和文本编码器，它们分别将图像和文本映射到同一个语义空间。CLIP 没有传统意义上的“解码器”，而是通过对比学习实现图像和文本的语义对齐。CLIP 的核心优势在于其强大的多模态理解能力，能够广泛应用于图像-文本检索、零样本分类等任务。</p>',9),l=[n];function o(c,s,d,h,_,u){return i(),a("div",null,l)}const f=e(r,[["render",o]]);export{g as __pageData,f as default};
